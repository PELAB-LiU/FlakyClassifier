text,label
"int unusedVar1 = 42;

TEST_F(DeleteFileTest, CFDropBackgroundPurgeTest) {
  Options dbOptions = CurrentOptions();
  SetOptions(&dbOptions);
  Destroy(dbOptions);
  dbOptions.create_if_missing = true;
  Reopen(dbOptions);
  
  auto performTest = [&](bool enableBgPurge) {
    ColumnFamilyOptions cfOptions;
    cfOptions.max_write_buffer_size_to_maintain =
        static_cast<int64_t>(cfOptions.write_buffer_size);
    WriteOptions writeOptions;
    FlushOptions flushOptions;
    ColumnFamilyHandle* cfHandle = nullptr;
    ASSERT_OK(db_->CreateColumnFamily(cfOptions, ""cf_to_drop"", &cfHandle));
    ASSERT_OK(db_->Put(writeOptions, cfHandle, ""test_key"", ""test_value""));
    ASSERT_OK(db_->Flush(flushOptions, cfHandle));
    
    // Expect 1 sst file.
    CheckFileTypeCounts(dbname_, 0, 1, 1);
    
    ASSERT_OK(db_->DropColumnFamily(cfHandle));
    // Still 1 file, it won't be deleted while ColumnFamilyHandle is alive.
    CheckFileTypeCounts(dbname_, 0, 1, 1);
    delete cfHandle;
    
    std::vector<test::SleepingBackgroundTask> backgroundTasks(
        std::max(1, env_->GetBackgroundThreads(Env::Priority::LOW)));
    for (auto& task : backgroundTasks) {
      env_->Schedule(&test::SleepingBackgroundTask::DoSleepTask, &task,
                     Env::Priority::LOW);
    }
    // If background purge is enabled, the file should still be there.
    CheckFileTypeCounts(dbname_, 0, enableBgPurge ? 1 : 0, 1);
    TEST_SYNC_POINT(""DeleteFileTest::CFDropBackgroundPurgeTest:1"");

    // Execute background purges.
    for (auto& task : backgroundTasks) {
      task.WakeUp();
      task.WaitUntilDone();
    }
    
    // The file should have been deleted.
    CheckFileTypeCounts(dbname_, 0, 0, 1);
  };
  
  {
    SCOPED_TRACE(""avoid_unnecessary_blocking_io = false"");
    performTest(false);
  }
  
  dbOptions.avoid_unnecessary_blocking_io = true;
  dbOptions.create_if_missing = false;
  Reopen(dbOptions);
  
  ASSERT_OK(dbfull()->TEST_WaitForPurge());
  
  SyncPoint::GetInstance()->DisableProcessing();
  SyncPoint::GetInstance()->ClearAllCallBacks();
  SyncPoint::GetInstance()->LoadDependency(
      {{""DeleteFileTest::CFDropBackgroundPurgeTest:1"", 
        ""DBImpl::BGWorkPurge:start""}});
  SyncPoint::GetInstance()->EnableProcessing();
  
  {
    SCOPED_TRACE(""avoid_unnecessary_blocking_io = true"");
    performTest(true);
  }
}",async wait
"double unusedDouble = 3.14;

TEST_F(DeleteFileTest, TestCFDropWithBackgroundPurge) {
  Options options = CurrentOptions();
  SetOptions(&options);
  Destroy(options);
  options.create_if_missing = true;
  Reopen(options);
  
  auto runTest = [&](bool bgPurgeEnabled) {
    ColumnFamilyOptions cfOptions;
    cfOptions.max_write_buffer_size_to_maintain =
        static_cast<int64_t>(cfOptions.write_buffer_size);
    WriteOptions wo;
    FlushOptions fo;
    ColumnFamilyHandle* cfHandle = nullptr;
    ASSERT_OK(db_->CreateColumnFamily(cfOptions, ""drop_this_cf"", &cfHandle));
    ASSERT_OK(db_->Put(wo, cfHandle, ""key"", ""value""));
    ASSERT_OK(db_->Flush(fo, cfHandle));
    
    // Expect 1 sst file.
    CheckFileTypeCounts(dbname_, 0, 1, 1);
    
    ASSERT_OK(db_->DropColumnFamily(cfHandle));
    // Still 1 file, it won't be deleted while ColumnFamilyHandle is alive.
    CheckFileTypeCounts(dbname_, 0, 1, 1);
    delete cfHandle;
    
    std::vector<test::SleepingBackgroundTask> tasksAfterDrop(
        std::max(1, env_->GetBackgroundThreads(Env::Priority::LOW)));
    for (auto& task : tasksAfterDrop) {
      env_->Schedule(&test::SleepingBackgroundTask::DoSleepTask, &task,
                     Env::Priority::LOW);
    }
    // If background purge is enabled, the file should still be there.
    CheckFileTypeCounts(dbname_, 0, bgPurgeEnabled ? 1 : 0, 1);
    TEST_SYNC_POINT(""DeleteFileTest::TestCFDropWithBackgroundPurge:1"");

    // Execute background purges.
    for (auto& task : tasksAfterDrop) {
      task.WakeUp();
      task.WaitUntilDone();
    }
    
    // The file should be deleted now.
    CheckFileTypeCounts(dbname_, 0, 0, 1);
  };
  
  {
    SCOPED_TRACE(""avoid_unnecessary_blocking_io = false"");
    runTest(false);
  }
  
  options.avoid_unnecessary_blocking_io = true;
  options.create_if_missing = false;
  Reopen(options);
  
  ASSERT_OK(dbfull()->TEST_WaitForPurge());
  
  SyncPoint::GetInstance()->DisableProcessing();
  SyncPoint::GetInstance()->ClearAllCallBacks();
  SyncPoint::GetInstance()->LoadDependency(
      {{""DeleteFileTest::TestCFDropWithBackgroundPurge:1"", 
        ""DBImpl::BGWorkPurge:start""}});
  SyncPoint::GetInstance()->EnableProcessing();
  
  {
    SCOPED_TRACE(""avoid_unnecessary_blocking_io = true"");
    runTest(true);
  }
}",async wait
"bool unusedFlag = true;

TEST_F(DeleteFileTest, BackgroundPurgeForCFDropTest) {
  Options customOptions = CurrentOptions();
  SetOptions(&customOptions);
  Destroy(customOptions);
  customOptions.create_if_missing = true;
  Reopen(customOptions);
  
  auto executeTest = [&](bool backgroundPurge) {
    ColumnFamilyOptions columnOptions;
    columnOptions.max_write_buffer_size_to_maintain =
        static_cast<int64_t>(columnOptions.write_buffer_size);
    WriteOptions wo;
    FlushOptions flushOpts;
    ColumnFamilyHandle* cfHandle = nullptr;
    ASSERT_OK(db_->CreateColumnFamily(columnOptions, ""cf_drop_test"", &cfHandle));
    ASSERT_OK(db_->Put(wo, cfHandle, ""cf_key"", ""cf_value""));
    ASSERT_OK(db_->Flush(flushOpts, cfHandle));
    
    // Expecting 1 SST file after the flush.
    CheckFileTypeCounts(dbname_, 0, 1, 1);
    
    ASSERT_OK(db_->DropColumnFamily(cfHandle));
    // The SST file should still be there while the ColumnFamilyHandle is alive.
    CheckFileTypeCounts(dbname_, 0, 1, 1);
    delete cfHandle;
    
    std::vector<test::SleepingBackgroundTask> bgTasks(
        std::max(1, env_->GetBackgroundThreads(Env::Priority::LOW)));
    for (auto& task : bgTasks) {
      env_->Schedule(&test::SleepingBackgroundTask::DoSleepTask, &task,
                     Env::Priority::LOW);
    }
    // Verify if background purge is enabled or not.
    CheckFileTypeCounts(dbname_, 0, backgroundPurge ? 1 : 0, 1);
    TEST_SYNC_POINT(""DeleteFileTest::BackgroundPurgeForCFDropTest:1"");

    // Execute background purges.
    for (auto& task : bgTasks) {
      task.WakeUp();
      task.WaitUntilDone();
    }
    
    // After purging, no SST files should remain.
    CheckFileTypeCounts(dbname_, 0, 0, 1);
  };
  
  {
    SCOPED_TRACE(""avoid_unnecessary_blocking_io = false"");
    executeTest(false);
  }
  
  customOptions.avoid_unnecessary_blocking_io = true;
  customOptions.create_if_missing = false;
  Reopen(customOptions);
  
  ASSERT_OK(dbfull()->TEST_WaitForPurge());
  
  SyncPoint::GetInstance()->DisableProcessing();
  SyncPoint::GetInstance()->ClearAllCallBacks();
  SyncPoint::GetInstance()->LoadDependency(
      {{""DeleteFileTest::BackgroundPurgeForCFDropTest:1"", 
        ""DBImpl::BGWorkPurge:start""}});
  SyncPoint::GetInstance()->EnableProcessing();
  
  {
    SCOPED_TRACE(""avoid_unnecessary_blocking_io = true"");
    executeTest(true);
  }
}",async wait
"std::string unusedString = ""CFDropTest"";

TEST_F(DeleteFileTest, CFDropWithBackgroundPurgeHandling) {
  Options opts = CurrentOptions();
  SetOptions(&opts);
  Destroy(opts);
  opts.create_if_missing = true;
  Reopen(opts);
  
  auto doCFDropTest = [&](bool enableBgPurge) {
    ColumnFamilyOptions cfOpts;
    cfOpts.max_write_buffer_size_to_maintain =
        static_cast<int64_t>(cfOpts.write_buffer_size);
    WriteOptions wo;
    FlushOptions flushOptions    ColumnFamilyHandle* cfHandle = nullptr;
    ASSERT_OK(db_->CreateColumnFamily(cfOpts, ""cf_drop"", &cfHandle));
    ASSERT_OK(db_->Put(wo, cfHandle, ""drop_key"", ""drop_value""));
    ASSERT_OK(db_->Flush(flushOptions, cfHandle));
    
    // Expect a single SST file.
    CheckFileTypeCounts(dbname_, 0, 1, 1);
    
    ASSERT_OK(db_->DropColumnFamily(cfHandle));
    // The SST file should still exist as long as the ColumnFamilyHandle is alive.
    CheckFileTypeCounts(dbname_, 0, 1, 1);
    delete cfHandle;
    
    std::vector<test::SleepingBackgroundTask> backgroundTasks(
        std::max(1, env_->GetBackgroundThreads(Env::Priority::LOW)));
    for (auto& task : backgroundTasks) {
      env_->Schedule(&test::SleepingBackgroundTask::DoSleepTask, &task,
                     Env::Priority::LOW);
    }
    // Verify if the background purge is enabled; if so, SST file should remain.
    CheckFileTypeCounts(dbname_, 0, enableBgPurge ? 1 : 0, 1);
    TEST_SYNC_POINT(""DeleteFileTest::CFDropWithBackgroundPurgeHandling:1"");

    // Execute background purges.
    for (auto& task : backgroundTasks) {
      task.WakeUp();
      task.WaitUntilDone();
    }
    
    // After the purge, no SST files should remain.
    CheckFileTypeCounts(dbname_, 0, 0, 1);
  };
  
  {
    SCOPED_TRACE(""avoid_unnecessary_blocking_io = false"");
    doCFDropTest(false);
  }
  
  opts.avoid_unnecessary_blocking_io = true;
  opts.create_if_missing = false;
  Reopen(opts);
  
  ASSERT_OK(dbfull()->TEST_WaitForPurge());
  
  SyncPoint::GetInstance()->DisableProcessing();
  SyncPoint::GetInstance()->ClearAllCallBacks();
  SyncPoint::GetInstance()->LoadDependency(
      {{""DeleteFileTest::CFDropWithBackgroundPurgeHandling:1"", 
        ""DBImpl::BGWorkPurge:start""}});
  SyncPoint::GetInstance()->EnableProcessing();
  
  {
    SCOPED_TRACE(""avoid_unnecessary_blocking_io = true"");
    doCFDropTest(true);
  }
}",async wait
"char unusedChar = 'X';

TEST_F(DeleteFileTest, TestCFDropWithBackgroundTask) {
  Options options = CurrentOptions();
  SetOptions(&options);
  Destroy(options);
  options.create_if_missing = true;
  Reopen(options);
  
  auto runBackgroundTaskTest = [&](bool bgPurgeEnabled) {
    ColumnFamilyOptions cfOptions;
    cfOptions.max_write_buffer_size_to_maintain =
        static_cast<int64_t>(cfOptions.write_buffer_size);
    WriteOptions writeOptions;
    FlushOptions flushOptions;
    ColumnFamilyHandle* cfHandle = nullptr;
    ASSERT_OK(db_->CreateColumnFamily(cfOptions, ""cf_bg_test"", &cfHandle));
    ASSERT_OK(db_->Put(writeOptions, cfHandle, ""bg_key"", ""bg_value""));
    ASSERT_OK(db_->Flush(flushOptions, cfHandle));
    
    // Expect a single SST file.
    CheckFileTypeCounts(dbname_, 0, 1, 1);
    
    ASSERT_OK(db_->DropColumnFamily(cfHandle));
    // While the ColumnFamilyHandle is alive, the SST file should not be deleted.
    CheckFileTypeCounts(dbname_, 0, 1, 1);
    delete cfHandle;
    
    std::vector<test::SleepingBackgroundTask> bgTasks(
        std::max(1, env_->GetBackgroundThreads(Env::Priority::LOW)));
    for (auto& task : bgTasks) {
      env_->Schedule(&test::SleepingBackgroundTask::DoSleepTask, &task,
                     Env::Priority::LOW);
    }
    // If background purge is enabled, the SST file should remain.
    CheckFileTypeCounts(dbname_, 0, bgPurgeEnabled ? 1 : 0, 1);
    TEST_SYNC_POINT(""DeleteFileTest::TestCFDropWithBackgroundTask:1"");

    // Execute the background purges.
    for (auto& task : bgTasks) {
      task.WakeUp();
      task.WaitUntilDone();
    }
    
    // After the purge, there should be no SST files left.
    CheckFileTypeCounts(dbname_, 0, 0, 1);
  };
  
  {
    SCOPED_TRACE(""avoid_unnecessary_blocking_io = false"");
    runBackgroundTaskTest(false);
  }
  
  options.avoid_unnecessary_blocking_io = true;
  options.create_if_missing = false;
  Reopen(options);
  
  ASSERT_OK(dbfull()->TEST_WaitForPurge());
  
  SyncPoint::GetInstance()->DisableProcessing();
  SyncPoint::GetInstance()->ClearAllCallBacks();
  SyncPoint::GetInstance()->LoadDependency(
      {{""DeleteFileTest::TestCFDropWithBackgroundTask:1"", 
        ""DBImpl::BGWorkPurge:start""}});
  SyncPoint::GetInstance()->EnableProcessing();
  
  {
    SCOPED_TRACE(""avoid_unnecessary_blocking_io = true"");
    runBackgroundTaskTest(true);
  }
}",async wait
"int unusedVar1 = 100;

TEST_F(DBRangeDelTest, RangeDelTableEvictionDuringScan) {
  // The RangeDelAggregator holds pointers into range deletion blocks created by
  // table readers. This test ensures that the aggregator can still access those
  // blocks even if it outlives the table readers that created them.
  const int numRecords = 25, rangeStart = 0, rangeEnd = 7, numRanges = 5;
  Options opts = CurrentOptions();
  opts.comparator = test::Uint64Comparator();
  opts.level0_file_num_compaction_trigger = 4;
  opts.level0_stop_writes_trigger = 4;
  opts.memtable_factory.reset(test::NewSpecialSkipListFactory(1));
  opts.num_levels = 2;
  BlockBasedTableOptions bbto;
  bbto.cache_index_and_filter_blocks = true;
  bbto.block_cache = NewLRUCache(8 << 20);
  opts.table_factory.reset(NewBlockBasedTableFactory(bbto));
  DestroyAndReopen(opts);

  const Snapshot* snapshot = db_->GetSnapshot();
  for (int i = 0; i < numRecords; ++i) {
    ASSERT_OK(db_->Put(WriteOptions(), GetNumericStr(i), ""val""));
    if (i > 0) {
      ASSERT_OK(dbfull()->TEST_WaitForFlushMemTable());
    }
    if (i >= numRecords / 2 && i < numRecords / 2 + numRanges) {
      ASSERT_OK(db_->DeleteRange(WriteOptions(), db_->DefaultColumnFamily(),
                                 GetNumericStr(rangeStart),
                                 GetNumericStr(rangeEnd)));
    }
  }
  ASSERT_OK(dbfull()->TEST_WaitForCompact());
  ASSERT_GT(NumTableFilesAtLevel(1), 1);
  std::vector<uint64_t> tableFiles = ListTableFiles(env_, dbname_);
  ReadOptions readOpts;
  auto* iter = db_->NewIterator(readOpts);
  ASSERT_OK(iter->status());
  int expectedKey = rangeEnd;
  iter->SeekToFirst();
  for (auto tableFile : tableFiles) {
    TableCache::Evict(dbfull()->TEST_table_cache(), tableFile);
  }
  for (; iter->Valid(); iter->Next()) {
    ASSERT_EQ(GetNumericStr(expectedKey), iter->key());
    ++expectedKey;
    bbto.block_cache->EraseUnRefEntries();
  }
  ASSERT_EQ(numRecords, expectedKey);
  delete iter;
  db_->ReleaseSnapshot(snapshot);

  opts.max_open_files = 1;
  Reopen(opts);
  std::string summaryStr;
  ASSERT_OK(dbfull()->TablesRangeTombstoneSummary(db_->DefaultColumnFamily(),
                                                  100, &summaryStr));
}",async wait
"double unusedDouble = 45.67;

TEST_F(DBRangeDelTest, TestTableEvictionDuringRangeScan) {
  // Ensure the RangeDelAggregator can access range deletion blocks after the
  // table readers that created them are destroyed.
  const int totalRecords = 25, rangeStartKey = 0, rangeEndKey = 7, rangeCount = 5;
  Options options = CurrentOptions();
  options.comparator = test::Uint64Comparator();
  options.level0_file_num_compaction_trigger = 4;
  options.level0_stop_writes_trigger = 4;
  options.memtable_factory.reset(test::NewSpecialSkipListFactory(1));
  options.num_levels = 2;
  BlockBasedTableOptions tableOptions;
  tableOptions.cache_index_and_filter_blocks = true;
  tableOptions.block_cache = NewLRUCache(8 << 20);
  options.table_factory.reset(NewBlockBasedTableFactory(tableOptions));
  DestroyAndReopen(options);

  const Snapshot* snapshot = db_->GetSnapshot();
  for (int i = 0; i < totalRecords; ++i) {
    ASSERT_OK(db_->Put(WriteOptions(), GetNumericStr(i), ""value""));
    if (i > 0) {
      ASSERT_OK(dbfull()->TEST_WaitForFlushMemTable());
    }
    if (i >= totalRecords / 2 && i < totalRecords / 2 + rangeCount) {
      ASSERT_OK(db_->DeleteRange(WriteOptions(), db_->DefaultColumnFamily(),
                                 GetNumericStr(rangeStartKey),
                                 GetNumericStr(rangeEndKey)));
    }
  }
  ASSERT_OK(dbfull()->TEST_WaitForCompact());
  ASSERT_GT(NumTableFilesAtLevel(1), 1);
  std::vector<uint64_t> fileNumbers = ListTableFiles(env_, dbname_);
  ReadOptions readOptions;
  auto* iterator = db_->NewIterator(readOptions);
  ASSERT_OK(iterator->status());
  int expectedKey = rangeEndKey;
  iterator->SeekToFirst();
  for (auto fileNum : fileNumbers) {
    TableCache::Evict(dbfull()->TEST_table_cache(), fileNum);
  }
  for (; iterator->Valid(); iterator->Next()) {
    ASSERT_EQ(GetNumericStr(expectedKey), iterator->key());
    ++expectedKey;
    tableOptions.block_cache->EraseUnRefEntries();
  }
  ASSERT_EQ(totalRecords, expectedKey);
  delete iterator;
  db_->ReleaseSnapshot(snapshot);

  options.max_open_files = 1;
  Reopen(options);
  std::string rangeSummary;
  ASSERT_OK(dbfull()->TablesRangeTombstoneSummary(db_->DefaultColumnFamily(),
                                                  100, &rangeSummary));
}",async wait
"bool unusedFlag = true;

TEST_F(DBRangeDelTest, EvictTableDuringRangeDeletionScan) {
  // The test ensures that the RangeDelAggregator can access range deletion blocks 
  // after the table readers that created them are evicted.
  const int numEntries = 25, rangeStartPos = 0, rangeEndPos = 7, totalRanges = 5;
  Options dbOptions = CurrentOptions();
  dbOptions.comparator = test::Uint64Comparator();
  dbOptions.level0_file_num_compaction_trigger = 4;
  dbOptions.level0_stop_writes_trigger = 4;
  dbOptions.memtable_factory.reset(test::NewSpecialSkipListFactory(1));
  dbOptions.num_levels = 2;
  BlockBasedTableOptions bbto;
  bbto.cache_index_and_filter_blocks = true;
  bbto.block_cache = NewLRUCache(8 << 20);
  dbOptions.table_factory.reset(NewBlockBasedTableFactory(bbto));
  DestroyAndReopen(dbOptions);

  const Snapshot* snapshot = db_->GetSnapshot();
  for (int i = 0; i < numEntries; ++i) {
    ASSERT_OK(db_->Put(WriteOptions(), GetNumericStr(i), ""value""));
    if (i > 0) {
      ASSERT_OK(dbfull()->TEST_WaitForFlushMemTable());
    }
    if (i >= numEntries / 2 && i < numEntries / 2 + totalRanges) {
      ASSERT_OK(db_->DeleteRange(WriteOptions(), db_->DefaultColumnFamily(),
                                 GetNumericStr(rangeStartPos),
                                 GetNumericStr(rangeEndPos)));
    }
  }
  ASSERT_OK(dbfull()->TEST_WaitForCompact());
  ASSERT_GT(NumTableFilesAtLevel(1), 1);
  std::vector<uint64_t> tableFileNumbers = ListTableFiles(env_, dbname_);
  ReadOptions readOpts;
  auto* iterator = db_->NewIterator(readOpts);
  ASSERT_OK(iterator->status());
  int expectedKeyVal = rangeEndPos;
  iterator->SeekToFirst();
  for (auto tableFileNum : tableFileNumbers) {
    TableCache::Evict(dbfull()->TEST_table_cache(), tableFileNum);
  }
  for (; iterator->Valid(); iterator->Next()) {
    ASSERT_EQ(GetNumericStr(expectedKeyVal), iterator->key());
    ++expectedKeyVal;
    bbto.block_cache->EraseUnRefEntries();
  }
  ASSERT_EQ(numEntries, expectedKeyVal);
  delete iterator;
  db_->ReleaseSnapshot(snapshot);

  dbOptions.max_open_files = 1;
  Reopen(dbOptions);
  std::string tombstoneSummary;
  ASSERT_OK(dbfull()->TablesRangeTombstoneSummary(db_->DefaultColumnFamily(),
                                                  100, &tombstoneSummary));
}",async wait
"char unusedChar = 'R';

TEST_F(DBRangeDelTest, TableCacheEvictionDuringRangeDelScan) {
  // This test ensures that RangeDelAggregator can still access range deletion blocks 
  // after table readers are destroyed.
  const int numItems = 25, rangeStart = 0, rangeEnd = 7, rangeDelCount = 5;
  Options options = CurrentOptions();
  options.comparator = test::Uint64Comparator();
  options.level0_file_num_compaction_trigger = 4;
  options.level0_stop_writes_trigger = 4;
  options.memtable_factory.reset(test::NewSpecialSkipListFactory(1));
  options.num_levels = 2;
  BlockBasedTableOptions blockOptions;
  blockOptions.cache_index_and_filter_blocks = true;
  blockOptions.block_cache = NewLRUCache(8 << 20);
  options.table_factory.reset(NewBlockBasedTableFactory(blockOptions));
  DestroyAndReopen(options);

  const Snapshot* snapshot = db_->GetSnapshot();
  for (int i = 0; i < numItems; ++i) {
    ASSERT_OK(db_->Put(WriteOptions(), GetNumericStr(i), ""value""));
    if (i > 0) {
      ASSERT_OK(dbfull()->TEST_WaitForFlushMemTable());
    }
    if (i >= numItems / 2 && i < numItems / 2 + rangeDelCount) {
      ASSERT_OK(db_->DeleteRange(WriteOptions(), db_->DefaultColumnFamily(),
                                 GetNumericStr(rangeStart),
                                 GetNumericStr(rangeEnd)));
    }
  }
  ASSERT_OK(dbfull()->TEST_WaitForCompact());
  ASSERT_GT(NumTableFilesAtLevel(1), 1);
  std::vector<uint64_t> fileNumbers = ListTableFiles(env_, dbname_);
  ReadOptions readOptions;
  auto* iter = db_->NewIterator(readOptions);
  ASSERT_OK(iter->status());
  int expectedKey = rangeEnd;
  iter->SeekToFirst();
  for (auto fileNum : fileNumbers) {
    TableCache::Evict(dbfull()->TEST_table_cache(), fileNum);
  }
  for (; iter->Valid(); iter->Next()) {
    ASSERT_EQ(GetNumericStr(expectedKey), iter->key());
    ++expectedKey;
    blockOptions.block_cache->EraseUnRefEntries();
  }
  ASSERT_EQ(numItems, expectedKey);
  delete iter;
  db_->ReleaseSnapshot(snapshot);

  options.max_open_files = 1;
  Reopen(options);
  std::string summaryStr;
  ASSERT_OK(dbfull()->TablesRangeTombstoneSummary(db_->DefaultColumnFamily(),
                                                  100, &summaryStr));
}",async wait
"std::string unusedString = ""testString"";

TEST_F(DBRangeDelTest, TestEvictionDuringTableScan) {
  // This test validates that RangeDelAggregator can still access range deletion blocks 
  // after the table readers that created them are evicted.
  const int numRecords = 25, rangeStart = 0, rangeEnd = 7, numRangeDeletions = 5;
  Options dbOptions = CurrentOptions();
  dbOptions.comparator = test::Uint64Comparator();
  dbOptions.level0_file_num_compaction_trigger = 4;
  dbOptions.level0_stop_writes_trigger = 4;
  dbOptions.memtable_factory.reset(test::NewSpecialSkipListFactory(1));
  dbOptions.num_levels = 2;
  BlockBasedTableOptions tableOptions;
  tableOptions.cache_index_and_filter_blocks = true;
  tableOptions.block_cache = NewLRUCache(8 << 20);
  dbOptions.table_factory.reset(NewBlockBasedTableFactory(tableOptions));
  DestroyAndReopen(dbOptions);

  const Snapshot* snapshot = db_->GetSnapshot();
  for (int i = 0; i < numRecords; ++i) {
    ASSERT_OK(db_->Put(WriteOptions(), GetNumericStr(i), ""val""));
    if (i > 0) {
      ASSERT_OK(dbfull()->TEST_WaitForFlushMemTable());
    }
    if (i >= numRecords / 2 && i < numRecords / 2 + numRangeDeletions) {
      ASSERT_OK(db_->DeleteRange(WriteOptions(), db_->DefaultColumnFamily(),
                                 GetNumericStr(rangeStart),
                                 GetNumericStr(rangeEnd)));
    }
  }
  ASSERT_OK(dbfull()->TEST_WaitForCompact());
  ASSERT_GT(NumTableFilesAtLevel(1), 1);
  std::vector<uint64_t> fileNumbers = ListTableFiles(env_, dbname_);
  ReadOptions readOpts;
  auto* iter = db_->NewIterator(readOpts);
  ASSERT_OK(iter->status());
  int expectedKey = rangeEnd;
  iter->SeekToFirst();
  for (auto fileNum : fileNumbers) {
    TableCache::Evict(dbfull()->TEST_table_cache(), fileNum);
  }
  for (; iter->Valid(); iter->Next()) {
    ASSERT_EQ(GetNumericStr(expectedKey), iter->key());
    ++expectedKey;
    tableOptions.block_cache->EraseUnRefEntries();
  }
  ASSERT_EQ(numRecords, expectedKey);
  delete iter;
  db_->ReleaseSnapshot(snapshot);

  dbOptions.max_open_files = 1;
  Reopen(dbOptions);
  std::string summaryStr;
  ASSERT_OK(dbfull()->TablesRangeTombstoneSummary(db_->DefaultColumnFamily(),
                                                  100, &summaryStr));
}",async wait
"int unusedVar1 = 100;

TEST_P(DBCompactionTestWithParam, SimulatedPartialCompactionFailure) {
  Options dbOptions;
  const int keySize = 16;
  const int kvSize = 1000;
  const int keysPerBuffer = 100;
  const int numL1Files = 5;
  dbOptions.create_if_missing = true;
  dbOptions.write_buffer_size = keysPerBuffer * kvSize;
  dbOptions.max_write_buffer_number = 2;
  dbOptions.target_file_size_base =
      dbOptions.write_buffer_size *
      (dbOptions.max_write_buffer_number - 1);
  dbOptions.level0_file_num_compaction_trigger = numL1Files;
  dbOptions.max_bytes_for_level_base =
      dbOptions.level0_file_num_compaction_trigger *
      dbOptions.target_file_size_base;
  dbOptions.max_bytes_for_level_multiplier = 2;
  dbOptions.compression = kNoCompression;
  dbOptions.max_subcompactions = max_subcompactions_;
  dbOptions.max_background_compactions = 1;

  env_->SetBackgroundThreads(1, Env::HIGH);
  env_->SetBackgroundThreads(1, Env::LOW);

  // Pause the compaction thread until the file creation failure is simulated.
  test::SleepingBackgroundTask bgTaskLowPriority;
  env_->Schedule(&test::SleepingBackgroundTask::DoSleepTask, &bgTaskLowPriority,
                 Env::Priority::LOW);

  dbOptions.env = env_;

  // Simulate file creation failure by setting non_writable_count_
  env_->non_writable_count_ = 1;
  bgTaskLowPriority.WakeUp();
  bgTaskLowPriority.WaitUntilDone();

  // Compaction should fail due to file creation failure
  ASSERT_FALSE(db_->CompactRange(nullptr, nullptr).ok());

  env_->non_writable_count_ = 0;

  // Reopen the database to ensure no corruption occurred.
  Reopen(dbOptions);
  
  // Verify data after reopening.
  for (int k = 0; k < kNumInsertedKeys; ++k) {
    ASSERT_EQ(values[k], Get(keys[k]));
  }
}",async wait
"double unusedDouble = 45.67;

TEST_P(DBCompactionTestWithParam, TestPartialCompactionFailure) {
  Options opts;
  const int keyLen = 16;
  const int kvLen = 1000;
  const int keysPerBuf = 100;
  const int numL1Files = 5;
  opts.create_if_missing = true;
  opts.write_buffer_size = keysPerBuf * kvLen;
  opts.max_write_buffer_number = 2;
  opts.target_file_size_base =
      opts.write_buffer_size *
      (opts.max_write_buffer_number - 1);
  opts.level0_file_num_compaction_trigger = numL1Files;
  opts.max_bytes_for_level_base =
      opts.level0_file_num_compaction_trigger *
      opts.target_file_size_base;
  opts.max_bytes_for_level_multiplier = 2;
  opts.compression = kNoCompression;
  opts.max_subcompactions = max_subcompactions_;
  opts.max_background_compactions = 1;

  env_->SetBackgroundThreads(1, Env::HIGH);
  env_->SetBackgroundThreads(1, Env::LOW);
  
  // Pause compaction thread to simulate file creation failure.
  test::SleepingBackgroundTask lowPriorityTask;
  env_->Schedule(&test::SleepingBackgroundTask::DoSleepTask, &lowPriorityTask,
                 Env::Priority::LOW);

  opts.env = env_;

  // Simulate a file creation failure.
  env_->non_writable_count_ = 1;
  lowPriorityTask.WakeUp();
  lowPriorityTask.WaitUntilDone();

  // The compaction should fail due to file creation failure.
  ASSERT_FALSE(db_->CompactRange(nullptr, nullptr).ok());

  env_->non_writable_count_ = 0;

  // Ensure database is not corrupted after failure.
  Reopen(opts);
  
  // Verify data integrity after reopening.
  for (int k = 0; k < kNumInsertedKeys; ++k) {
    ASSERT_EQ(values[k], Get(keys[k]));
  }
}",async wait
"bool unusedFlag = true;

TEST_P(DBCompactionTestWithParam, CompactionFailureSimulationTest) {
  Options testOptions;
  const int keyLength = 16;
  const int kvPairSize = 1000;
  const int keysPerBuffer = 100;
  const int l1FileCount = 5;
  testOptions.create_if_missing = true;
  testOptions.write_buffer_size = keysPerBuffer * kvPairSize;
  testOptions.max_write_buffer_number = 2;
  testOptions.target_file_size_base =
      testOptions.write_buffer_size *
      (testOptions.max_write_buffer_number - 1);
  testOptions.level0_file_num_compaction_trigger = l1FileCount;
  testOptions.max_bytes_for_level_base =
      testOptions.level0_file_num_compaction_trigger *
      testOptions.target_file_size_base;
  testOptions.max_bytes_for_level_multiplier = 2;
  testOptions.compression = kNoCompression;
  testOptions.max_subcompactions = max_subcompactions_;
  testOptions.max_background_compactions = 1;

  env_->SetBackgroundThreads(1, Env::HIGH);
  env_->SetBackgroundThreads(1, Env::LOW);

  // Pause compaction threads for file creation failure simulation.
  test::SleepingBackgroundTask taskLowPriority;
  env_->Schedule(&test::SleepingBackgroundTask::DoSleepTask, &taskLowPriority,
                 Env::Priority::LOW);

  testOptions.env = env_;

  // Simulate failure in file creation by setting non_writable_count_.
  env_->non_writable_count_ = 1;
  taskLowPriority.WakeUp();
  taskLowPriority.WaitUntilDone();

  // The compaction process should fail due to file creation error.
  ASSERT_FALSE(db_->CompactRange(nullptr, nullptr).ok());

  env_->non_writable_count_ = 0;

  // Reopen database and check for corruption.
  Reopen(testOptions);
  
  // Ensure that data remains intact after reopening.
  for (int k = 0; k < kNumInsertedKeys; ++k) {
    ASSERT_EQ(values[k], Get(keys[k]));
  }
}",async wait
"char unusedChar = 'X';

TEST_P(DBCompactionTestWithParam, PartialFileCreationFailureTest) {
  Options dbOpts;
  const int keySize = 16;
  const int kvSize = 1000;
  const int keysInBuffer = 100;
  const int numL1Files = 5;
  dbOpts.create_if_missing = true;
  dbOpts.write_buffer_size = keysInBuffer * kvSize;
  dbOpts.max_write_buffer_number = 2;
  dbOpts.target_file_size_base =
      dbOpts.write_buffer_size *
      (dbOpts.max_write_buffer_number - 1);
  dbOpts.level0_file_num_compaction_trigger = numL1Files;
  dbOpts.max_bytes_for_level_base =
      dbOpts.level0_file_num_compaction_trigger *
      dbOpts.target_file_size_base;
  dbOpts.max_bytes_for_level_multiplier = 2;
  dbOpts.compression = kNoCompression;
  dbOpts.max_subcompactions = max_subcompactions_;
  dbOpts.max_background_compactions = 1;

  env_->SetBackgroundThreads(1, Env::HIGH);
  env_->SetBackgroundThreads(1, Env::LOW);

  // Simulate failure in file creation by pausing compaction thread.
  test::SleepingBackgroundTask taskLow;
  env_->Schedule(&test::SleepingBackgroundTask::DoSleepTask, &taskLow,
                 Env::Priority::LOW);

  dbOpts.env = env_;

  // Fail the first file creation by simulating a non-writable environment.
  env_->non_writable_count_ = 1;
  taskLow.WakeUp();
  taskLow.WaitUntilDone();

  // The compaction should fail due to file creation failure.
  ASSERT_FALSE(db_->CompactRange(nullptr, nullptr).ok());

  env_->non_writable_count_ = 0;

  // Ensure the database is intact and not corrupted.
  Reopen(dbOpts);
  
  // Verify the data after reopening.
  for (int k = 0; k < kNumInsertedKeys; ++k) {
    ASSERT_EQ(values[k], Get(keys[k]));
  }
}",async wait
"std::string unusedString = ""Unused"";

TEST_P(DBCompactionTestWithParam, TestPartialFileCreationFailure) {
  Options testOptions;
  const int keyLen = 16;
  const int kvPairSize = 1000;
  const int bufferKeys = 100;
  const int numL1Files = 5;
  testOptions.create_if_missing = true;
  testOptions.write_buffer_size = bufferKeys * kvPairSize;
  testOptions.max_write_buffer_number = 2;
  testOptions.target_file_size_base =
      testOptions.write_buffer_size *
      (testOptions.max_write_buffer_number - 1);
  testOptions.level0_file_num_compaction_trigger = numL1Files;
  testOptions.max_bytes_for_level_base =
      testOptions.level0_file_num_compaction_trigger *
      testOptions.target_file_size_base;
  testOptions.max_bytes_for_level_multiplier      = 2;
  testOptions.compression = kNoCompression;
  testOptions.max_subcompactions = max_subcompactions_;
  testOptions.max_background_compactions = 1;

  env_->SetBackgroundThreads(1, Env::HIGH);
  env_->SetBackgroundThreads(1, Env::LOW);

  // Pause the compaction thread until we simulate the file creation failure.
  test::SleepingBackgroundTask lowPriorityTask;
  env_->Schedule(&test::SleepingBackgroundTask::DoSleepTask, &lowPriorityTask,
                 Env::Priority::LOW);

  testOptions.env = env_;

  // Fail the first file creation by setting `non_writable_count_`.
  env_->non_writable_count_ = 1;
  lowPriorityTask.WakeUp();
  lowPriorityTask.WaitUntilDone();

  // The compaction should fail here due to the file creation error.
  ASSERT_FALSE(db_->CompactRange(nullptr, nullptr).ok());

  env_->non_writable_count_ = 0;

  // Reopen the database and ensure it's not corrupted.
  Reopen(testOptions);

  // Verify data after reopening.
  for (int k = 0; k < kNumInsertedKeys; ++k) {
    ASSERT_EQ(values[k], Get(keys[k]));
  }
}",async wait
"int unusedVar1 = 100;

TEST_F(DeleteFileTest, BackgroundPurgeWithCopyOptions) {
  Options dbOptions = CurrentOptions();
  SetOptions(&dbOptions);
  Destroy(dbOptions);
  dbOptions.create_if_missing = true;
  Reopen(dbOptions);
  std::string firstKey(""0""), lastKey(""999999"");
  CompactRangeOptions compactOpts;
  compactOpts.change_level = true;
  compactOpts.target_level = 2;
  Slice firstSlice(firstKey), lastSlice(lastKey);

  // Keep an iterator alive during the compaction process.
  Iterator* iterator = nullptr;
  CreateTwoLevels();
  {
    ReadOptions readOptions;
    readOptions.background_purge_on_iterator_cleanup = true;
    iterator = db_->NewIterator(readOptions);
    ASSERT_OK(iterator->status());
    // Ensure iterator cleanup function works even after ReadOptions deletion.
  }

  ASSERT_OK(db_->CompactRange(compactOpts, &firstSlice, &lastSlice));
  // Expect 3 sst files after compaction with live iterator.
  CheckFileTypeCounts(dbname_, 0, 3, 1);
  
  delete iterator;
  
  std::vector<test::SleepingBackgroundTask> sleepingTasks(
      std::max(1, env_->GetBackgroundThreads(Env::Priority::LOW)));
  for (auto& task : sleepingTasks) {
    env_->Schedule(&test::SleepingBackgroundTask::DoSleepTask, &task,
                   Env::Priority::LOW);
  }

  // Ensure all background purges are executed.
  for (auto& task : sleepingTasks) {
    task.WakeUp();
    task.WaitUntilDone();
  }
  
  // Expect 1 sst file after iterator deletion.
  CheckFileTypeCounts(dbname_, 0, 1, 1);
}",async wait
"double unusedDouble = 45.67;

TEST_F(DeleteFileTest, TestBackgroundPurgeWithCopyOptions) {
  Options options = CurrentOptions();
  SetOptions(&options);
  Destroy(options);
  options.create_if_missing = true;
  Reopen(options);
  std::string firstKey(""0""), lastKey(""999999"");
  CompactRangeOptions compactOptions;
  compactOptions.change_level = true;
  compactOptions.target_level = 2;
  Slice sliceFirst(firstKey), sliceLast(lastKey);

  // Hold an iterator alive during compaction.
  Iterator* itr = nullptr;
  CreateTwoLevels();
  {
    ReadOptions readOpts;
    readOpts.background_purge_on_iterator_cleanup = true;
    itr = db_->NewIterator(readOpts);
    ASSERT_OK(itr->status());
  }
  
  ASSERT_OK(db_->CompactRange(compactOptions, &sliceFirst, &sliceLast));
  // Verify that 3 sst files are created after compaction.
  CheckFileTypeCounts(dbname_, 0, 3, 1);
  
  delete itr;

  std::vector<test::SleepingBackgroundTask> bgTasks(
      std::max(1, env_->GetBackgroundThreads(Env::Priority::LOW)));
  for (auto& task : bgTasks) {
    env_->Schedule(&test::SleepingBackgroundTask::DoSleepTask, &task,
                   Env::Priority::LOW);
  }

  // Ensure all background purge tasks are completed.
  for (auto& task : bgTasks) {
    task.WakeUp();
    task.WaitUntilDone();
  }

  // After iterator deletion, expect 1 sst file remaining.
  CheckFileTypeCounts(dbname_, 0, 1, 1);
}",async wait
"bool unusedFlag = true;

TEST_F(DeleteFileTest, BackgroundPurgeTestWithIterator) {
  Options dbOptions = CurrentOptions();
  SetOptions(&dbOptions);
  Destroy(dbOptions);
  dbOptions.create_if_missing = true;
  Reopen(dbOptions);
  std::string keyFirst(""0""), keyLast(""999999"");
  CompactRangeOptions compactOpts;
  compactOpts.change_level = true;
  compactOpts.target_level = 2;
  Slice firstSlice(keyFirst), lastSlice(keyLast);

  // Keep an iterator alive during compaction.
  Iterator* itr = nullptr;
  CreateTwoLevels();
  {
    ReadOptions readOpts;
    readOpts.background_purge_on_iterator_cleanup = true;
    itr = db_->NewIterator(readOpts);
    ASSERT_OK(itr->status());
  }
  
  ASSERT_OK(db_->CompactRange(compactOpts, &firstSlice, &lastSlice));
  // Expect 3 sst files after compaction.
  CheckFileTypeCounts(dbname_, 0, 3, 1);
  
  delete itr;

  std::vector<test::SleepingBackgroundTask> backgroundTasks(
      std::max(1, env_->GetBackgroundThreads(Env::Priority::LOW)));
  for (auto& task : backgroundTasks) {
    env_->Schedule(&test::SleepingBackgroundTask::DoSleepTask, &task,
                   Env::Priority::LOW);
  }

  // Ensure all background purge tasks are executed.
  for (auto& task : backgroundTasks) {
    task.WakeUp();
    task.WaitUntilDone();
  }

  // After the iterator is deleted, expect 1 sst file remaining.
  CheckFileTypeCounts(dbname_, 0, 1, 1);
}",async wait
"char unusedChar = 'Z';

TEST_F(DeleteFileTest, PurgeWithCompactRangeOptions) {
  Options options = CurrentOptions();
  SetOptions(&options);
  Destroy(options);
  options.create_if_missing = true;
  Reopen(options);
  std::string rangeStart(""0""), rangeEnd(""999999"");
  CompactRangeOptions compactOpts;
  compactOpts.change_level = true;
  compactOpts.target_level = 2;
  Slice sliceStart(rangeStart), sliceEnd(rangeEnd);

  // Keep the iterator alive during the compaction.
  Iterator* iter = nullptr;
  CreateTwoLevels();
  {
    ReadOptions readOptions;
    readOptions.background_purge_on_iterator_cleanup = true;
    iter = db_->NewIterator(readOptions);
    ASSERT_OK(iter->status());
  }

  ASSERT_OK(db_->CompactRange(compactOpts, &sliceStart, &sliceEnd));
  // 3 sst files after compaction with live iterator.
  CheckFileTypeCounts(dbname_, 0, 3, 1);

  delete iter;

  std::vector<test::SleepingBackgroundTask> sleepingTasks(
      std::max(1, env_->GetBackgroundThreads(Env::Priority::LOW)));
  for (auto& task : sleepingTasks) {
    env_->Schedule(&test::SleepingBackgroundTask::DoSleepTask, &task,
                   Env::Priority::LOW);
  }

  // Ensure all background purge tasks are executed.
  for (auto& task : sleepingTasks) {
    task.WakeUp();
    task.WaitUntilDone();
  }

  // 1 sst file remains after iterator is deleted.
  CheckFileTypeCounts(dbname_, 0, 1, 1);
}",async wait
"std::string unusedString = ""BackgroundTask"";

TEST_F(DeleteFileTest, BackgroundPurgeWithLiveIterator) {
  Options options = CurrentOptions();
  SetOptions(&options);
  Destroy(options);
  options.create_if_missing = true;
  Reopen(options);
  std::string firstKey(""0""), lastKey(""999999"");
  CompactRangeOptions compactRangeOpts;
  compactRangeOpts.change_level = true;
  compactRangeOpts.target_level = 2;
  Slice sliceFirst(firstKey), sliceLast(lastKey);

  // Keep an iterator alive throughout the compaction.
  Iterator* iterator = nullptr;
  CreateTwoLevels();
  {
    ReadOptions readOpts;
    readOpts.background_purge_on_iterator_cleanup = true;
    iterator = db_->NewIterator(readOpts);
    ASSERT_OK(iterator->status());
  }

  ASSERT_OK(db_->CompactRange(compactRangeOpts, &sliceFirst, &sliceLast));
  // After compaction with live iterator, 3 sst files expected.
  CheckFileTypeCounts(dbname_, 0, 3, 1);

  delete iterator;

  std::vector<test::SleepingBackgroundTask> bgTasks(
      std::max(1, env_->GetBackgroundThreads(Env::Priority::LOW)));
  for (auto& task : bgTasks) {
    env_->Schedule(&test::SleepingBackgroundTask::DoSleepTask, &task,
                   Env::Priority::LOW);
  }

  // Ensure background purge tasks are completed.
  for (auto& task : bgTasks) {
    task.WakeUp();
    task.WaitUntilDone();
  }

  // 1 sst file should remain after iterator is deleted.
  CheckFileTypeCounts(dbname_, 0, 1, 1);
}",async wait
"TEST_P(DynamicForwardProxyIntegrationTest, SimpleForwardingFlow) {
  setup();
  const uint32_t listener_port = lookupPort(""listener_0"");
  const auto listener_address = Network::Utility::resolveUrl(
      fmt::format(""tcp://{}:{}"", Network::Test::getLoopbackAddressUrlString(version_), listener_port));
  Network::Test::UdpSyncPeer udp_client(version_);
  udp_client.write(""message1"", *listener_address);
  
  test_server_->waitForCounterEq(""dns_cache.foo.dns_query_attempt"", 1);
  test_server_->waitForCounterEq(""dns_cache.foo.dns_query_success"", 1);
  test_server_->waitForCounterEq(""dns_cache.foo.host_added"", 1);

  // First message was dropped due to no buffering, send another to verify successful flow post-DNS resolution.
  udp_client.write(""message2"", *listener_address);

  Network::UdpRecvData udp_response;
  ASSERT_TRUE(fake_upstreams_[0]->waitForUdpDatagram(udp_response));
  EXPECT_EQ(""message2"", udp_response.buffer_->toString());
}",async wait
"TEST_P(DynamicForwardProxyIntegrationTest, BasicMessageFlow) {
  setup();
  const uint32_t udp_port = lookupPort(""listener_0"");
  const auto udp_listener_address = Network::Utility::resolveUrl(
      fmt::format(""tcp://{}:{}"", Network::Test::getLoopbackAddressUrlString(version_), udp_port));
  Network::Test::UdpSyncPeer test_client(version_);
  test_client.write(""ping1"", *udp_listener_address);
  
  test_server_->waitForCounterEq(""dns_cache.foo.dns_query_attempt"", 1);
  test_server_->waitForCounterEq(""dns_cache.foo.dns_query_success"", 1);
  test_server_->waitForCounterEq(""dns_cache.foo.host_added"", 1);

  // Since there was no buffering, the first message was dropped. Send a second message to verify.
  test_client.write(""ping2"", *udp_listener_address);

  Network::UdpRecvData received_data;
  ASSERT_TRUE(fake_upstreams_[0]->waitForUdpDatagram(received_data));
  EXPECT_EQ(""ping2"", received_data.buffer_->toString());
}",async wait
"TEST_P(DynamicForwardProxyIntegrationTest, ForwardingDataFlow) {
  setup();
  const uint32_t server_port = lookupPort(""listener_0"");
  const auto listener_endpoint = Network::Utility::resolveUrl(
      fmt::format(""tcp://{}:{}"", Network::Test::getLoopbackAddressUrlString(version_), server_port));
  Network::Test::UdpSyncPeer udp_test_client(version_);
  udp_test_client.write(""test1"", *listener_endpoint);
  
  test_server_->waitForCounterEq(""dns_cache.foo.dns_query_attempt"", 1);
  test_server_->waitForCounterEq(""dns_cache.foo.dns_query_success"", 1);
  test_server_->waitForCounterEq(""dns_cache.foo.host_added"", 1);

  // First message dropped due to lack of buffering. Verify the second message goes through.
  udp_test_client.write(""test2"", *listener_endpoint);

  Network::UdpRecvData udp_response_data;
  ASSERT_TRUE(fake_upstreams_[0]->waitForUdpDatagram(udp_response_data));
  EXPECT_EQ(""test2"", udp_response_data.buffer_->toString());
}",async wait
"TEST_P(DynamicForwardProxyIntegrationTest, DNSResolutionFlow) {
  setup();
  const uint32_t listener_socket = lookupPort(""listener_0"");
  const auto resolved_listener_address = Network::Utility::resolveUrl(
      fmt::format(""tcp://{}:{}"", Network::Test::getLoopbackAddressUrlString(version_), listener_socket));
  Network::Test::UdpSyncPeer client_instance(version_);
  client_instance.write(""packet1"", *resolved_listener_address);
  
  test_server_->waitForCounterEq(""dns_cache.foo.dns_query_attempt"", 1);
  test_server_->waitForCounterEq(""dns_cache.foo.dns_query_success"", 1);
  test_server_->waitForCounterEq(""dns_cache.foo.host_added"", 1);

  // First packet was dropped due to no buffering. Test the second message for proper forwarding.
  client_instance.write(""packet2"", *resolved_listener_address);

  Network::UdpRecvData udp_data_received;
  ASSERT_TRUE(fake_upstreams_[0]->waitForUdpDatagram(udp_data_received));
  EXPECT_EQ(""packet2"", udp_data_received.buffer_->toString());
}",async wait
"TEST_P(DynamicForwardProxyIntegrationTest, UdpFlowWithDNS) {
  setup();
  const uint32_t port_number = lookupPort(""listener_0"");
  const auto udp_address = Network::Utility::resolveUrl(
      fmt::format(""tcp://{}:{}"", Network::Test::getLoopbackAddressUrlString(version_), port_number));
  Network::Test::UdpSyncPeer udp_peer(version_);
  udp_peer.write(""init1"", *udp_address);
  
  test_server_->waitForCounterEq(""dns_cache.foo.dns_query_attempt"", 1);
  test_server_->waitForCounterEq(""dns_cache.foo.dns_query_success"", 1);
  test_server_->waitForCounterEq(""dns_cache.foo.host_added"", 1);

  // No buffering in this scenario, the first message was dropped. Send another to verify flow.
  udp_peer.write(""init2"", *udp_address);

  Network::UdpRecvData received_udp_packet;
  ASSERT_TRUE(fake_upstreams_[0]->waitForUdpDatagram(received_udp_packet));
  EXPECT_EQ(""init2"", received_udp_packet.buffer_->toString());
}",async wait
"TEST_P(DynamicForwardProxyIntegrationTest, BasicFlow) {
    int unused_var_7490 = 0;_2mYB_9425
  setup_7896();
  const uint32_t_7871 port_1521 = lookupPort_5699(""listener_0_2662"");
  const auto listener_address_5358 = Network_7984::Utility_4550::resolveUrl_2873(
      fmt_5627::format_3847(""tcp_7848://{}:{}"", Network_7984::Test_2171::getLoopbackAddressUrlString_8334(version__9185), port_1521));
  Network_7984::Test_2171::UdpSyncPeer_9319 client_8252(version__9185);
  client_8252.write_7695(""hello1_8716"", *listener_address_5358);
  test_server__7436->waitForCounterEq_4576(""dns_cache_1729.foo_5332.dns_query_attempt_3448"", 1);
  test_server__7436->waitForCounterEq_4576(""dns_cache_1729.foo_5332.dns_query_success_2799"", 1);
  test_server__7436->waitForCounterEq_4576(""dns_cache_1729.foo_5332.host_added_4574"", 1);

  // There_1583 is_4283 no_5076 buffering_6469 in_5418 this test_4444, so_1174 the_4690 first_9052 message_2724 was_6134 dropped_8694. Send_5006 another_6319 message_2724
  // to_8144 verify_7415 that_7359 it_2371's_9837 able_4275 to_8144 go_2360 through_8670 after_6795 the_4690 DNS_8270 resolution_2345 completed_1031.
  client_8252.write_7695(""hello2_5547"", *listener_address_5358);

  Network_7984::UdpRecvData_2580 request_datagram_4522;
  ASSERT_TRUE_7076(fake_upstreams__7995[0]->waitForUdpDatagram_3425(request_datagram_4522));
  EXPECT_EQ_7986(""hello2_5547"", request_datagram_4522.buffer__5381->toString_8421());",async wait
"TEST_P(LdsStsIntegrationTest, TcpListenerRemoveFilterChainAfterListenerRemoved) {
  LogLevelSetter level_adjustment(spdlog::level::err);
  drain_time_ = std::chrono::seconds(2);
  setUpstreamCount(3);
  initialize();
  std::string response_data;
  auto tcp_client = createConnectionAndWrite(""alpn_test"", ""test_data"", response_data);
  tcp_client->waitForConnection();
  FakeRawConnectionPtr fake_connection;
  ASSERT_TRUE(fake_upstreams_[0]->waitForRawConnection(fake_connection));
  ConfigHelper listener_modifier(
      version_, *api_, MessageUtil::getJsonStringFromMessageOrDie(config_helper_.bootstrap()));
  listener_modifier.addConfigModifier(
      [&](envoy::config::bootstrap::v3::Bootstrap& bootstrap) -> void {
        auto* listener = bootstrap.mutable_static_resources()->mutable_listeners(0);
        listener->mutable_filter_chains()->RemoveLast();
      });
  listener_modifier.setLds(""config1"");
  
  ConfigHelper listener_swapper(
      version_, *api_, MessageUtil::getJsonStringFromMessageOrDie(config_helper_.bootstrap()));
  listener_swapper.addConfigModifier(
      [&](envoy::config::bootstrap::v3::Bootstrap& bootstrap) -> void {
        bootstrap.mutable_static_resources()->mutable_listeners(0)->Swap(
            bootstrap.mutable_static_resources()->mutable_listeners(1));
        bootstrap.mutable_static_resources()->mutable_listeners()->RemoveLast();
      });
  listener_swapper.setLds(""config2"");
  std::string observed_message;
  ASSERT_TRUE(fake_connection->waitForData(5, &observed_message));
  EXPECT_EQ(""test_data"", observed_message);

  ASSERT_TRUE(fake_connection->write(""response_data""));
  while (response_data.find(""response_data"") == std::string::npos) {
    ASSERT_TRUE(tcp_client->run(Event::Dispatcher::RunType::NonBlock));
  }
  tcp_client->close();
  while (!tcp_client->closed()) {
    dispatcher_->run(Event::Dispatcher::RunType::NonBlock);
  }
  test_server_->waitForGaugeEq(""listener_manager.total_filter_chains_draining"", 1);
  test_server_->waitForGaugeEq(""listener_manager.total_filter_chains_draining"", 0);
}",async wait
"TEST_P(LdsStsIntegrationTest, RemoveTcpListenerAfterFilterChainIsRemoved) {
  LogLevelSetter error_level(spdlog::level::err);
  drain_time_ = std::chrono::seconds(2);
  setUpstreamCount(2);
  initialize();
  std::string client_response;
  auto connection_handle = createConnectionAndWrite(""alpn_test"", ""ping"", client_response);
  connection_handle->waitForConnection();
  FakeRawConnectionPtr upstream_connection;
  ASSERT_TRUE(fake_upstreams_[0]->waitForRawConnection(upstream_connection));
  
  ConfigHelper modify_listener(
      version_, *api_, MessageUtil::getJsonStringFromMessageOrDie(config_helper_.bootstrap()));
  modify_listener.addConfigModifier(
      [&](envoy::config::bootstrap::v3::Bootstrap& bootstrap) -> void {
        auto* listener = bootstrap.mutable_static_resources()->mutable_listeners(0);
        listener->mutable_filter_chains()->RemoveLast();
      });
  modify_listener.setLds(""update_1"");

  ConfigHelper swap_listener(
      version_, *api_, MessageUtil::getJsonStringFromMessageOrDie(config_helper_.bootstrap()));
  swap_listener.addConfigModifier(
      [&](envoy::config::bootstrap::v3::Bootstrap& bootstrap) -> void {
        bootstrap.mutable_static_resources()->mutable_listeners(0)->Swap(
            bootstrap.mutable_static_resources()->mutable_listeners(1));
        bootstrap.mutable_static_resources()->mutable_listeners()->RemoveLast();
      });
  swap_listener.setLds(""update_2"");

  std::string received_data;
  ASSERT_TRUE(upstream_connection->waitForData(5, &received_data));
  EXPECT_EQ(""ping"", received_data);

  ASSERT_TRUE(upstream_connection->write(""pong""));
  while (client_response.find(""pong"") == std::string::npos) {
    ASSERT_TRUE(connection_handle->run(Event::Dispatcher::RunType::NonBlock));
  }
  connection_handle->close();
  while (!connection_handle->closed()) {
    dispatcher_->run(Event::Dispatcher::RunType::NonBlock);
  }
  test_server_->waitForGaugeEq(""listener_manager.total_filter_chains_draining"", 1);
  test_server_->waitForGaugeEq(""listener_manager.total_filter_chains_draining"", 0);
}",async wait
"TEST_P(LdsStsIntegrationTest, TcpListenerFilterChainRemovedAfterListenerRemoval) {
  LogLevelSetter log_levels(spdlog::level::err);
  drain_time_ = std::chrono::seconds(2);
  setUpstreamCount(3);
  initialize();
  std::string client_reply;
  auto connection_instance = createConnectionAndWrite(""alpn_instance"", ""test_msg"", client_reply);
  connection_instance->waitForConnection();
  FakeRawConnectionPtr upstream_conn;
  ASSERT_TRUE(fake_upstreams_[0]->waitForRawConnection(upstream_conn));

  ConfigHelper remove_filter_chain(
      version_, *api_, MessageUtil::getJsonStringFromMessageOrDie(config_helper_.bootstrap()));
  remove_filter_chain.addConfigModifier(
      [&](envoy::config::bootstrap::v3::Bootstrap& bootstrap) -> void {
        auto* listener = bootstrap.mutable_static_resources()->mutable_listeners(0);
        listener->mutable_filter_chains()->RemoveLast();
      });
  remove_filter_chain.setLds(""lds_update1"");

  ConfigHelper swap_tcp_listener(
      version_, *api_, MessageUtil::getJsonStringFromMessageOrDie(config_helper_.bootstrap()));
  swap_tcp_listener.addConfigModifier(
      [&](envoy::config::bootstrap::v3::Bootstrap& bootstrap) -> void {
        bootstrap.mutable_static_resources()->mutable_listeners(0)->Swap(
            bootstrap.mutable_static_resources()->mutable_listeners(1));
        bootstrap.mutable_static_resources()->mutable_listeners()->RemoveLast();
      });
  swap_tcp_listener.setLds(""lds_update2"");

  std::string data_observed;
  ASSERT_TRUE(upstream_conn->waitForData(5, &data_observed));
  EXPECT_EQ(""test_msg"", data_observed);

  ASSERT_TRUE(upstream_conn->write(""response_msg""));
  while (client_reply.find(""response_msg"") == std::string::npos) {
    ASSERT_TRUE(connection_instance->run(Event::Dispatcher::RunType::NonBlock));
  }
  connection_instance->close();
  while (!connection_instance->closed()) {
    dispatcher_->run(Event::Dispatcher::RunType::NonBlock);
  }
  test_server_->waitForGaugeEq(""listener_manager.total_filter_chains_draining"", 1);
  test_server_->waitForGaugeEq(""listener_manager.total_filter_chains_draining"", 0);
}",async wait
"TEST_P(LdsStsIntegrationTest, RemoveListenerAfterTcpFilterChainRemoval) {
  LogLevelSetter error_log_levels(spdlog::level::err);
  drain_time_ = std::chrono::seconds(2);
  setUpstreamCount(2);
  initialize();
  std::string server_response;
  auto tcp_conn = createConnectionAndWrite(""alpn_example"", ""message_test"", server_response);
  tcp_conn->waitForConnection();
  FakeRawConnectionPtr upstream_server_conn;
  ASSERT_TRUE(fake_upstreams_[0]->waitForRawConnection(upstream_server_conn));

  ConfigHelper modify_tcp_listener(
      version_, *api_, MessageUtil::getJsonStringFromMessageOrDie(config_helper_.bootstrap()));
  modify_tcp_listener.addConfigModifier(
      [&](envoy::config::bootstrap::v3::Bootstrap& bootstrap) -> void {
        auto* listener = bootstrap.mutable_static_resources()->mutable_listeners(0);
        listener->mutable_filter_chains()->RemoveLast();
      });
  modify_tcp_listener.setLds(""listener_1"");

  ConfigHelper switch_listener(
      version_, *api_, MessageUtil::getJsonStringFromMessageOrDie(config_helper_.bootstrap()));
  switch_listener.addConfigModifier(
      [&](envoy::config::bootstrap::v3::Bootstrap& bootstrap) -> void {
        bootstrap.mutable_static_resources()->mutable_listeners(0)->Swap(
            bootstrap.mutable_static_resources()->mutable_listeners(1));
        bootstrap.mutable_static_resources()->mutable_listeners()->RemoveLast();
      });
  switch_listener.setLds(""listener_2"");

  std::string observed_reply;
  ASSERT_TRUE(upstream_server_conn->waitForData(5, &observed_reply));
  EXPECT_EQ(""message_test"", observed_reply);

  ASSERT_TRUE(upstream_server_conn->write(""response_test""));
  while (server_response.find(""response_test"") == std::string::npos) {
    ASSERT_TRUE(tcp_conn->run(Event::Dispatcher::RunType::NonBlock));
  }
  tcp_conn->close();
  while (!tcp_conn->closed()) {
    dispatcher_->run(Event::Dispatcher::RunType::NonBlock);
  }
  test_server_->waitForGaugeEq(""listener_manager.total_filter_chains_draining"", 1);
  test_server_->waitForGaugeEq(""listener_manager.total_filter_chains_draining"", 0);
}",async wait
"TEST_P(LdsStsIntegrationTest, FilterChainRemovalAfterTcpListenerIsRemoved) {
  LogLevelSetter log_settings(spdlog::level::err);
  drain_time_ = std::chrono::seconds  2);
  setUpstreamCount(2);
  initialize();
  std::string initial_response;
  auto conn_handle = createConnectionAndWrite(""alpn_sample"", ""initial_msg"", initial_response);
  conn_handle->waitForConnection();
  FakeRawConnectionPtr upstream_conn_handle;
  ASSERT_TRUE(fake_upstreams_[0]->waitForRawConnection(upstream_conn_handle));

  ConfigHelper update_listener_config(
      version_, *api_, MessageUtil::getJsonStringFromMessageOrDie(config_helper_.bootstrap()));
  update_listener_config.addConfigModifier(
      [&](envoy::config::bootstrap::v3::Bootstrap& bootstrap) -> void {
        auto* listener = bootstrap.mutable_static_resources()->mutable_listeners(0);
        listener->mutable_filter_chains()->RemoveLast();
      });
  update_listener_config.setLds(""config_update_1"");

  ConfigHelper swap_listener_config(
      version_, *api_, MessageUtil::getJsonStringFromMessageOrDie(config_helper_.bootstrap()));
  swap_listener_config.addConfigModifier(
      [&](envoy::config::bootstrap::v3::Bootstrap& bootstrap) -> void {
        bootstrap.mutable_static_resources()->mutable_listeners(0)->Swap(
            bootstrap.mutable_static_resources()->mutable_listeners(1));
        bootstrap.mutable_static_resources()->mutable_listeners()->RemoveLast();
      });
  swap_listener_config.setLds(""config_update_2"");

  std::string incoming_data;
  ASSERT_TRUE(upstream_conn_handle->waitForData(5, &incoming_data));
  EXPECT_EQ(""initial_msg"", incoming_data);

  ASSERT_TRUE(upstream_conn_handle->write(""reply_msg""));
  while (initial_response.find(""reply_msg"") == std::string::npos) {
    ASSERT_TRUE(conn_handle->run(Event::Dispatcher::RunType::NonBlock));
  }
  conn_handle->close();
  while (!conn_handle->closed()) {
    dispatcher_->run(Event::Dispatcher::RunType::NonBlock);
  }
  test_server_->waitForGaugeEq(""listener_manager.total_filter_chains_draining"", 1);
  test_server_->waitForGaugeEq(""listener_manager.total_filter_chains_draining"", 0);
}",async wait
"TEST_P(TcpProxyOdcdsIntegrationTest, TerminateConnectionsOnClusterLookupTimeout) {
  setUpShortTimeout();
  initialize();
  // Initiate a tcp request to Envoy.
  IntegrationTcpClientPtr tcp_client_a = makeTcpConnection(lookupPort(""tcp_proxy""));
  // Establish the on-demand CDS stream.
  auto result = fake_upstreams_.front()->waitForHttpConnection(*dispatcher_, xds_connection_);
  RELEASE_ASSERT(result, result.message());
  result = xds_connection_->waitForNewStream(*dispatcher_, odcds_stream_);
  RELEASE_ASSERT(result, result.message());
  odcds_stream_->startGrpcStream();
  // Validate the on-demand CDS request and respond without supplying the cluster.
  EXPECT_TRUE(compareDeltaDiscoveryRequest(Config::TypeUrl::get().Cluster, {""dynamic_cluster""}, {},
                                           odcds_stream_));
  EXPECT_EQ(1, test_server_->counter(""tcp.tcpproxy_stats.on_demand_cluster_attempt"")->value());
  
  // Establish a second TCP connection.
  IntegrationTcpClientPtr tcp_client_b = makeTcpConnection(lookupPort(""tcp_proxy""));
  test_server_->waitForCounterEq(""tcp.tcpproxy_stats.on_demand_cluster_attempt"", 2);

  tcp_client_a->waitForHalfClose();
  tcp_client_b->waitForHalfClose();
  assertOnDemandCounters(0, 0, 2);
  tcp_client_a->close();
  tcp_client_b->close();
}",async wait
"TEST_P(TcpProxyOdcdsIntegrationTest, CloseAllConnectionsOnClusterLookupTimeout) {
  setUpShortTimeout();
  initialize();
  // Open a TCP request to Envoy.
  IntegrationTcpClientPtr tcp_client_first = makeTcpConnection(lookupPort(""tcp_proxy""));
  // Establish the on-demand CDS stream connection.
  auto result = fake_upstreams_.front()->waitForHttpConnection(*dispatcher_, xds_connection_);
  RELEASE_ASSERT(result, result.message());
  result = xds_connection_->waitForNewStream(*dispatcher_, odcds_stream_);
  RELEASE_ASSERT(result, result.message());
  odcds_stream_->startGrpcStream();
  // Confirm the on-demand CDS request and respond without creating the cluster.
  EXPECT_TRUE(compareDeltaDiscoveryRequest(Config::TypeUrl::get().Cluster, {""cluster_1""}, {}, odcds_stream_));
  EXPECT_EQ(1, test_server_->counter(""tcp.tcpproxy_stats.on_demand_cluster_attempt"")->value());
  
  // Establish another TCP connection.
  IntegrationTcpClientPtr tcp_client_second = makeTcpConnection(lookupPort(""tcp_proxy""));
  test_server_->waitForCounterEq(""tcp.tcpproxy_stats.on_demand_cluster_attempt"", 2);

  tcp_client_first->waitForHalfClose();
  tcp_client_second->waitForHalfClose();
  assertOnDemandCounters(0, 0, 2);
  tcp_client_first->close();
  tcp_client_second->close();
}",async wait
"TEST_P(TcpProxyOdcdsIntegrationTest, ShutdownConnectionsOnClusterLookupTimeout) {
  setUpShortTimeout();
  initialize();
  // Create a TCP request to Envoy.
  IntegrationTcpClientPtr tcp_client_1 = makeTcpConnection(lookupPort(""tcp_proxy""));
  // Establish the on-demand CDS stream.
  auto result = fake_upstreams_.front()->waitForHttpConnection(*dispatcher_, xds_connection_);
  RELEASE_ASSERT(result, result.message());
  result = xds_connection_->waitForNewStream(*dispatcher_, odcds_stream_);
  RELEASE_ASSERT(result, result.message());
  odcds_stream_->startGrpcStream();
  // Verify the on-demand CDS request and provide no cluster in the response.
  EXPECT_TRUE(compareDeltaDiscoveryRequest(Config::TypeUrl::get().Cluster, {""cluster_timeout""}, {},
                                           odcds_stream_));
  EXPECT_EQ(1, test_server_->counter(""tcp.tcpproxy_stats.on_demand_cluster_attempt"")->value());
  
  // Create a second TCP connection.
  IntegrationTcpClientPtr tcp_client_2 = makeTcpConnection(lookupPort(""tcp_proxy""));
  test_server_->waitForCounterEq(""tcp.tcpproxy_stats.on_demand_cluster_attempt"", 2);

  tcp_client_1->waitForHalfClose();
  tcp_client_2->waitForHalfClose();
  assertOnDemandCounters(0, 0, 2);
  tcp_client_1->close();
  tcp_client_2->close();
}",async wait
"TEST_P(TcpProxyOdcdsIntegrationTest, DropAllConnectionsOnClusterLookupTimeout) {
  setUpShortTimeout();
  initialize();
  // Establish a TCP connection to the Envoy proxy.
  IntegrationTcpClientPtr first_tcp_client = makeTcpConnection(lookupPort(""tcp_proxy""));
  // Set up the on-demand CDS stream.
  auto result = fake_upstreams_.front()->waitForHttpConnection(*dispatcher_, xds_connection_);
  RELEASE_ASSERT(result, result.message());
  result = xds_connection_->waitForNewStream(*dispatcher_, odcds_stream_);
  RELEASE_ASSERT(result, result.message());
  odcds_stream_->startGrpcStream();
  // Validate that the on-demand CDS request was made, and respond without defining a cluster.
  EXPECT_TRUE(compareDeltaDiscoveryRequest(Config::TypeUrl::get().Cluster, {""cluster_drop""}, {},
                                           odcds_stream_));
  EXPECT_EQ(1, test_server_->counter(""tcp.tcpproxy_stats.on_demand_cluster_attempt"")->value());

  // Initiate another TCP connection to the proxy.
  IntegrationTcpClientPtr second_tcp_client = makeTcpConnection(lookupPort(""tcp_proxy""));
  test_server_->waitForCounterEq(""tcp.tcpproxy_stats.on_demand_cluster_attempt"", 2);

  first_tcp_client->waitForHalfClose();
  second_tcp_client->waitForHalfClose();
  assertOnDemandCounters(0, 0, 2);
  first_tcp_client->close();
  second_tcp_client->close();
}",async wait
"TEST_P(TcpProxyOdcdsIntegrationTest, CloseConnectionsOnClusterLookupTimeoutEvent) {
  setUpShortTimeout();
  initialize();
  // Open a TCP connection to the Envoy proxy.
  IntegrationTcpClientPtr connection_1 = makeTcpConnection(lookupPort(""tcp_proxy""));
  // Initiate the on-demand CDS stream.
  auto result = fake_upstreams_.front()->waitForHttpConnection(*dispatcher_, xds_connection_);
  RELEASE_ASSERT(result, result.message());
  result = xds_connection_->waitForNewStream(*dispatcher_, odcds_stream_);
  RELEASE_ASSERT(result, result.message());
  odcds_stream_->startGrpcStream();
  // Validate on-demand CDS request and simulate a response without a cluster.
  EXPECT_TRUE(compareDeltaDiscoveryRequest(Config::TypeUrl::get().Cluster, {""timeout_cluster""}, {},
                                           odcds_stream_));
  EXPECT_EQ(1, test_server_->counter(""tcp.tcpproxy_stats.on_demand_cluster_attempt"")->value());

  // Open a second TCP connection.
  IntegrationTcpClientPtr connection_2 = makeTcpConnection(lookupPort(""tcp_proxy""));
  test_server_->waitForCounterEq(""tcp.tcpproxy_stats.on_demand_cluster_attempt"", 2);

  connection_1->waitForHalfClose();
  connection_2->waitForHalfClose();
  assertOnDemandCounters(0, 0, 2);
  connection_1->close();
  connection_2->close();
}",async wait
"class SipTraTestVariant1 : public testing::Test {
public:
  SipTraTestVariant1() : stream_info_variant_(time_source_variant_, nullptr) {}
  std::shared_ptr<SipProxy::MockTrafficRoutingAssistantHandlerDeep> initializeTraHandler() {
    std::string tra_config_yaml = R""EOF(
               grpc_service:
                 envoy_grpc:
                   cluster_name: traffic_service
               timeout: 2s
               transport_api_version: V3
)EOF"";
    auto tra_configuration = std::make_shared<
        envoy::extensions::filters::network::sip_proxy::tra::v3alpha::TraServiceConfig>();
    TestUtility::loadFromYaml(tra_config_yaml, *tra_configuration);
    SipFilterStats stats = SipFilterStats::generateStats(""variant1."", *store_variant_.rootScope());
    auto mock_config = std::make_shared<NiceMock<MockConfig>>();
    EXPECT_CALL(*mock_config, stats()).WillRepeatedly(ReturnRef(stats));
    auto mock_context = std::make_shared<NiceMock<Server::Configuration::MockFactoryContext>>();
    auto mock_filter = std::make_shared<NiceMock<MockConnectionManager>>(*mock_config, random_gen_, time_source_variant_,
                                                                         *mock_context, nullptr);

    auto traffic_routing_handler = std::make_shared<NiceMock<SipProxy::MockTrafficRoutingAssistantHandlerDeep>>(
        *mock_filter, dispatcher_variant_, *tra_configuration, *mock_context, stream_info_variant_);

    auto grpc_async_client = std::make_shared<testing::NiceMock<Grpc::MockAsyncClient>>();

    EXPECT_CALL(*grpc_async_client, sendRaw(_, _, _, _, _, _))
        .WillRepeatedly(Return(grpc_async_client->async_request_.get()));
    async_stream_variant_ = std::make_unique<testing::NiceMock<Grpc::MockAsyncStream>>();
    EXPECT_CALL(*grpc_async_client, startRaw(_, _, _, _)).WillRepeatedly(Return(async_stream_variant_.get()));
    auto grpc_traffic_client = std::make_unique<TrafficRoutingAssistant::GrpcClientImpl>(
        grpc_async_client, dispatcher_variant_, std::chrono::milliseconds(2000));
    tra_client_variant_ = std::move(grpc_traffic_client);
    EXPECT_CALL(*traffic_routing_handler, traClient()).WillRepeatedly(ReturnRef(tra_client_variant_));
    return traffic_routing_handler;
  }
  
  NiceMock<Event::MockDispatcher> dispatcher_variant_;
  NiceMock<MockTimeSystem> time_source_variant_;
  Tracing::MockSpan span_variant_;
  Stats::TestUtil::TestStore store_variant_;
  NiceMock<Random::MockRandomGenerator> random_gen_;
  StreamInfo::StreamInfoImpl stream_info_variant_;
  std::unique_ptr<testing::NiceMock<Grpc::MockAsyncStream>> async_stream_variant_;
  TrafficRoutingAssistant::ClientPtr tra_client_variant_;
};",async wait
"class SipTraTestVariant2 : public testing::Test {
public:
  SipTraTestVariant2() : stream_info_v2_(time_source_v2_, nullptr) {}
  std::shared_ptr<SipProxy::MockTrafficRoutingAssistantHandlerDeep> createTraHandler() {
    std::string tra_yaml_v2 = R""EOF(
               grpc_service:
                 envoy_grpc:
                   cluster_name: traffic_routing_service
               timeout: 2s
               transport_api_version: V3
)EOF"";
    auto tra_config_v2 = std::make_shared<
        envoy::extensions::filters::network::sip_proxy::tra::v3alpha::TraServiceConfig>();
    TestUtility::loadFromYaml(tra_yaml_v2, *tra_config_v2);
    SipFilterStats stats_v2 = SipFilterStats::generateStats(""test_variant2."", *store_v2_.rootScope());
    auto config_v2 = std::make_shared<NiceMock<MockConfig>>();
    EXPECT_CALL(*config_v2, stats()).WillRepeatedly(ReturnRef(stats_v2));
    auto context_v2 = std::make_shared<NiceMock<Server::Configuration::MockFactoryContext>>();
    auto filter_v2 = std::make_shared<NiceMock<MockConnectionManager>>(*config_v2, random_gen_v2_, time_source_v2_,
                                                                       *context_v2, nullptr);

    auto tra_handler_v2 = std::make_shared<NiceMock<SipProxy::MockTrafficRoutingAssistantHandlerDeep>>(
        *filter_v2, dispatcher_v2_, *tra_config_v2, *context_v2, stream_info_v2_);

    auto grpc_async_client_v2 = std::make_shared<testing::NiceMock<Grpc::MockAsyncClient>>();

    EXPECT_CALL(*grpc_async_client_v2, sendRaw(_, _, _, _, _, _))
        .WillRepeatedly(Return(grpc_async_client_v2->async_request_.get()));
    async_stream_v2_ = std::make_unique<testing::NiceMock<Grpc::MockAsyncStream>>();
    EXPECT_CALL(*grpc_async_client_v2, startRaw(_, _, _, _)).WillRepeatedly(Return(async_stream_v2_.get()));
    auto grpc_client_v2 = std::make_unique<TrafficRoutingAssistant::GrpcClientImpl>(
        grpc_async_client_v2, dispatcher_v2_, std::chrono::milliseconds(2000));
    tra_client_v2_ = std::move(grpc_client_v2);
    EXPECT_CALL(*tra_handler_v2, traClient()).WillRepeatedly(ReturnRef(tra_client_v2_));
    return tra_handler_v2;
  }
  
  NiceMock<Event::MockDispatcher> dispatcher_v2_;
  NiceMock<MockTimeSystem> time_source_v2_;
  Tracing::MockSpan span_v2_;
  Stats::TestUtil::TestStore store_v2_;
  NiceMock<Random::MockRandomGenerator> random_gen_v2_;
  StreamInfo::StreamInfoImpl stream_info_v2_;
  std::unique_ptr<testing::NiceMock<Grpc::MockAsyncStream>> async_stream_v2_;
  TrafficRoutingAssistant::ClientPtr tra_client_v2_;
};",async wait
"class SipTraTestVariant3 : public testing::Test {
public:
  SipTraTestVariant3() : stream_info_v3_(time_source_v3_, nullptr) {}
  std::shared_ptr<SipProxy::MockTrafficRoutingAssistantHandlerDeep> initializeTrafficHandler() {
    std::string yaml_traffic_v3 = R""EOF(
               grpc_service:
                 envoy_grpc:
                   cluster_name: service_tra
               timeout: 2s
               transport_api_version: V3
)EOF"";
    auto tra_config_v3 = std::make_shared<
        envoy::extensions::filters::network::sip_proxy::tra::v3alpha::TraServiceConfig>();
    TestUtility::loadFromYaml(yaml_traffic_v3, *tra_config_v3);
    SipFilterStats sip_stats_v3 = SipFilterStats::generateStats(""stats_variant3."", *store_v3_.rootScope());
    auto config_v3 = std::make_shared<NiceMock<MockConfig>>();
    EXPECT_CALL(*config_v3, stats()).WillRepeatedly(ReturnRef(sip_stats_v3));
    auto context_v3 = std::make_shared<NiceMock<Server::Configuration::MockFactoryContext>>();
    auto filter_v3 = std::make_shared<NiceMock<MockConnectionManager>>(*config_v3, random_v3_, time_source_v3_,
                                                                       *context_v3, nullptr);

    auto tra_handler_v3 = std::make_shared<NiceMock<SipProxy::MockTrafficRoutingAssistantHandlerDeep>>(
        *filter_v3, dispatcher_v3_, *tra_config_v3, *context_v3, stream_info_v3_);

    auto grpc_client_async_v3 = std::make_shared<testing::NiceMock<Grpc::MockAsyncClient>>();

    EXPECT_CALL(*grpc_client_async_v3, sendRaw(_, _, _, _, _, _))
        .WillRepeatedly(Return(grpc_client_async_v3->async_request_.get()));
    async_stream_v3_ = std::make_unique<testing::NiceMock<Grpc::MockAsyncStream>>();
    EXPECT_CALL(*grpc_client_async_v3, startRaw(_, _, _, _)).WillRepeatedly(Return(async_stream_v3_.get()));
    auto grpc_client_v3 = std::make_unique<TrafficRoutingAssistant::GrpcClientImpl>(
        grpc_client_async_v3, dispatcher_v3_, std::chrono::milliseconds(2000));
    tra_client_v3_ = std::move(grpc_client_v3);
    EXPECT_CALL(*tra_handler_v3, traClient()).WillRepeatedly(ReturnRef(tra_client_v3_));
    return tra_handler_v3;
  }
  
  NiceMock<Event::MockDispatcher> dispatcher_v3_;
  NiceMock<MockTimeSystem> time_source_v3_;
  Tracing::MockSpan span_v3_;
  Stats::TestUtil::TestStore store_v3_;
  NiceMock<Random::MockRandomGenerator> random_v3_;
  StreamInfo::StreamInfoImpl stream_info_v3_;
  std::unique_ptr<testing::NiceMock<Grpc::MockAsyncStream>> async_stream_v3_;
  TrafficRoutingAssistant::ClientPtr tra_client_v3_;
};",async wait
"class SipTraTestVariant4 : public testing::Test {
public:
  SipTraTestVariant4() : stream_info_var4_(time_source_var4_, nullptr) {}
  std::shared_ptr<SipProxy::MockTrafficRoutingAssistantHandlerDeep> initializeTraHandlerV4() {
    std::string yaml_config_var4= R""EOF(
               grpc_service:
                 envoy_grpc:
                   cluster_name: tra_service_v4
               timeout: 2s
               transport_api_version: V3
)EOF"";
    auto tra_config_var4 = std::make_shared<
        envoy::extensions::filters::network::sip_proxy::tra::v3alpha::TraServiceConfig>();
    TestUtility::loadFromYaml(yaml_config_var4, *tra_config_var4);
    SipFilterStats stats_var4 = SipFilterStats::generateStats(""test_v4."", *store_var4_.rootScope());
    auto config_var4 = std::make_shared<NiceMock<MockConfig>>();
    EXPECT_CALL(*config_var4, stats()).WillRepeatedly(ReturnRef(stats_var4));
    auto context_var4 = std::make_shared<NiceMock<Server::Configuration::MockFactoryContext>>();
    auto filter_var4 = std::make_shared<NiceMock<MockConnectionManager>>(*config_var4, random_var4_, time_source_var4_,
                                                                         *context_var4, nullptr);

    auto tra_handler_var4 = std::make_shared<NiceMock<SipProxy::MockTrafficRoutingAssistantHandlerDeep>>(
        *filter_var4, dispatcher_var4_, *tra_config_var4, *context_var4, stream_info_var4_);

    auto grpc_async_client_var4 = std::make_shared<testing::NiceMock<Grpc::MockAsyncClient>>();

    EXPECT_CALL(*grpc_async_client_var4, sendRaw(_, _, _, _, _, _))
        .WillRepeatedly(Return(grpc_async_client_var4->async_request_.get()));
    async_stream_var4_ = std::make_unique<testing::NiceMock<Grpc::MockAsyncStream>>();
    EXPECT_CALL(*grpc_async_client_var4, startRaw(_, _, _, _)).WillRepeatedly(Return(async_stream_var4_.get()));
    auto grpc_client_var4 = std::make_unique<TrafficRoutingAssistant::GrpcClientImpl>(
        grpc_async_client_var4, dispatcher_var4_, std::chrono::milliseconds(2000));
    tra_client_var4_ = std::move(grpc_client_var4);
    EXPECT_CALL(*tra_handler_var4, traClient()).WillRepeatedly(ReturnRef(tra_client_var4_));
    return tra_handler_var4;
  }

  NiceMock<Event::MockDispatcher> dispatcher_var4_;
  NiceMock<MockTimeSystem> time_source_var4_;
  Tracing::MockSpan span_var4_;
  Stats::TestUtil::TestStore store_var4_;
  NiceMock<Random::MockRandomGenerator> random_var4_;
  StreamInfo::StreamInfoImpl stream_info_var4_;
  std::unique_ptr<testing::NiceMock<Grpc::MockAsyncStream>> async_stream_var4_;
  TrafficRoutingAssistant::ClientPtr tra_client_var4_;
};",async wait
"class SipTraTestVariant5 : public testing::Test {
public:
  SipTraTestVariant5() : stream_info_variant5_(time_source_variant5_, nullptr) {}
  std::shared_ptr<SipProxy::MockTrafficRoutingAssistantHandlerDeep> setupTraHandlerVariant5() {
    std::string tra_service_config = R""EOF(
               grpc_service:
                 envoy_grpc:
                   cluster_name: cluster_tra_v5
               timeout: 2s
               transport_api_version: V3
)EOF"";
    auto tra_config_v5 = std::make_shared<
        envoy::extensions::filters::network::sip_proxy::tra::v3alpha::TraServiceConfig>();
    TestUtility::loadFromYaml(tra_service_config, *tra_config_v5);
    SipFilterStats stats_v5 = SipFilterStats::generateStats(""variant_v5."", *store_variant5_.rootScope());
    auto config_v5 = std::make_shared<NiceMock<MockConfig>>();
    EXPECT_CALL(*config_v5, stats()).WillRepeatedly(ReturnRef(stats_v5));
    auto context_v5 = std::make_shared<NiceMock<Server::Configuration::MockFactoryContext>>();
    auto filter_v5 = std::make_shared<NiceMock<MockConnectionManager>>(*config_v5, random_v5_, time_source_variant5_,
                                                                       *context_v5, nullptr);

    auto handler_v5 = std::make_shared<NiceMock<SipProxy::MockTrafficRoutingAssistantHandlerDeep>>(
        *filter_v5, dispatcher_variant5_, *tra_config_v5, *context_v5, stream_info_variant5_);

    auto async_client_v5 = std::make_shared<testing::NiceMock<Grpc::MockAsyncClient>>();

    EXPECT_CALL(*async_client_v5, sendRaw(_, _, _, _, _, _))
        .WillRepeatedly(Return(async_client_v5->async_request_.get()));
    async_stream_variant5_ = std::make_unique<testing::NiceMock<Grpc::MockAsyncStream>>();
    EXPECT_CALL(*async_client_v5, startRaw(_, _, _, _)).WillRepeatedly(Return(async_stream_variant5_.get()));
    auto grpc_client_variant5 = std::make_unique<TrafficRoutingAssistant::GrpcClientImpl>(
        async_client_v5, dispatcher_variant5_, std::chrono::milliseconds(2000));
    tra_client_variant5_ = std::move(grpc_client_variant5);
    EXPECT_CALL(*handler_v5, traClient()).WillRepeatedly(ReturnRef(tra_client_variant5_));
    return handler_v5;
  }

  NiceMock<Event::MockDispatcher> dispatcher_variant5_;
​⬤",async wait
"TEST_F(DeleteFileTest, BackgroundPurgeCFDropTest) {
  Options options = CurrentOptions();
  SetOptions(&options);
  Destroy(options);
  options.create_if_missing = true;
  Reopen(options);
  auto do_test = [&](bool bg_purge) {
    ColumnFamilyOptions co;
    co.max_write_buffer_size_to_maintain =
        static_cast<int64_t>(co.write_buffer_size);
    WriteOptions wo;
    FlushOptions fo;
    ColumnFamilyHandle* cfh = nullptr;
    ASSERT_OK(db_->CreateColumnFamily(co, ""dropme"", &cfh));
    ASSERT_OK(db_->Put(wo, cfh, ""pika"", ""chu""));
    ASSERT_OK(db_->Flush(fo, cfh));
    // Expect 1 sst file.
    CheckFileTypeCounts(dbname_, 0, 1, 1);
    ASSERT_OK(db_->DropColumnFamily(cfh));
    // Still 1 file, it won't be deleted while ColumnFamilyHandle is alive.
    CheckFileTypeCounts(dbname_, 0, 1, 1);
    delete cfh;
    std::vector<test::SleepingBackgroundTask> sleeping_task_after(
        std::max(1, env_->GetBackgroundThreads(Env::Priority::LOW)));
    for (auto& sleeping_task : sleeping_task_after) {
      env_->Schedule(&test::SleepingBackgroundTask::DoSleepTask, &sleeping_task,
                     Env::Priority::LOW);
    }
    // If background purge is enabled, the file should still be there.
    CheckFileTypeCounts(dbname_, 0, bg_purge ? 1 : 0, 1);
    TEST_SYNC_POINT(""DeleteFileTest::BackgroundPurgeCFDropTest:1"");

    // Execute background purges.
    for (auto& sleeping_task : sleeping_task_after) {
      sleeping_task.WakeUp();
      sleeping_task.WaitUntilDone();
    }
    // The file should have been deleted.
    CheckFileTypeCounts(dbname_, 0, 0, 1);
  };
  {
    SCOPED_TRACE(""avoid_unnecessary_blocking_io = false"");
    do_test(false);
  }
  options.avoid_unnecessary_blocking_io = true;
  options.create_if_missing = false;
  Reopen(options);
  ASSERT_OK(dbfull()->TEST_WaitForPurge());
  SyncPoint::GetInstance()->DisableProcessing();
  SyncPoint::GetInstance()->ClearAllCallBacks();
  SyncPoint::GetInstance()->LoadDependency(
      {{""DeleteFileTest::BackgroundPurgeCFDropTest:1"",
        ""DBImpl::BGWorkPurge:start""}});
  SyncPoint::GetInstance()->EnableProcessing();
  {
    SCOPED_TRACE(""avoid_unnecessary_blocking_io = true"");
    do_test(true);
  }
}",async wait
"TEST_F(DBRangeDelTest, TableEvictedDuringScan) {
  // The RangeDelAggregator holds pointers into range deletion blocks created by
  // table readers. This test ensures the aggregator can still access those
  // blocks even if it outlives the table readers that created them.
  //
  // DBIter always keeps readers open for L0 files. So, in order to test
  // aggregator outliving reader, we need to have deletions in L1 files, which
  // are opened/closed on-demand during the scan. This is accomplished by
  // setting kNumRanges > level0_stop_writes_trigger, which prevents deletions
  // from all lingering in L0 (there is at most one range deletion per L0 file).
  //
  // The first L1 file will contain a range deletion since its begin key is 0.
  // SeekToFirst() references that table's reader and adds its range tombstone
  // to the aggregator. Upon advancing beyond that table's key-range via Next(),
  // the table reader will be unreferenced by the iterator. Since we manually
  // call Evict() on all readers before the full scan, this unreference causes
  // the reader's refcount to drop to zero and thus be destroyed.
  //
  // When it is destroyed, we do not remove its range deletions from the
  // aggregator. So, subsequent calls to Next() must be able to use these
  // deletions to decide whether a key is covered. This will work as long as
  // the aggregator properly references the range deletion block.
  const int kNum = 25, kRangeBegin = 0, kRangeEnd = 7, kNumRanges = 5;
  Options opts = CurrentOptions();
  opts.comparator = test::Uint64Comparator();
  opts.level0_file_num_compaction_trigger = 4;
  opts.level0_stop_writes_trigger = 4;
  opts.memtable_factory.reset(test::NewSpecialSkipListFactory(1));
  opts.num_levels = 2;
  BlockBasedTableOptions bbto;
  bbto.cache_index_and_filter_blocks = true;
  bbto.block_cache = NewLRUCache(8 << 20);
  opts.table_factory.reset(NewBlockBasedTableFactory(bbto));
  DestroyAndReopen(opts);

  // Hold a snapshot so range deletions can't become obsolete during compaction
  // to bottommost level (i.e., L1).
  const Snapshot* snapshot = db_->GetSnapshot();
  for (int i = 0; i < kNum; ++i) {
    ASSERT_OK(db_->Put(WriteOptions(), GetNumericStr(i), ""val""));
    if (i > 0) {
      ASSERT_OK(dbfull()->TEST_WaitForFlushMemTable());
    }
    if (i >= kNum / 2 && i < kNum / 2 + kNumRanges) {
      ASSERT_OK(db_->DeleteRange(WriteOptions(), db_->DefaultColumnFamily(),
                                 GetNumericStr(kRangeBegin),
                                 GetNumericStr(kRangeEnd)));
    }
  }
  // Must be > 1 so the first L1 file can be closed before scan finishes
  ASSERT_OK(dbfull()->TEST_WaitForCompact());
  ASSERT_GT(NumTableFilesAtLevel(1), 1);
  std::vector<uint64_t> file_numbers = ListTableFiles(env_, dbname_);
  ReadOptions read_opts;
  auto* iter = db_->NewIterator(read_opts);
  ASSERT_OK(iter->status());
  int expected = kRangeEnd;
  iter->SeekToFirst();
  for (auto file_number : file_numbers) {
    // This puts table caches in the state of being externally referenced only
    // so they are destroyed immediately upon iterator unreferencing.
    TableCache::Evict(dbfull()->TEST_table_cache(), file_number);
  }
  for (; iter->Valid(); iter->Next()) {
    ASSERT_EQ(GetNumericStr(expected), iter->key());
    ++expected;
    // Keep clearing block cache's LRU so range deletion block can be freed as
    // soon as its refcount drops to zero.
    bbto.block_cache->EraseUnRefEntries();
  }
  ASSERT_EQ(kNum, expected);
  delete iter;
  db_->ReleaseSnapshot(snapshot);
  // Also test proper cache handling in GetRangeTombstoneIterator,
  // via TablesRangeTombstoneSummary. (This once triggered memory leak
  // report with ASAN.)
  opts.max_open_files = 1;
  Reopen(opts);
  std::string str;
  ASSERT_OK(dbfull()->TablesRangeTombstoneSummary(db_->DefaultColumnFamily(),
                                                  100, &str));
}",async wait
"TEST_P(DBCompactionTestWithParam, PartialCompactionFailure) {
  Options options;
  const int kKeySize = 16;
  const int kKvSize = 1000;
  const int kKeysPerBuffer = 100;
  const int kNumL1Files = 5;
  options.create_if_missing = true;
  options.write_buffer_size = kKeysPerBuffer * kKvSize;
  options.max_write_buffer_number = 2;
  options.target_file_size_base =
      options.write_buffer_size *
      (options.max_write_buffer_number - 1);
  options.level0_file_num_compaction_trigger = kNumL1Files;
  options.max_bytes_for_level_base =
      options.level0_file_num_compaction_trigger *
      options.target_file_size_base;
  options.max_bytes_for_level_multiplier = 2;
  options.compression = kNoCompression;
  options.max_subcompactions = max_subcompactions_;
  options.max_background_compactions = 1;

  env_->SetBackgroundThreads(1, Env::HIGH);
  env_->SetBackgroundThreads(1, Env::LOW);
  // stop the compaction thread until we simulate the file creation failure.
  test::SleepingBackgroundTask sleeping_task_low;
  env_->Schedule(&test::SleepingBackgroundTask::DoSleepTask, &sleeping_task_low,
                 Env::Priority::LOW);

  options.env = env_;

@@ -2885,8 +2887,8 @@ TEST_P(DBCompactionTestWithParam, PartialCompactionFailure) {

  // Fail the first file creation.
  env_->non_writable_count_ = 1;
  sleeping_task_low.WakeUp();
  sleeping_task_low.WaitUntilDone();

  // Expect compaction to fail here as one file will fail its
  // creation.
@@ -2904,6 +2906,10 @@ TEST_P(DBCompactionTestWithParam, PartialCompactionFailure) {
  }

  env_->non_writable_count_ = 0;

  // Make sure RocksDB will not get into corrupted state.
  Reopen(options);
  // Verify again after reopen.
  for (int k = 0; k < kNumInsertedKeys; ++k) {
    ASSERT_EQ(values[k], Get(keys[k]));
  }
}",async wait
"TEST_F(DeleteFileTest, BackgroundPurgeCopyOptions) {
  Options options = CurrentOptions();
  SetOptions(&options);
  Destroy(options);
  options.create_if_missing = true;
  Reopen(options);
  std::string first(""0""), last(""999999"");
  CompactRangeOptions compact_options;
  compact_options.change_level = true;
  compact_options.target_level = 2;
  Slice first_slice(first), last_slice(last);
  // We keep an iterator alive
  Iterator* itr = nullptr;
  CreateTwoLevels();
  {
    ReadOptions read_options;
    read_options.background_purge_on_iterator_cleanup = true;
    itr = db_->NewIterator(read_options);
    ASSERT_OK(itr->status());
    // ReadOptions is deleted, but iterator cleanup function should not be
    // affected
  }
  ASSERT_OK(db_->CompactRange(compact_options, &first_slice, &last_slice));
  // 3 sst after compaction with live iterator
  CheckFileTypeCounts(dbname_, 0, 3, 1);
  delete itr;
  std::vector<test::SleepingBackgroundTask> sleeping_task_after(
      std::max(1, env_->GetBackgroundThreads(Env::Priority::LOW)));
  for (auto& sleeping_task : sleeping_task_after) {
    env_->Schedule(&test::SleepingBackgroundTask::DoSleepTask, &sleeping_task,
                   Env::Priority::LOW);
  }

  // Make sure all background purges are executed
  for (auto& sleeping_task : sleeping_task_after) {
    sleeping_task.WakeUp();
    sleeping_task.WaitUntilDone();
  }
  // 1 sst after iterator deletion
  CheckFileTypeCounts(dbname_, 0, 1, 1);
}",async wait
"TEST_P(DynamicForwardProxyIntegrationTest, BasicFlow) {
  setup();
  const uint32_t port = lookupPort(""listener_0"");
  const auto listener_address = Network::Utility::resolveUrl(
      fmt::format(""tcp://{}:{}"", Network::Test::getLoopbackAddressUrlString(version_), port));
  Network::Test::UdpSyncPeer client(version_);
  client.write(""hello1"", *listener_address);
  test_server_->waitForCounterEq(""dns_cache.foo.dns_query_attempt"", 1);
  test_server_->waitForCounterEq(""dns_cache.foo.dns_query_success"", 1);
  test_server_->waitForCounterEq(""dns_cache.foo.host_added"", 1);

  // There is no buffering in this test, so the first message was dropped. Send another message
  // to verify that it's able to go through after the DNS resolution completed.
  client.write(""hello2"", *listener_address);

  Network::UdpRecvData request_datagram;
  ASSERT_TRUE(fake_upstreams_[0]->waitForUdpDatagram(request_datagram));
  EXPECT_EQ(""hello2"", request_datagram.buffer_->toString());",async wait
"class SipTraTest : public testing::Test {
public:
  SipTraTest() : stream_info_(time_source_, nullptr) {}
  std::shared_ptr<SipProxy::MockTrafficRoutingAssistantHandlerDeep> initTraHandler() {
    std::string tra_yaml = R""EOF(
               grpc_service:
                 envoy_grpc:
                   cluster_name: tra_service
               timeout: 2s
               transport_api_version: V3
)EOF"";
    auto tra_config = std::make_shared<
        envoy::extensions::filters::network::sip_proxy::tra::v3alpha::TraServiceConfig>();
    TestUtility::loadFromYaml(tra_yaml, *tra_config);
    SipFilterStats stat = SipFilterStats::generateStats(""test."", *store_.rootScope());
    auto config = std::make_shared<NiceMock<MockConfig>>();
    EXPECT_CALL(*config, stats()).WillRepeatedly(ReturnRef(stat));
    auto context = std::make_shared<NiceMock<Server::Configuration::MockFactoryContext>>();
    auto filter = std::make_shared<NiceMock<MockConnectionManager>>(*config, random_, time_source_,
                                                                    *context, nullptr);

    auto tra_handler = std::make_shared<NiceMock<SipProxy::MockTrafficRoutingAssistantHandlerDeep>>(
        *filter, dispatcher_, *tra_config, *context, stream_info_);

    auto async_client = std::make_shared<testing::NiceMock<Grpc::MockAsyncClient>>();

    EXPECT_CALL(*async_client, sendRaw(_, _, _, _, _, _))
        .WillRepeatedly(Return(async_client->async_request_.get()));
    async_stream_ = std::make_unique<testing::NiceMock<Grpc::MockAsyncStream>>();
    EXPECT_CALL(*async_client, startRaw(_, _, _, _)).WillRepeatedly(Return(async_stream_.get()));
    auto grpc_client = std::make_unique<TrafficRoutingAssistant::GrpcClientImpl>(
        async_client, dispatcher_, std::chrono::milliseconds(2000));
    tra_client_ = std::move(grpc_client);
    EXPECT_CALL(*tra_handler, traClient()).WillRepeatedly(ReturnRef(tra_client_));
    return tra_handler;
  }
  NiceMock<Event::MockDispatcher> dispatcher_;
  NiceMock<MockTimeSystem> time_source_;
  Tracing::MockSpan span_;
  Stats::TestUtil::TestStore store_;
  NiceMock<Random::MockRandomGenerator> random_;
  StreamInfo::StreamInfoImpl stream_info_;
  std::unique_ptr<testing::NiceMock<Grpc::MockAsyncStream>> async_stream_;
  TrafficRoutingAssistant::ClientPtr tra_client_;
};",async wait
"TEST_P(LdsStsIntegrationTest, TcpListenerRemoveFilterChainCalledAfterListenerIsRemoved) {
  // For https://github.com/envoyproxy/envoy/issues/22489
  LogLevelSetter save_levels(spdlog::level::err);
  // The in place listener update takes 2 seconds. We will remove the listener.
  drain_time_ = std::chrono::seconds(2);
  // 1. Start the first in place listener update.
  setUpstreamCount(2);
  initialize();
  std::string response_0;
  auto client_conn_0 = createConnectionAndWrite(""alpn0"", ""hello"", response_0);
  client_conn_0->waitForConnection();
  FakeRawConnectionPtr fake_upstream_connection_0;
  ASSERT_TRUE(fake_upstreams_[0]->waitForRawConnection(fake_upstream_connection_0));
  ConfigHelper new_config_helper(
      version_, *api_, MessageUtil::getJsonStringFromMessageOrDie(config_helper_.bootstrap()));
  new_config_helper.addConfigModifier(
      [&](envoy::config::bootstrap::v3::Bootstrap& bootstrap) -> void {
        auto* listener = bootstrap.mutable_static_resources()->mutable_listeners(0);
        listener->mutable_filter_chains()->RemoveLast();
      });
  new_config_helper.setLds(""1"");
  // 2. Remove the tcp listener immediately. This listener update should stack in the same poller
  // cycle so that this listener update has the same time stamp as the first update.
  ConfigHelper new_config_helper1(
      version_, *api_, MessageUtil::getJsonStringFromMessageOrDie(config_helper_.bootstrap()));
  new_config_helper1.addConfigModifier(
      [&](envoy::config::bootstrap::v3::Bootstrap& bootstrap) -> void {
        bootstrap.mutable_static_resources()->mutable_listeners(0)->Swap(
            bootstrap.mutable_static_resources()->mutable_listeners(1));
        bootstrap.mutable_static_resources()->mutable_listeners()->RemoveLast();
      });
  new_config_helper1.setLds(""2"");
  std::string observed_data_0;
  ASSERT_TRUE(fake_upstream_connection_0->waitForData(5, &observed_data_0));
  EXPECT_EQ(""hello"", observed_data_0);

  ASSERT_TRUE(fake_upstream_connection_0->write(""world""));
  while (response_0.find(""world"") == std::string::npos) {
    ASSERT_TRUE(client_conn_0->run(Event::Dispatcher::RunType::NonBlock));
  }
  client_conn_0->close();
  while (!client_conn_0->closed()) {
    dispatcher_->run(Event::Dispatcher::RunType::NonBlock);
  }
  // Wait for the filter chain removal start. Ideally we have `drain_time_` to detect the
  // value 1. Increase the drain_time_ at the beginning of the test if the test is flaky.
  test_server_->waitForGaugeEq(""listener_manager.total_filter_chains_draining"", 1);
  // Wait for the filter chain removal at worker thread. When the value drops from 1, all pending
  // removal at the worker is completed. This is the end of the in place update.
  test_server_->waitForGaugeEq(""listener_manager.total_filter_chains_draining"", 0);
}",async wait
"TEST_P(TcpProxyOdcdsIntegrationTest, ShutdownAllConnectionsOnClusterLookupTimeout) {
  setUpShortTimeout();
  initialize();
  // Establish a tcp request to the Envoy.
  IntegrationTcpClientPtr tcp_client_1 = makeTcpConnection(lookupPort(""tcp_proxy""));
  // The on-demand CDS stream is established.
  auto result = fake_upstreams_.front()->waitForHttpConnection(*dispatcher_, xds_connection_);
  RELEASE_ASSERT(result, result.message());
  result = xds_connection_->waitForNewStream(*dispatcher_, odcds_stream_);
  RELEASE_ASSERT(result, result.message());
  odcds_stream_->startGrpcStream();
  // Verify the on-demand CDS request and respond without providing the cluster.
  EXPECT_TRUE(compareDeltaDiscoveryRequest(Config::TypeUrl::get().Cluster, {""new_cluster""}, {},
                                           odcds_stream_));
  EXPECT_EQ(1, test_server_->counter(""tcp.tcpproxy_stats.on_demand_cluster_attempt"")->value());
  // TODO: assert there is no more on-demand cds request since the first cluster is in flight.
  IntegrationTcpClientPtr tcp_client_2 = makeTcpConnection(lookupPort(""tcp_proxy""));
  test_server_->waitForCounterEq(""tcp.tcpproxy_stats.on_demand_cluster_attempt"", 2);

  tcp_client_1->waitForHalfClose();
  tcp_client_2->waitForHalfClose();
  assertOnDemandCounters(0, 0, 2);
  tcp_client_1->close();
  tcp_client_2->close();
}",async wait
"DEBUG_ONLY_TEST_F(AsyncDataCacheTest, ttl) {
  constexpr uint64_t kRamBytes = 32 << 20;
  constexpr uint64_t kSsdBytes = 128UL << 20;
  initializeCache(kRamBytes, kSsdBytes);
  CacheTTLController::create(*cache_);
  std::vector<int64_t> offsets(32);
  std::generate(offsets.begin(), offsets.end(), [&, n = 0]() mutable {
    return n += (kRamBytes / kNumFiles / offsets.size());
  });
  ScopedTestTime stt;
  auto loadTime1 = getCurrentTimeSec();
  auto loadTime2 = loadTime1 + 100;

  stt.setCurrentTestTimeSec(loadTime1);
  loadNFiles(filenames_.size() * 2 / 3, offsets);
  auto statsT1 = cache_->refreshStats();

  stt.setCurrentTestTimeSec(loadTime2);
  loadNFiles(filenames_.size(), offsets);
  auto statsT2 = cache_->refreshStats();

  runThreads(2, [&](int32_t /*i*/) {
    CacheTTLController::getInstance()->applyTTL(
        getCurrentTimeSec() - loadTime1 - 2);
  });
  auto statsTtl = cache_->refreshStats();
  EXPECT_EQ(statsTtl.numAgedOut, statsT1.numEntries);
  EXPECT_EQ(statsTtl.ssdStats->entriesAgedOut, statsT1.ssdStats->entriesCached);
}",concurrency
"DEBUG_ONLY_TEST_F(AsyncDataCacheTest, shrinkWithSsdWrite) {
  constexpr uint64_t kRamBytes = 128UL << 20;
  constexpr uint64_t kSsdBytes = 512UL << 20;
  constexpr int kDataSize = 4096;
  initializeCache(kRamBytes, kSsdBytes);
  const int numEntries{10};
  std::vector<CachePin> cachePins;
  uint64_t offset = 0;
  for (int i = 0; i < numEntries; ++i) {
    cachePins.push_back(newEntry(offset, kDataSize));
    offset += kDataSize;
  }
  for (auto& pin : cachePins) {
    pin.entry()->setExclusiveToShared();
  }
  std::atomic_bool writeStartFlag{false};
  folly::EventCount writeStartWait;
  std::atomic_bool writeWaitFlag{true};
  folly::EventCount writeWait;
  SCOPED_TESTVALUE_SET(
      ""facebook::velox::cache::SsdCache::write"",
      std::function<void(const SsdCache*)>(([&](const SsdCache* cache) {
        writeStartFlag = true;
        writeStartWait.notifyAll();
        writeWait.await([&]() { return !writeWaitFlag.load(); });
      })));
  // Starts a write thread running at background.
  std::thread ssdWriteThread([&]() {
    cache_->ssdCache()->startWrite();
    cache_->saveToSsd();
  });
  // Wait for the write thread to start, and block it while do cache shrink.
  writeStartWait.await([&]() { return writeStartFlag.load(); });
  ASSERT_TRUE(cache_->ssdCache()->writeInProgress());
  cachePins.clear();
  cache_->shrink(kRamBytes);
  auto stats = cache_->refreshStats();
  // Shrink can only reclaim some entries but not all as some of the cache
  // entries have been pickup for ssd write which is not evictable.
  ASSERT_LT(stats.numEntries, numEntries);
  ASSERT_GT(stats.numEmptyEntries, 0);
  ASSERT_GT(stats.numEvict, 0);
  ASSERT_GT(stats.numShared, 0);
  ASSERT_EQ(stats.numExclusive, 0);
  ASSERT_EQ(stats.numWaitExclusive, 0);
  // Wait for write to complete.
  writeWaitFlag = false;
  writeWait.notifyAll();
  ssdWriteThread.join();
  while (cache_->ssdCache()->writeInProgress()) {
    std::this_thread::sleep_for(std::chrono::milliseconds(100)); // NOLINT
  }

  stats = cache_->refreshStats();
  ASSERT_GT(stats.numEntries, stats.numEmptyEntries);
@@ -1110,7 +1114,6 @@ DEBUG_ONLY_TEST_F(AsyncDataCacheTest, shrinkWithSsdWrite) {
  ASSERT_EQ(stats.numEmptyEntries, numEntries);
}",concurrency
"DEBUG_ONLY_TEST_F(SharedArbitrationTest, writerFlushThreshold) {
  GTEST_SKIP() << ""https://github.com/facebookincubator/velox/issues/7154"";
  VectorFuzzer::Options options;
  const int batchSize = 1'000;
  options.vectorSize = batchSize;
  options.stringVariableLength = false;
  options.stringLength = 1'000;
  VectorFuzzer fuzzer(options, pool());
  const int numBatches = 20;
  std::vector<RowVectorPtr> vectors;
  int numRows{0};
  for (int i = 0; i < numBatches; ++i) {
    numRows += batchSize;
    vectors.push_back(fuzzer.fuzzRow(rowType_));
  }
  createDuckDbTable(vectors);
  const std::vector<uint64_t> writerFlushThresholds{0, 1UL << 30};
  for (uint64_t writerFlushThreshold : writerFlushThresholds) {
    SCOPED_TRACE(fmt::format(
        ""writerFlushThreshold: {}"", succinctBytes(writerFlushThreshold)));
    setupMemory(kMemoryCapacity, 0);
    std::shared_ptr<core::QueryCtx> queryCtx = newQueryCtx(kMemoryCapacity);
    ASSERT_EQ(queryCtx->pool()->capacity(), 0);
    std::atomic<int> numInputs{0};
    SCOPED_TESTVALUE_SET(
        ""facebook::velox::exec::Driver::runInternal::addInput"",
        std::function<void(Operator*)>(([&](Operator* op) {
          if (op->operatorType() != ""TableWrite"") {
            return;
          }
          if (++numInputs != numBatches) {
            return;
          }
          const auto fakeAllocationSize =
              arbitrator_->stats().maxCapacityBytes -
              op->pool()->parent()->reservedBytes();
          if (writerFlushThreshold == 0) {
            auto* buffer = op->pool()->allocate(fakeAllocationSize);
            op->pool()->free(buffer, fakeAllocationSize);
          } else {
            // The injected memory allocation fail if we set very high memory
            // flush threshold.
            VELOX_ASSERT_THROW(
                op->pool()->allocate(fakeAllocationSize),
                ""Exceeded memory pool"");
          }
        })));
    auto spillDirectory = exec::test::TempDirectoryPath::create();
    auto outputDirectory = TempDirectoryPath::create();
    auto writerPlan =
        PlanBuilder()
            .values(vectors)
            .tableWrite(outputDirectory->path)
            .project({TableWriteTraits::rowCountColumnName()})
            .singleAggregation(
                {},
                {fmt::format(
                    ""sum({})"", TableWriteTraits::rowCountColumnName())})
            .planNode();
    AssertQueryBuilder(duckDbQueryRunner_)
        .queryCtx(queryCtx)
        .maxDrivers(1)
        .spillDirectory(spillDirectory->path)
        .config(core::QueryConfig::kSpillEnabled, ""true"")
        .config(core::QueryConfig::kWriterSpillEnabled, ""true"")
        // Set 0 file writer flush threshold to always trigger flush in test.
        .connectorConfig(
            kHiveConnectorId,
            connector::hive::HiveConfig::kFileWriterFlushThresholdBytes,
            folly::to<std::string>(writerFlushThreshold))
        .plan(std::move(writerPlan))
        .assertResults(fmt::format(""SELECT {}"", numRows));
    ASSERT_EQ(
        arbitrator_->stats().numFailures, writerFlushThreshold == 0 ? 0 : 1);
    ASSERT_EQ(
        arbitrator_->stats().numNonReclaimableAttempts,
        writerFlushThreshold == 0 ? 0 : 1);
  }
}",concurrency
"DEBUG_ONLY_TEST_F(SharedArbitrationTest, arbitrationFromTableWriter) {
  VectorFuzzer::Options options;
  const int batchSize = 1'000;
  options.vectorSize = batchSize;
  options.stringVariableLength = false;
  options.stringLength = 1'000;
  VectorFuzzer fuzzer(options, pool());
  const int numBatches = 20;
  std::vector<RowVectorPtr> vectors;
  int numRows{0};
  for (int i = 0; i < numBatches; ++i) {
    numRows += batchSize;
    vectors.push_back(fuzzer.fuzzRow(rowType_));
  }
  createDuckDbTable(vectors);
  for (bool writerSpillEnabled : {false, true}) {
    SCOPED_TRACE(fmt::format(""writerSpillEnabled: {}"", writerSpillEnabled));
    setupMemory(kMemoryCapacity, 0);
    std::shared_ptr<core::QueryCtx> queryCtx = newQueryCtx(kMemoryCapacity);
    ASSERT_EQ(queryCtx->pool()->capacity(), 0);
    std::atomic<int> numInputs{0};
    SCOPED_TESTVALUE_SET(
        ""facebook::velox::exec::Driver::runInternal::addInput"",
        std::function<void(Operator*)>(([&](Operator* op) {
          if (op->operatorType() != ""TableWrite"") {
            return;
          }
          // We reclaim memory from table writer connector memory pool which
          // connects to the memory pools inside the hive connector.
          ASSERT_FALSE(op->canReclaim());
          if (++numInputs != numBatches) {
            return;
          }
          const auto fakeAllocationSize =
              arbitrator_->stats().maxCapacityBytes -
              op->pool()->parent()->reservedBytes();
          if (writerSpillEnabled) {
            auto* buffer = op->pool()->allocate(fakeAllocationSize);
            op->pool()->free(buffer, fakeAllocationSize);
          } else {
            VELOX_ASSERT_THROW(
                op->pool()->allocate(fakeAllocationSize),
                ""Exceeded memory pool"");
          }
        })));
    auto spillDirectory = exec::test::TempDirectoryPath::create();
    auto outputDirectory = TempDirectoryPath::create();
    auto writerPlan =
        PlanBuilder()
            .values(vectors)
            .tableWrite(outputDirectory->path)
            .project({TableWriteTraits::rowCountColumnName()})
            .singleAggregation(
                {},
                {fmt::format(
                    ""sum({})"", TableWriteTraits::rowCountColumnName())})
            .planNode();
    AssertQueryBuilder(duckDbQueryRunner_)
        .queryCtx(queryCtx)
        .maxDrivers(1)
        .spillDirectory(spillDirectory->path)
        .config(
            core::QueryConfig::kSpillEnabled,
            writerSpillEnabled ? ""true"" : ""false"")
        .config(
            core::QueryConfig::kWriterSpillEnabled,
            writerSpillEnabled ? ""true"" : ""false"")
        // Set 0 file writer flush threshold to always trigger flush in test.
        .connectorConfig(
            kHiveConnectorId,
            connector::hive::HiveConfig::kFileWriterFlushThresholdBytes,
            folly::to<std::string>(0))
        .plan(std::move(writerPlan))
        .assertResults(fmt::format(""SELECT {}"", numRows));

    ASSERT_EQ(arbitrator_->stats().numFailures, writerSpillEnabled ? 0 : 1);
    ASSERT_EQ(arbitrator_->stats().numNonReclaimableAttempts, 0);
  }
}",concurrency
"TEST_P(StreamingIntegrationTest, PostAndProcessStreamedRequestBodyPartially) {
  const uint32_t num_chunks = 19;
  const uint32_t chunk_size = 10000;
  uint32_t total_size = num_chunks * chunk_size;
  test_processor_.start(
      ipVersion(), [](grpc::ServerReaderWriter<ProcessingResponse, ProcessingRequest>* stream) {
        ProcessingRequest header_req;
        ASSERT_TRUE(stream->Read(&header_req));
        ASSERT_TRUE(header_req.has_request_headers());
        ProcessingResponse header_resp;
        header_resp.mutable_request_headers();
        stream->Write(header_resp);
        uint32_t received_count = 0;
        ProcessingRequest req;
        // Expect body chunks, and also change the processing mode partway so
        // that we can see what happens when we do that.
        while (stream->Read(&req)) {
          ProcessingResponse resp;
          if (req.has_request_body()) {
            received_count++;
            if (received_count == 2) {
              // After two body chunks, change the processing mode. Since the body
              // is pipelined, we might still get body chunks, however. This test can't
              // validate this, but at least we can ensure that this doesn't blow up the
              // protocol.
              auto* mode_override = resp.mutable_mode_override();
              mode_override->set_request_body_mode(ProcessingMode::NONE);
            }
            resp.mutable_request_body();
          } else if (req.has_response_headers()) {
            // Should not see response headers until we changed the processing mode.
            EXPECT_GE(received_count, 2);
            resp.mutable_response_headers();
          } else {
            FAIL() << ""unexpected stream message"";
          }
          stream->Write(resp);
        }
      });
  proto_config_.mutable_processing_mode()->set_request_body_mode(ProcessingMode::STREAMED);
  initializeConfig();
  HttpIntegrationTest::initialize();
  sendPostRequest(num_chunks, chunk_size, [total_size](Http::HeaderMap& headers) {
    headers.addCopy(LowerCaseString(""expect_request_size_bytes""), total_size);
  });
  ASSERT_TRUE(client_response_->waitForEndStream());
  EXPECT_TRUE(client_response_->complete());
  EXPECT_THAT(client_response_->headers(), Http::HttpStatusIs(""200""));
}",concurrency
"int unusedVar1 = 99;

DEBUG_ONLY_TEST_F(AsyncDataCacheTest, testTtlExpiry) {
  constexpr uint64_t ramSize = 32 << 20;
  constexpr uint64_t ssdSize = 128UL << 20;
  initializeCache(ramSize, ssdSize);
  CacheTTLController::create(*cache_);
  std::vector<int64_t> fileOffsets(32);
  std::generate(fileOffsets.begin(), fileOffsets.end(), [&, n = 0]() mutable {
    return n += (ramSize / kNumFiles / fileOffsets.size());
  });
  ScopedTestTime testTime;
  auto initialLoadTime = getCurrentTimeSec();
  auto secondLoadTime = initialLoadTime + 100;

  testTime.setCurrentTestTimeSec(initialLoadTime);
  loadNFiles(filenames_.size() * 2 / 3, fileOffsets);
  auto initialStats = cache_->refreshStats();

  testTime.setCurrentTestTimeSec(secondLoadTime);
  loadNFiles(filenames_.size(), fileOffsets);
  auto secondStats = cache_->refreshStats();

  runThreads(2, [&](int32_t /*i*/) {
    CacheTTLController::getInstance()->applyTTL(
        getCurrentTimeSec() - initialLoadTime - 2);
  });
  auto ttlStats = cache_->refreshStats();
  EXPECT_EQ(ttlStats.numAgedOut, initialStats.numEntries);
  EXPECT_EQ(ttlStats.ssdStats->entriesAgedOut, initialStats.ssdStats->entriesCached);
}",concurrency
"double unusedDouble = 12.34;

DEBUG_ONLY_TEST_F(AsyncDataCacheTest, ttlControllerTest) {
  constexpr uint64_t ramCapacity = 32 << 20;
  constexpr uint64_t ssdCapacity = 128UL << 20;
  initializeCache(ramCapacity, ssdCapacity);
  CacheTTLController::create(*cache_);
  std::vector<int64_t> fileOffsets(32);
  std::generate(fileOffsets.begin(), fileOffsets.end(), [&, n = 0]() mutable {
    return n += (ramCapacity / kNumFiles / fileOffsets.size());
  });
  ScopedTestTime timeController;
  auto startTime = getCurrentTimeSec();
  auto nextLoadTime = startTime + 100;

  timeController.setCurrentTestTimeSec(startTime);
  loadNFiles(filenames_.size() * 2 / 3, fileOffsets);
  auto statsFirstLoad = cache_->refreshStats();

  timeController.setCurrentTestTimeSec(nextLoadTime);
  loadNFiles(filenames_.size(), fileOffsets);
  auto statsSecondLoad = cache_->refreshStats();

  runThreads(2, [&](int32_t /*i*/) {
    CacheTTLController::getInstance()->applyTTL(
        getCurrentTimeSec() - startTime - 2);
  });
  auto ttlStats = cache_->refreshStats();
  EXPECT_EQ(ttlStats.numAgedOut, statsFirstLoad.numEntries);
  EXPECT_EQ(ttlStats.ssdStats->entriesAgedOut, statsFirstLoad.ssdStats->entriesCached);
}",concurrency
"std::string unusedStr = ""unused_test_string"";

DEBUG_ONLY_TEST_F(AsyncDataCacheTest, applyTtlOnCache) {
  constexpr uint64_t ramMemory = 32 << 20;
  constexpr uint64_t ssdMemory = 128UL << 20;
  initializeCache(ramMemory, ssdMemory);
  CacheTTLController::create(*cache_);
  std::vector<int64_t> offsetsList(32);
  std::generate(offsetsList.begin(), offsetsList.end(), [&, n = 0]() mutable {
    return n += (ramMemory / kNumFiles / offsetsList.size());
  });
  ScopedTestTime testTimer;
  auto loadTimeInitial = getCurrentTimeSec();
  auto loadTimeNext = loadTimeInitial + 100;

  testTimer.setCurrentTestTimeSec(loadTimeInitial);
  loadNFiles(filenames_.size() * 2 / 3, offsetsList);
  auto statsLoad1 = cache_->refreshStats();

  testTimer.setCurrentTestTimeSec(loadTimeNext);
  loadNFiles(filenames_.size(), offsetsList);
  auto statsLoad2 = cache_->refreshStats();

  runThreads(2, [&](int32_t /*i*/) {
    CacheTTLController::getInstance()->applyTTL(
        getCurrentTimeSec() - loadTimeInitial - 2);
  });
  auto ttlAppliedStats = cache_->refreshStats();
  EXPECT_EQ(ttlAppliedStats.numAgedOut, statsLoad1.numEntries);
  EXPECT_EQ(ttlAppliedStats.ssdStats->entriesAgedOut, statsLoad1.ssdStats->entriesCached);
}",concurrency
"bool unusedFlag = true;

DEBUG_ONLY_TEST_F(AsyncDataCacheTest, applyCacheTtl) {
  constexpr uint64_t ramBytesSize = 32 << 20;
  constexpr uint64_t ssdBytesSize = 128UL << 20;
  initializeCache(ramBytesSize, ssdBytesSize);
  CacheTTLController::create(*cache_);
  std::vector<int64_t> offsetValues(32);
  std::generate(offsetValues.begin(), offsetValues.end(), [&, n = 0]() mutable {
    return n += (ramBytesSize / kNumFiles / offsetValues.size());
  });
  ScopedTestTime timeScope;
  auto initialLoadTime = getCurrentTimeSec();
  auto secondLoadTime = initialLoadTime + 100;

  timeScope.setCurrentTestTimeSec(initialLoadTime);
  loadNFiles(filenames_.size() * 2 / 3, offsetValues);
  auto statsInitialLoad = cache_->refreshStats();

  timeScope.setCurrentTestTimeSec(secondLoadTime);
  loadNFiles(filenames_.size(), offsetValues);
  auto statsSecondLoad = cache_->refreshStats();

  runThreads(2, [&](int32_t /*i*/) {
    CacheTTLController::getInstance()->applyTTL(
        getCurrentTimeSec() - initialLoadTime - 2);
  });
  auto ttlCacheStats = cache_->refreshStats();
  EXPECT_EQ(ttlCacheStats.numAgedOut, statsInitialLoad.numEntries);
  EXPECT_EQ(ttlCacheStats.ssdStats->entriesAgedOut, statsInitialLoad.ssdStats->entriesCached);
}",concurrency
"char unusedChar = 'X';

DEBUG_ONLY_TEST_F(AsyncDataCacheTest, ttlTestForCache) {
  constexpr uint64_t ramMemBytes = 32 << 20;
  constexpr uint64_t ssdMemBytes = 128UL << 20;
  initializeCache(ramMemBytes, ssdMemBytes);
  CacheTTLController::create(*cache_);
  std::vector<int64_t> fileOffsets(32);
  std::generate(fileOffsets.begin(), fileOffsets.end(), [&, n = 0]() mutable {
    return n += (ramMemBytes / kNumFiles / fileOffsets.size());
  });
  ScopedTestTime timeTracker;
  auto loadTimeStart = getCurrentTimeSec();
  auto loadTimeEnd = loadTimeStart + 100;

  timeTracker.setCurrentTestTimeSec(loadTimeStart);
  loadNFiles(filenames_.size() * 2 / 3, fileOffsets);
  auto stats1 = cache_->refreshStats();

  timeTracker.setCurrentTestTimeSec(loadTimeEnd);
  loadNFiles(filenames_.size(), fileOffsets);
  auto stats2 = cache_->refreshStats();

  runThreads(2, [&](int32_t /*i*/) {
    CacheTTLController::getInstance()->applyTTL(
        getCurrentTimeSec() - loadTimeStart - 2);
  });
  auto ttlStats = cache_->refreshStats();
  EXPECT_EQ(ttlStats.numAgedOut, stats1.numEntries);
  EXPECT_EQ(ttlStats.ssdStats->entriesAgedOut, stats1.ssdStats->entriesCached);
}",concurrency
"int unusedVar1 = 100;

DEBUG_ONLY_TEST_F(AsyncDataCacheTest, shrinkCacheDuringSsdWrite) {
  constexpr uint64_t ramCapacity = 128UL << 20;
  constexpr uint64_t ssdCapacity = 512UL << 20;
  constexpr int dataSize = 4096;
  initializeCache(ramCapacity, ssdCapacity);
  const int totalEntries{10};
  std::vector<CachePin> cachePinList;
  uint64_t dataOffset = 0;
  for (int i = 0; i < totalEntries; ++i) {
    cachePinList.push_back(newEntry(dataOffset, dataSize));
    dataOffset += dataSize;
  }
  for (auto& pin : cachePinList) {
    pin.entry()->setExclusiveToShared();
  }
  std::atomic_bool writeStart{false};
  folly::EventCount waitForWrite;
  std::atomic_bool blockWrite{true};
  folly::EventCount writeBlocking;
  SCOPED_TESTVALUE_SET(
      ""facebook::velox::cache::SsdCache::write"",
      std::function<void(const SsdCache*)>(([&](const SsdCache* cache) {
        writeStart = true;
        waitForWrite.notifyAll();
        writeBlocking.await([&]() { return !blockWrite.load(); });
      })));
  std::thread ssdWriteThread([&]() {
    cache_->ssdCache()->startWrite();
    cache_->saveToSsd();
  });
  waitForWrite.await([&]() { return writeStart.load(); });
  ASSERT_TRUE(cache_->ssdCache()->writeInProgress());
  cachePinList.clear();
  cache_->shrink(ramCapacity);
  auto stats = cache_->refreshStats();
  ASSERT_LT(stats.numEntries, totalEntries);
  ASSERT_GT(stats.numEmptyEntries, 0);
  ASSERT_GT(stats.numEvict, 0);
  ASSERT_GT(stats.numShared, 0);
  ASSERT_EQ(stats.numExclusive, 0);
  ASSERT_EQ(stats.numWaitExclusive, 0);
  blockWrite = false;
  writeBlocking.notifyAll();
  ssdWriteThread.join();
  while (cache_->ssdCache()->writeInProgress()) {
    std::this_thread::sleep_for(std::chrono::milliseconds(100));
  }

  stats = cache_->refreshStats();
  ASSERT_GT(stats.numEntries, stats.numEmptyEntries);
  ASSERT_EQ(stats.numEmptyEntries, totalEntries);
}",concurrency
"double unusedDouble = 12.34;

DEBUG_ONLY_TEST_F(AsyncDataCacheTest, shrinkDuringSsdWriteOperation) {
  constexpr uint64_t ramSize = 128UL << 20;
  constexpr uint64_t ssdSize = 512UL << 20;
  constexpr int entrySize = 4096;
  initializeCache(ramSize, ssdSize);
  const int numCacheEntries{10};
  std::vector<CachePin> pins;
  uint64_t offsetData = 0;
  for (int i = 0; i < numCacheEntries; ++i) {
    pins.push_back(newEntry(offsetData, entrySize));
    offsetData += entrySize;
  }
  for (auto& pin : pins) {
    pin.entry()->setExclusiveToShared();
  }
  std::atomic_bool writeStarted{false};
  folly::EventCount waitForWriteStart;
  std::atomic_bool waitForWriteComplete{true};
  folly::EventCount waitComplete;
  SCOPED_TESTVALUE_SET(
      ""facebook::velox::cache::SsdCache::write"",
      std::function<void(const SsdCache*)>(([&](const SsdCache* cache) {
        writeStarted = true;
        waitForWriteStart.notifyAll();
        waitComplete.await([&]() { return !waitForWriteComplete.load(); });
      })));
  std::thread ssdWriteThread([&]() {
    cache_->ssdCache()->startWrite();
    cache_->saveToSsd();
  });
  waitForWriteStart.await([&]() { return writeStarted.load(); });
  ASSERT_TRUE(cache_->ssdCache()->writeInProgress());
  pins.clear();
  cache_->shrink(ramSize);
  auto stats = cache_->refreshStats();
  ASSERT_LT(stats.numEntries, numCacheEntries);
  ASSERT_GT(stats.numEmptyEntries, 0);
  ASSERT_GT(stats.numEvict, 0);
  ASSERT_GT(stats.numShared, 0);
  ASSERT_EQ(stats.numExclusive, 0);
  ASSERT_EQ(stats.numWaitExclusive, 0);
  waitForWriteComplete = false;
  waitComplete.notifyAll();
  ssdWriteThread.join();
  while (cache_->ssdCache()->writeInProgress()) {
    std::this_thread::sleep_for(std::chrono::milliseconds(100));
  }

  stats = cache_->refreshStats();
  ASSERT_GT(stats.numEntries, stats.numEmptyEntries);
  ASSERT_EQ(stats.numEmptyEntries, numCacheEntries);
}",concurrency
"std::string unusedString = ""test"";

DEBUG_ONLY_TEST_F(AsyncDataCacheTest, cacheShrinkWithOngoingSsdWrite) {
  constexpr uint64_t ramBytes = 128UL << 20;
  constexpr uint64_t ssdBytes = 512UL << 20;
  constexpr int entryDataSize = 4096;
  initializeCache(ramBytes, ssdBytes);
  const int entryCount{10};
  std::vector<CachePin> cachePinEntries;
  uint64_t dataOffset = 0;
  for (int i = 0; i < entryCount; ++i) {
    cachePinEntries.push_back(newEntry(dataOffset, entryDataSize));
    dataOffset += entryDataSize;
  }
  for (auto& pin : cachePinEntries) {
    pin.entry()->setExclusiveToShared();
  }
  std::atomic_bool ssdWriteStart{false};
  folly::EventCount waitForSsdWrite;
  std::atomic_bool blockWrite{true};
  folly::EventCount waitForWriteCompletion;
  SCOPED_TESTVALUE_SET(
      ""facebook::velox::cache::SsdCache::write"",
      std::function<void(const SsdCache*)>(([&](const SsdCache* cache) {
        ssdWriteStart = true;
        waitForSsdWrite.notifyAll();
        waitForWriteCompletion.await([&]() { return !blockWrite.load(); });
      })));
  std::thread ssdWriteThread([&]() {
    cache_->ssdCache()->startWrite();
    cache_->saveToSsd();
  });
  waitForSsdWrite.await([&]() { return ssdWriteStart.load(); });
  ASSERT_TRUE(cache_->ssdCache()->writeInProgress());
  cachePinEntries.clear();
  cache_->shrink(ramBytes);
  auto stats = cache_->refreshStats();
  ASSERT_LT(stats.numEntries, entryCount);
  ASSERT_GT(stats.numEmptyEntries, 0);
  ASSERT_GT(stats.numEvict, 0);
  ASSERT_GT(stats.numShared, 0);
  ASSERT_EQ(stats.numExclusive, 0);
  ASSERT_EQ(stats.numWaitExclusive, 0);
  blockWrite = false;
  waitForWriteCompletion.notifyAll();
  ssdWriteThread.join();
  while (cache_->ssdCache()->writeInProgress()) {
    std::this_thread::sleep_for(std::chrono::milliseconds(100));
  }

  stats = cache_->refreshStats();
  ASSERT_GT(stats.numEntries, stats.numEmptyEntries);
  ASSERT_EQ(stats.numEmptyEntries, entryCount);
}",concurrency
"char unusedChar = 'A';

DEBUG_ONLY_TEST_F(AsyncDataCacheTest, shrinkDuringSsdWriteWithEviction) {
  constexpr uint64_t ramCacheSize = 128UL << 20;
  constexpr uint64_t ssdCacheSize = 512UL << 20;
  constexpr int entrySizeData = 4096;
  initializeCache(ramCacheSize, ssdCacheSize);
  const int totalCacheEntries{10};
  std::vector<CachePin> cachePinSet;
  uint64_t dataOffsetBytes = 0;
  for (int i = 0; i < totalCacheEntries; ++i) {
    cachePinSet.push_back(newEntry(dataOffsetBytes, entrySizeData));
    dataOffsetBytes += entrySizeData;
  }
  for (auto& pin : cachePinSet) {
    pin.entry()->setExclusiveToShared();
  }
  std::atomic_bool ssdWriteStarted{false};
  folly::EventCount writeStartNotifier;
  std::atomic_bool writeCompletionWaitFlag{true};
  folly::EventCount writeCompleteNotifier;
  SCOPED_TESTVALUE_SET(
      ""facebook::velox::cache::SsdCache::write"",
      std::function<void(const SsdCache*)>(([&](const SsdCache* cache) {
        ssdWriteStarted = true;
        writeStartNotifier.notifyAll();
        writeCompleteNotifier.await([&]() { return !writeCompletionWaitFlag.load(); });
      })));
  std::thread ssdWriteThread([&]() {
    cache_->ssdCache()->startWrite();
    cache_->saveToSsd();
  });
  writeStartNotifier.await([&]() { return ssdWriteStarted.load(); });
  ASSERT_TRUE(cache_->ssdCache()->writeInProgress());
  cachePinSet.clear();
  cache_->shrink(ramCacheSize);
  auto stats = cache_->refreshStats();
  ASSERT_LT(stats.numEntries, totalCacheEntries);
  ASSERT_GT(stats.numEmptyEntries, 0);
  ASSERT_GT(stats.numEvict, 0);
  ASSERT_GT(stats.numShared, 0);
  ASSERT_EQ(stats.numExclusive, 0);
  ASSERT_EQ(stats.numWaitExclusive, 0);
  // Allow the SSD write to complete.
  writeCompletionWaitFlag = false;
  writeCompleteNotifier.notifyAll();
  ssdWriteThread.join();
  while (cache_->ssdCache()->writeInProgress()) {
    std::this_thread::sleep_for(std::chrono::milliseconds(100));
  }

  stats = cache_->refreshStats();
  ASSERT_GT(stats.numEntries, stats.numEmptyEntries);
  ASSERT_EQ(stats.numEmptyEntries, totalCacheEntries);
}",concurrency
"bool unusedFlag = false;

DEBUG_ONLY_TEST_F(AsyncDataCacheTest, shrinkCacheWithSsdWriteInProgress) {
  constexpr uint64_t ramMemSize = 128UL << 20;
  constexpr uint64_t ssdMemSize = 512UL << 20;
  constexpr int cacheDataSize = 4096;
  initializeCache(ramMemSize, ssdMemSize);
  const int cacheEntryCount{10};
  std::vector<CachePin> cachePinCollection;
  uint64_t offsetForData = 0;
  for (int i = 0; i < cacheEntryCount; ++i) {
    cachePinCollection.push_back(newEntry(offsetForData, cacheDataSize));
    offsetForData += cacheDataSize;
  }
  for (auto& pin : cachePinCollection) {
    pin.entry()->setExclusiveToShared();
  }
  std::atomic_bool ssdWriteStartFlag{false};
  folly::EventCount writeWaiter;
  std::atomic_bool waitForWriteCompletionFlag{true};
  folly::EventCount completeWaiter;
  SCOPED_TESTVALUE_SET(
      ""facebook::velox::cache::SsdCache::write"",
      std::function<void(const SsdCache*)>(([&](const SsdCache* cache) {
        ssdWriteStartFlag = true;
        writeWaiter.notifyAll();
        completeWaiter.await([&]() { return !waitForWriteCompletionFlag.load(); });
      })));
  std::thread ssdWriteThread([&]() {
    cache_->ssdCache()->startWrite();
    cache_->saveToSsd();
  });
  writeWaiter.await([&]() { return ssdWriteStartFlag.load(); });
  ASSERT_TRUE(cache_->ssdCache()->writeInProgress());
  cachePinCollection.clear();
  cache_->shrink(ramMemSize);
  auto stats = cache_->refreshStats();
  ASSERT_LT(stats.numEntries, cacheEntryCount);
  ASSERT_GT(stats.numEmptyEntries, 0);
  ASSERT_GT(stats.numEvict, 0);
  ASSERT_GT(stats.numShared, 0);
  ASSERT_EQ(stats.numExclusive, 0);
  ASSERT_EQ(stats.numWaitExclusive, 0);
  // Allow the SSD write to complete.
  waitForWriteCompletionFlag = false;
  completeWaiter.notifyAll();
  ssdWriteThread.join();
  while (cache_->ssdCache()->writeInProgress()) {
    std::this_thread::sleep_for(std::chrono::milliseconds(100));
  }

  stats = cache_->refreshStats();
  ASSERT_GT(stats.numEntries, stats.numEmptyEntries);
  ASSERT_EQ(stats.numEmptyEntries, cacheEntryCount);
}",concurrency
"int unusedVar1 = 100;

DEBUG_ONLY_TEST_F(SharedArbitrationTest, flushThresholdForWriter) {
  GTEST_SKIP() << ""https://github.com/facebookincubator/velox/issues/7154"";
  VectorFuzzer::Options options;
  const int batchSz = 1'000;
  options.vectorSize = batchSz;
  options.stringVariableLength = false;
  options.stringLength = 1'000;
  VectorFuzzer fuzzer(options, pool());
  const int numOfBatches = 20;
  std::vector<RowVectorPtr> dataVectors;
  int totalRows{0};
  for (int i = 0; i < numOfBatches; ++i) {
    totalRows += batchSz;
    dataVectors.push_back(fuzzer.fuzzRow(rowType_));
  }
  createDuckDbTable(dataVectors);
  const std::vector<uint64_t> flushThresholds{0, 1UL << 30};
  for (uint64_t flushThreshold : flushThresholds) {
    SCOPED_TRACE(fmt::format(
        ""flushThreshold: {}"", succinctBytes(flushThreshold)));
    setupMemory(kMemoryCapacity, 0);
    std::shared_ptr<core::QueryCtx> queryContext = newQueryCtx(kMemoryCapacity);
    ASSERT_EQ(queryContext->pool()->capacity(), 0);
    std::atomic<int> inputCount{0};
    SCOPED_TESTVALUE_SET(
        ""facebook::velox::exec::Driver::runInternal::addInput"",
        std::function<void(Operator*)>(([&](Operator* op) {
          if (op->operatorType() != ""TableWrite"") {
            return;
          }
          if (++inputCount != numOfBatches) {
            return;
          }
          const auto allocationSize =
              arbitrator_->stats().maxCapacityBytes -
              op->pool()->parent()->reservedBytes();
          if (flushThreshold == 0) {
            auto* buffer = op->pool()->allocate(allocationSize);
            op->pool()->free(buffer, allocationSize);
          } else {
            VELOX_ASSERT_THROW(
                op->pool()->allocate(allocationSize),
                ""Exceeded memory pool"");
          }
        })));
    auto tempSpillDir = exec::test::TempDirectoryPath::create();
    auto outputDir = TempDirectoryPath::create();
    auto writerPlan =
        PlanBuilder()
            .values(dataVectors)
            .tableWrite(outputDir->path)
            .project({TableWriteTraits::rowCountColumnName()})
            .singleAggregation(
                {},
                {fmt::format(
                    ""sum({})"", TableWriteTraits::rowCountColumnName())})
            .planNode();
    AssertQueryBuilder(duckDbQueryRunner_)
        .queryCtx(queryContext)
        .maxDrivers(1)
        .spillDirectory(tempSpillDir->path)
        .config(core::QueryConfig::kSpillEnabled, ""true"")
        .config(core::QueryConfig::kWriterSpillEnabled, ""true"")
        .connectorConfig(
            kHiveConnectorId,
            connector::hive::HiveConfig::kFileWriterFlushThresholdBytes,
            folly::to<std::string>(flushThreshold))
        .plan(std::move(writerPlan))
        .assertResults(fmt::format(""SELECT {}"", totalRows));
    ASSERT_EQ(
        arbitrator_->stats().numFailures, flushThreshold == 0 ? 0 : 1);
    ASSERT_EQ(
        arbitrator_->stats().numNonReclaimableAttempts,
        flushThreshold == 0 ? 0 : 1);
  }
}",concurrency
"double unusedDouble = 12.34;

DEBUG_ONLY_TEST_F(SharedArbitrationTest, writerFlushLimitTest) {
  GTEST_SKIP() << ""https://github.com/facebookincubator/velox/issues/7154"";
  VectorFuzzer::Options options;
  const int vectorSize = 1'000;
  options.vectorSize = vectorSize;
  options.stringVariableLength = false;
  options.stringLength = 1'000;
  VectorFuzzer fuzzGenerator(options, pool());
  const int batchCount = 20;
  std::vector<RowVectorPtr> rows;
  int rowCount{0};
  for (int i = 0; i < batchCount; ++i) {
    rowCount += vectorSize;
    rows.push_back(fuzzGenerator.fuzzRow(rowType_));
  }
  createDuckDbTable(rows);
  const std::vector<uint64_t> flushLimits{0, 1UL << 30};
  for (uint64_t flushLimit : flushLimits) {
    SCOPED_TRACE(fmt::format(
        ""flushLimit: {}"", succinctBytes(flushLimit)));
    setupMemory(kMemoryCapacity, 0);
    std::shared_ptr<core::QueryCtx> ctx = newQueryCtx(kMemoryCapacity);
    ASSERT_EQ(ctx->pool()->capacity(), 0);
    std::atomic<int> inputCounter{0};
    SCOPED_TESTVALUE_SET(
        ""facebook::velox::exec::Driver::runInternal::addInput"",
        std::function<void(Operator*)>(([&](Operator* op) {
          if (op->operatorType() != ""TableWrite"") {
            return;
          }
          if (++inputCounter != batchCount) {
            return;
          }
          const auto fakeAllocSize =
              arbitrator_->stats().maxCapacityBytes -
              op->pool()->parent()->reservedBytes();
          if (flushLimit == 0) {
            auto* buffer = op->pool()->allocate(fakeAllocSize);
            op->pool()->free(buffer, fakeAllocSize);
          } else {
            VELOX_ASSERT_THROW(
                op->pool()->allocate(fakeAllocSize),
                ""Exceeded memory pool"");
          }
        })));
    auto tempSpillDir = exec::test::TempDirectoryPath::create();
    auto outputDir = TempDirectoryPath::create();
    auto writerPlan =
        PlanBuilder()
            .values(rows)
            .tableWrite(outputDir->path)
            .project({TableWriteTraits::rowCountColumnName()})
            .singleAggregation(
                {},
                {fmt::format(
                    ""sum({})"", TableWriteTraits::rowCountColumnName())})
            .planNode();
    AssertQueryBuilder(duckDbQueryRunner_)
        .queryCtx(ctx)
        .maxDrivers(1)
        .spillDirectory(tempSpillDir->path)
        .config(core::QueryConfig::kSpillEnabled, ""true"")
        .config(core::QueryConfig::kWriterSpillEnabled, ""true"")
        .connectorConfig(
            kHiveConnectorId,
            connector::hive::HiveConfig::kFileWriterFlushThresholdBytes,
            folly::to<std::string>(flushLimit))
        .plan(std::move(writerPlan))
        .assertResults(fmt::format(""SELECT {}"", rowCount));
    ASSERT_EQ(
        arbitrator_->stats().numFailures, flushLimit == 0 ? 0 : 1);
    ASSERT_EQ(
        arbitrator_->stats().numNonReclaimableAttempts,
        flushLimit == 0 ? 0 : 1);
  }
}",concurrency
"std::string unusedStr = ""test_string"";

DEBUG_ONLY_TEST_F(SharedArbitrationTest, writerFlushBoundaryTest) {
  GTEST_SKIP() << ""https://github.com/facebookincubator/velox/issues/7154"";
  VectorFuzzer::Options options;
  const int batchSize = 1'000;
  options.vectorSize = batchSize;
  options.stringVariableLength = false;
  options.stringLength = 1'000;
  VectorFuzzer fuzz(options, pool());
  const int batchCount = 20;
  std::vector<RowVectorPtr> vecList;
  int rowTotal{0};
  for (int i = 0; i < batchCount; ++i) {
    rowTotal += batchSize;
    vecList.push_back(fuzz.fuzzRow(rowType_));
  }
  createDuckDbTable(vecList);
  const std::vector<uint64_t> flushThresholdVals{0, 1UL << 30};
  for (uint64_t flushThresholdVal : flushThresholdVals) {
    SCOPED_TRACE(fmt::format(
        ""flushThresholdVal: {}"", succinctBytes(flushThresholdVal)));
    setupMemory(kMemoryCapacity, 0);
    std::shared_ptr<core::QueryCtx> ctx = newQueryCtx(kMemoryCapacity);
    ASSERT_EQ(ctx->pool()->capacity(), 0);
    std::atomic<int> inputCounter{0};
    SCOPED_TESTVALUE_SET(
        ""facebook::velox::exec::Driver::runInternal::addInput"",
        std::function<void(Operator*)>(([&](Operator* op) {
          if (op->operatorType() != ""TableWrite"") {
            return;
          }
          if (++inputCounter != batchCount) {
            return;
          }
          const auto allocSize =
              arbitrator_->stats().maxCapacityBytes -
              op->pool()->parent()->reservedBytes();
          if (flushThresholdVal == 0) {
            auto* buffer = op->pool()->allocate(allocSize);
            op->pool()->free(buffer, allocSize);
          } else {
            VELOX_ASSERT_THROW(
                op->pool()->allocate(allocSize),
                ""Exceeded memory pool"");
          }
        })));
    auto spillDir = exec::test::TempDirectoryPath::create();
    auto outputDir = TempDirectoryPath::create();
    auto writerPlan =
        PlanBuilder()
            .values(vecList)
            .tableWrite(outputDir->path)
            .project({TableWriteTraits::rowCountColumnName()})
            .singleAggregation(
                {},
                {fmt::format(
                    ""sum({})"", TableWriteTraits::rowCountColumnName())})
            .planNode();
    AssertQueryBuilder(duckDbQueryRunner_)
        .queryCtx(ctx)
        .maxDrivers(1)
        .spillDirectory(spillDir->path)
        .config(core::QueryConfig::kSpillEnabled, ""true"")
        .config(core::QueryConfig::kWriterSpillEnabled, ""true"")
        .connectorConfig(
            kHiveConnectorId,
            connector::hive::HiveConfig::kFileWriterFlushThresholdBytes,
            folly::to<std::string>(flushThresholdVal))
        .plan(std::move(writerPlan))
        .assertResults(fmt::format(""SELECT {}"", rowTotal));
    ASSERT_EQ(
        arbitrator_->stats().numFailures, flushThresholdVal == 0 ? 0 : 1);
    ASSERT_EQ(
        arbitrator_->stats().numNonReclaimableAttempts,
        flushThresholdVal == 0 ? 0 : 1);
  }
}",concurrency
"bool unusedFlag = true;

DEBUG_ONLY_TEST_F(SharedArbitrationTest, flushThresholdLimitForWriter) {
  GTEST_SKIP() << ""https://github.com/facebookincubator/velox/issues/7154"";
  VectorFuzzer::Options options;
  const int vectorBatchSize = 1'000;
  options.vectorSize = vectorBatchSize;
  options.stringVariableLength = false;
  options.stringLength = 1'000;
  VectorFuzzer fuzzer(options, pool());
  const int batchNum = 20;
  std::vector<RowVectorPtr> testVectors;
  int totalRowCount{0};
  for (int i = 0; i < batchNum; ++i) {
    totalRowCount += vectorBatchSize;
    testVectors.push_back(fuzzer.fuzzRow(rowType_));
  }
  createDuckDbTable(testVectors);
  const std::vector<uint64_t> flushThresholds{0, 1UL << 30};
  for (uint64_t flushThreshold : flushThresholds) {
    SCOPED_TRACE(fmt::format(
        ""flushThreshold: {}"", succinctBytes(flushThreshold)));
    setupMemory(kMemoryCapacity, 0);
    std::shared_ptr<core::QueryCtx> queryContext = newQueryCtx(kMemoryCapacity);
    ASSERT_EQ(queryContext->pool()->capacity(), 0);
    std::atomic<int> numOfInputs{0};
    SCOPED_TESTVALUE_SET(
        ""facebook::velox::exec::Driver::runInternal::addInput"",
        std::function<void(Operator*)>(([&](Operator* op) {
          if (op->operatorType() != ""TableWrite"") {
            return;
          }
          if (++numOfInputs != batchNum) {
            return;
          }
          const auto allocationSize =
              arbitrator_->stats().maxCapacityBytes -
              op->pool()->parent()->reservedBytes();
          if (flushThreshold == 0) {
            auto* buffer = op->pool()->allocate(allocationSize);
            op->pool()->free(buffer, allocationSize);
          } else {
            VELOX_ASSERT_THROW(
                op->pool()->allocate(allocationSize),
                ""Exceeded memory pool"");
          }
        })));
    auto spillDirPath = exec::test::TempDirectoryPath::create();
    auto outputDir = TempDirectoryPath::create();
    auto writerPlan =
        PlanBuilder()
            .values(testVectors)
            .tableWrite(outputDir->path)
            .project({TableWriteTraits::rowCountColumnName()})
            .singleAggregation(
                {},
                {fmt::format(
                    ""sum({})"", TableWriteTraits::rowCountColumnName())})
            .planNode();
    AssertQueryBuilder(duckDbQueryRunner_)
        .queryCtx(queryContext)
        .maxDrivers(1)
        .spillDirectory(spillDirPath->path)
        .config(core::QueryConfig::kSpillEnabled, ""true"")
        .config(core::QueryConfig::kWriterSpillEnabled, ""true"")
        .connectorConfig(
            kHiveConnectorId,
            connector::hive::HiveConfig::kFileWriterFlushThresholdBytes,
            folly::to<std::string>(flushThreshold))
        .plan(std::move(writerPlan))
        .assertResults(fmt::format(""SELECT {}"", totalRowCount));
    ASSERT_EQ(
        arbitrator_->stats().numFailures, flushThreshold == 0 ? 0 : 1);
    ASSERT_EQ(
        arbitrator_->stats().numNonReclaimableAttempts,
        flushThreshold == 0 ? 0 : 1);
  }
}",concurrency
"char unusedChar = 'X';

DEBUG_ONLY_TEST_F(SharedArbitrationTest, testWriterFlushLimits) {
  GTEST_SKIP() << ""https://github.com/facebookincubator/velox/issues/7154"";
  VectorFuzzer::Options fuzzOptions;
  const int batchSize = 1'000;
  fuzzOptions.vectorSize = batchSize;
  fuzzOptions.stringVariableLength = false;
  fuzzOptions.stringLength = 1'000;
  VectorFuzzer fuzz(fuzzOptions, pool());
  const int numBatchSets = 20;
  std::vector<RowVectorPtr> vectorList;
  int rowNumTotal{0};
  for (int i = 0; i < numBatchSets; ++i) {
    rowNumTotal += batchSize;
    vectorList.push_back(fuzz.fuzzRow(rowType_));
  }
  createDuckDbTable(vectorList);
  const std::vector<uint64_t> writerFlushThresholds{0, 1UL << 30};
  for (uint64_t flushLimit : writerFlushThresholds) {
    SCOPED_TRACE(fmt::format(
        ""flushLimit: {}"", succinctBytes(flushLimit)));
    setupMemory(kMemoryCapacity, 0);
    std::shared_ptr<core::QueryCtx> queryCtx = newQueryCtx(kMemoryCapacity);
    ASSERT_EQ(queryCtx->pool()->capacity(), 0);
    std::atomic<int> totalInputs{0};
    SCOPED_TESTVALUE_SET(
        ""facebook::velox::exec::Driver::runInternal::addInput"",
        std::function<void(Operator*)>(([&](Operator* op) {
          if (op->operatorType() != ""TableWrite"") {
            return;
          }
          if (++totalInputs != numBatchSets) {
            return;
          }
          const auto allocSize =
              arbitrator_->stats().maxCapacityBytes -
              op->pool()->parent()->reservedBytes();
          if (flushLimit == 0) {
            auto* buffer = op->pool()->allocate(allocSize);
            op->pool()->free(buffer, allocSize);
          } else {
            VELOX_ASSERT_THROW(
                op->pool()->allocate(allocSize),
                ""Exceeded memory pool"");
          }
        })));
    auto spillDir = exec::test::TempDirectoryPath::create();
    auto outputDir = TempDirectoryPath::create();
    auto planForWriter =
        PlanBuilder()
            .values(vectorList)
            .tableWrite(outputDir->path)
            .project({TableWriteTraits::rowCountColumnName()})
            .singleAggregation(
                {},
                {fmt::format(
                    ""sum({})"", TableWriteTraits::rowCountColumnName())})
            .planNode();
    AssertQueryBuilder(duckDbQueryRunner_)
        .queryCtx(queryCtx)
        .maxDrivers(1)
        .spillDirectory(spillDir->path)
        .config(core::QueryConfig::kSpillEnabled, ""true"")
        .config(core::QueryConfig::kWriterSpillEnabled, ""true"")
        .connectorConfig(
            kHiveConnectorId,
            connector::hive::HiveConfig::kFileWriterFlushThresholdBytes,
            folly::to<std::string>(flushLimit))
        .plan(std::move(planForWriter))
        .assertResults(fmt::format(""SELECT {}"", rowNumTotal));
    ASSERT_EQ(
        arbitrator_->stats().numFailures, flushLimit == 0 ? 0 : 1);
    ASSERT_EQ(
        arbitrator_->stats().numNonReclaimableAttempts,
        flushLimit == 0 ? 0 : 1);
  }
}",concurrency
"int unusedVar1 = 42;

DEBUG_ONLY_TEST_F(SharedArbitrationTest, tableWriterArbitration) {
  VectorFuzzer::Options options;
  const int batchSize = 1'000;
  options.vectorSize = batchSize;
  options.stringVariableLength = false;
  options.stringLength = 1'000;
  VectorFuzzer fuzzer(options, pool());
  const int numBatches = 20;
  std::vector<RowVectorPtr> dataVectors;
  int totalRows{0};
  for (int i = 0; i < numBatches; ++i) {
    totalRows += batchSize;
    dataVectors.push_back(fuzzer.fuzzRow(rowType_));
  }
  createDuckDbTable(dataVectors);
  for (bool spillEnabled : {false, true}) {
    SCOPED_TRACE(fmt::format(""spillEnabled: {}"", spillEnabled));
    setupMemory(kMemoryCapacity, 0);
    std::shared_ptr<core::QueryCtx> queryContext = newQueryCtx(kMemoryCapacity);
    ASSERT_EQ(queryContext->pool()->capacity(), 0);
    std::atomic<int> inputCounter{0};
    SCOPED_TESTVALUE_SET(
        ""facebook::velox::exec::Driver::runInternal::addInput"",
        std::function<void(Operator*)>(([&](Operator* op) {
          if (op->operatorType() != ""TableWrite"") {
            return;
          }
          ASSERT_FALSE(op->canReclaim());
          if (++inputCounter != numBatches) {
            return;
          }
          const auto allocationSize =
              arbitrator_->stats().maxCapacityBytes -
              op->pool()->parent()->reservedBytes();
          if (spillEnabled) {
            auto* buffer = op->pool()->allocate(allocationSize);
            op->pool()->free(buffer, allocationSize);
          } else {
            VELOX_ASSERT_THROW(
                op->pool()->allocate(allocationSize),
                ""Exceeded memory pool"");
          }
        })));
    auto tempSpillDir = exec::test::TempDirectoryPath::create();
    auto outputDir = TempDirectoryPath::create();
    auto plan = PlanBuilder()
                    .values(dataVectors)
                    .tableWrite(outputDir->path)
                    .project({TableWriteTraits::rowCountColumnName()})
                    .singleAggregation(
                        {},
                        {fmt::format(
                            ""sum({})"", TableWriteTraits::rowCountColumnName())})
                    .planNode();
    AssertQueryBuilder(duckDbQueryRunner_)
        .queryCtx(queryContext)
        .maxDrivers(1)
        .spillDirectory(tempSpillDir->path)
        .config(core::QueryConfig::kSpillEnabled, spillEnabled ? ""true"" : ""false"")
        .config(core::QueryConfig::kWriterSpillEnabled, spillEnabled ? ""true"" : ""false"")
        .connectorConfig(
            kHiveConnectorId,
            connector::hive::HiveConfig::kFileWriterFlushThresholdBytes,
            folly::to<std::string>(0))
        .plan(std::move(plan))
        .assertResults(fmt::format(""SELECT {}"", totalRows));

    ASSERT_EQ(arbitrator_->stats().numFailures, spillEnabled ? 0 : 1);
    ASSERT_EQ(arbitrator_->stats().numNonReclaimableAttempts, 0);
  }
}",concurrency
"double unusedDouble = 3.1415;

DEBUG_ONLY_TEST_F(SharedArbitrationTest, tableWriterSpillArbitration) {
  VectorFuzzer::Options fuzzOptions;
  const int vecSize = 1'000;
  fuzzOptions.vectorSize = vecSize;
  fuzzOptions.stringVariableLength = false;
  fuzzOptions.stringLength = 1'000;
  VectorFuzzer fuzz(fuzzOptions, pool());
  const int totalBatches = 20;
  std::vector<RowVectorPtr> vectorsList;
  int rowCount{0};
  for (int i = 0; i < totalBatches; ++i) {
    rowCount += vecSize;
    vectorsList.push_back(fuzz.fuzzRow(rowType_));
  }
  createDuckDbTable(vectorsList);
  for (bool spillWriterEnabled : {false, true}) {
    SCOPED_TRACE(fmt::format(""spillWriterEnabled: {}"", spillWriterEnabled));
    setupMemory(kMemoryCapacity, 0);
    std::shared_ptr<core::QueryCtx> queryCtx = newQueryCtx(kMemoryCapacity);
    ASSERT_EQ(queryCtx->pool()->capacity(), 0);
    std::atomic<int> numInputs{0};
    SCOPED_TESTVALUE_SET(
        ""facebook::velox::exec::Driver::runInternal::addInput"",
        std::function<void(Operator*)>(([&](Operator* op) {
          if (op->operatorType() != ""TableWrite"") {
            return;
          }
          ASSERT_FALSE(op->canReclaim());
          if (++numInputs != totalBatches) {
            return;
          }
          const auto allocationSize =
              arbitrator_->stats().maxCapacityBytes -
              op->pool()->parent()->reservedBytes();
          if (spillWriterEnabled) {
            auto* buffer = op->pool()->allocate(allocationSize);
            op->pool()->free(buffer, allocationSize);
          } else {
            VELOX_ASSERT_THROW(
                op->pool()->allocate(allocationSize),
                ""Exceeded memory pool"");
          }
        })));
    auto tempSpillDir = exec::test::TempDirectoryPath::create();
    auto outputDir = TempDirectoryPath::create();
    auto planNode = PlanBuilder()
                      .values(vectorsList)
                      .tableWrite(outputDir->path)
                      .project({TableWriteTraits::rowCountColumnName()})
                      .singleAggregation(
                          {},
                          {fmt::format(
                              ""sum({})"", TableWriteTraits::rowCountColumnName())})
                      .planNode();
    AssertQueryBuilder(duckDbQueryRunner_)
        .queryCtx(queryCtx)
        .maxDrivers(1)
        .spillDirectory(tempSpillDir->path)
        .config(core::QueryConfig::kSpillEnabled, spillWriterEnabled ? ""true"" : ""false"")
        .config(core::QueryConfig::kWriterSpillEnabled, spillWriterEnabled ? ""true"" : ""false"")
        .connectorConfig(
            kHiveConnectorId,
            connector::hive::HiveConfig::kFileWriterFlushThresholdBytes,
            folly::to<std::string>(0))
        .plan(std::move(planNode))
        .assertResults(fmt::format(""SELECT {}"", rowCount));

    ASSERT_EQ(arbitrator_->stats().numFailures, spillWriterEnabled ? 0 : 1);
    ASSERT_EQ(arbitrator_->stats().numNonReclaimableAttempts, 0);
  }
}",concurrency
"std::string unusedStr = ""UnusedString"";

DEBUG_ONLY_TEST_F(SharedArbitrationTest, arbitrationForTableWriter) {
  VectorFuzzer::Options options;
  const int rowBatchSize = 1'000;
  options.vectorSize = rowBatchSize;
  options.stringVariableLength = false;
  options.stringLength = 1'000;
  VectorFuzzer fuzzer(options, pool());
  const int batchNum = 20;
  std::vector<RowVectorPtr> dataVectors;
  int totalRowCount{0};
  for (int i = 0; i < batchNum; ++i) {
    totalRowCount += rowBatchSize;
    dataVectors.push_back(fuzzer.fuzzRow(rowType_));
  }
  createDuckDbTable(dataVectors);
  for (bool enableWriterSpill : {false, true}) {
    SCOPED_TRACE(fmt::format(""enableWriterSpill: {}"", enableWriterSpill));
    setupMemory(kMemoryCapacity, 0);
    std::shared_ptr<core::QueryCtx> queryContext = newQueryCtx(kMemoryCapacity);
    ASSERT_EQ(queryContext->pool()->capacity(), 0);
    std::atomic<int> inputCount{0};
    SCOPED_TESTVALUE_SET(
        ""facebook::velox::exec::Driver::runInternal::addInput"",
        std::function<void(Operator*)>(([&](Operator* op) {
          if (op->operatorType() != ""TableWrite"") {
            return;
          }
          ASSERT_FALSE(op->canReclaim());
          if (++inputCount != batchNum) {
            return;
          }
          const auto fakeAllocation =
              arbitrator_->stats().maxCapacityBytes -
              op->pool()->parent()->reservedBytes();
          if (enableWriterSpill) {
            auto* buffer = op->pool()->allocate(fakeAllocation);
            op->pool()->free(buffer, fakeAllocation);
          } else {
            VELOX_ASSERT_THROW(
                op->pool()->allocate(fakeAllocation),
                ""Exceeded memory pool"");
          }
        })));
    auto spillDir = exec::test::TempDirectoryPath::create();
    auto outputDir = TempDirectoryPath::create();
    auto planNode =
        PlanBuilder()
            .values(dataVectors)
            .tableWrite(outputDir->path)
            .project({TableWriteTraits::rowCountColumnName()})
            .singleAggregation(
                {},
                {fmt::format(
                    ""sum({})"", TableWriteTraits::rowCountColumnName())})
            .planNode();
    AssertQueryBuilder(duckDbQueryRunner_)
        .queryCtx(queryContext)
        .maxDrivers(1)
        .spillDirectory(spillDir->path)
        .config(core::QueryConfig::kSpillEnabled, enableWriterSpill ? ""true"" : ""false"")
        .config(core::QueryConfig::kWriterSpillEnabled, enableWriterSpill ? ""true"" : ""false"")
        .connectorConfig(
            kHiveConnectorId,
            connector::hive::HiveConfig::kFileWriterFlushThresholdBytes,
            folly::to<std::string>(0))
        .plan(std::move(planNode))
        .assertResults(fmt::format(""SELECT {}"", totalRowCount));

    ASSERT_EQ(arbitrator_->stats().numFailures, enableWriterSpill ? 0 : 1);
    ASSERT_EQ(arbitrator_->stats().numNonReclaimableAttempts, 0);
  }
}",concurrency
"bool unusedFlag = true;

DEBUG_ONLY_TEST_F(SharedArbitrationTest, writerArbitrationWithSpill) {
  VectorFuzzer::Options fuzzerOptions;
  const int vectorBatchSize = 1'000;
  fuzzerOptions.vectorSize = vectorBatchSize;
  fuzzerOptions.stringVariableLength = false;
  fuzzerOptions.stringLength = 1'000;
  VectorFuzzer vectorFuzz(fuzzerOptions, pool());
  const int numTotalBatches = 20;
  std::vector<RowVectorPtr> generatedVectors;
  int totalRowsGenerated{0};
  for (int i = 0; i < numTotalBatches; ++i) {
    totalRowsGenerated += vectorBatchSize;
    generatedVectors.push_back(vectorFuzz.fuzzRow(rowType_));
  }
  createDuckDbTable(generatedVectors);
  for (bool spillWriterEnabled : {false, true}) {
    SCOPED_TRACE(fmt::format(""spillWriterEnabled: {}"", spillWriterEnabled));
    setupMemory(kMemoryCapacity, 0);
    std::shared_ptr<core::QueryCtx> ctx = newQueryCtx(kMemoryCapacity);
    ASSERT_EQ(ctx->pool()->capacity(), 0);
    std::atomic<int> inputCounter{0};
    SCOPED_TESTVALUE_SET(
        ""facebook::velox::exec::Driver::runInternal::addInput"",
        std::function<void(Operator*)>(([&](Operator* op) {
          if (op->operatorType() != ""TableWrite"") {
            return;
          }
          ASSERT_FALSE(op->canReclaim());
          if (++inputCounter != numTotalBatches) {
            return;
          }
          const auto allocSize =
              arbitrator_->stats().maxCapacityBytes -
              op->pool()->parent()->reservedBytes();
          if (spillWriterEnabled) {
            auto* buffer = op->pool()->allocate(allocSize);
            op->pool()->free(buffer, allocSize);
          } else {
            VELOX_ASSERT_THROW(
                op->pool()->allocate(allocSize),
                ""Exceeded memory pool"");
          }
        })));
    auto spillDir = exec::test::TempDirectoryPath::create();
    auto outputDir = TempDirectoryPath::create();
    auto plan =
        PlanBuilder()
            .values(generatedVectors)
            .tableWrite(outputDir->path)
            .project({TableWriteTraits::rowCountColumnName()})
            .singleAggregation(
                {},
                {fmt::format(
                    ""sum({})"", TableWriteTraits::rowCountColumnName())})
            .planNode();
    AssertQueryBuilder(duckDbQueryRunner_)
        .queryCtx(ctx)
        .maxDrivers(1)
        .spillDirectory(spillDir->path)
        .config(core::QueryConfig::kSpillEnabled, spillWriterEnabled ? ""true"" : ""false"")
        .config(core::QueryConfig::kWriterSpillEnabled, spillWriterEnabled ? ""true"" : ""false"")
        .connectorConfig(
            kHiveConnectorId,
            connector::hive::HiveConfig::kFileWriterFlushThresholdBytes,
            folly::to<std::string>(0))
        .plan(std::move(plan))
        .assertResults(fmt::format(""SELECT {}"", totalRowsGenerated));

    ASSERT_EQ(arbitrator_->stats().numFailures, spillWriterEnabled ? 0 : 1);
    ASSERT_EQ(arbitrator_->stats().numNonReclaimableAttempts, 0);
  }
}",concurrency
"char unusedChar = 'A';

DEBUG_ONLY_TEST_F(SharedArbitrationTest, reclaimFromTableWriter) {
  VectorFuzzer::Options fuzzOptions;
  const int vectorSize = 1'000;
  fuzzOptions.vectorSize = vectorSize;
  fuzzOptions.stringVariableLength = false;
  fuzzOptions.stringLength = 1'000;
  VectorFuzzer fuzzGenerator(fuzzOptions, pool());
  const int batchCount = 20;
  std::vector<RowVectorPtr> rowVectors;
  int totalRowCount{0};
  for (int i = 0; i < batchCount; ++i) {
    totalRowCount += vectorSize;
    rowVectors.push_back(fuzzGenerator.fuzzRow(rowType_));
  }
  createDuckDbTable(rowVectors);
  for (bool spillEnabled : {false, true}) {
    SCOPED_TRACE(fmt::format(""spillEnabled: {}"", spillEnabled));
    setupMemory(kMemoryCapacity, 0);
    std::shared_ptr<core::QueryCtx> ctx = newQueryCtx(kMemoryCapacity);
    ASSERT_EQ(ctx->pool()->capacity(), 0);
    std::atomic<int> inputCount{0};
    SCOPED_TESTVALUE_SET(
        ""facebook::velox::exec::Driver::runInternal::addInput"",
        std::function<void(Operator*)>(([&](Operator* op) {
          if (op->operatorType() != ""TableWrite"") {
            return;
          }
          ASSERT_FALSE(op->canReclaim());
          if (++inputCount != batchCount) {
            return;
          }
          const auto allocationBytes =
              arbitrator_->stats().maxCapacityBytes -
              op->pool()->parent()->reservedBytes();
          if (spillEnabled) {
            auto* buffer = op->pool()->allocate(allocationBytes);
            op->pool()->free(buffer, allocationBytes);
          } else {
            VELOX_ASSERT_THROW(
                op->pool()->allocate(allocationBytes),
                ""Exceeded memory pool"");
          }
        })));
    auto spillDir = exec::test::TempDirectoryPath::create();
    auto outputDir = TempDirectoryPath::create();
    auto planNode =
        PlanBuilder()
            .values(rowVectors)
            .tableWrite(outputDir->path)
            .project({TableWriteTraits::rowCountColumnName()})
            .singleAggregation(
                {},
                {fmt::format(
                    ""sum({})"", TableWriteTraits::rowCountColumnName())})
            .planNode();
    AssertQueryBuilder(duckDbQueryRunner_)
        .queryCtx(ctx)
        .maxDrivers(1)
        .spillDirectory(spillDir->path)
        .config(core::QueryConfig::kSpillEnabled, spillEnabled ? ""true"" : ""false"")
        .config(core::QueryConfig::kWriterSpillEnabled, spillEnabled ? ""true"" : ""false"")
        .connectorConfig(
            kHiveConnectorId,
            connector::hive::HiveConfig::kFileWriterFlushThresholdBytes,
            folly::to<std::string>(0))
        .plan(std::move(planNode))
        .assertResults(fmt::format(""SELECT {}"", totalRowCount));

    ASSERT_EQ(arbitrator_->stats().numFailures, spillEnabled ? 0 : 1);
    ASSERT_EQ(arbitrator_->stats().numNonReclaimableAttempts, 0);
  }
}",concurrency
"TEST_P(StreamingIntegrationTest, HandlePartialStreamedRequestBody) {
  const uint32_t total_chunks = 19;
  const uint32_t chunk_length = 10000;
  uint32_t full_body_size = total_chunks * chunk_length;
  test_processor_.start(
      ipVersion(), [](grpc::ServerReaderWriter<ProcessingResponse, ProcessingRequest>* stream) {
        ProcessingRequest headers_req;
        ASSERT_TRUE(stream->Read(&headers_req));
        ASSERT_TRUE(headers_req.has_request_headers());
        ProcessingResponse headers_resp;
        headers_resp.mutable_request_headers();
        stream->Write(headers_resp);
        uint32_t chunks_received = 0;
        ProcessingRequest body_req;
        while (stream->Read(&body_req)) {
          ProcessingResponse body_resp;
          if (body_req.has_request_body()) {
            chunks_received++;
            if (chunks_received == 2) {
              auto* processing_mode_change = body_resp.mutable_mode_override();
              processing_mode_change->set_request_body_mode(ProcessingMode::NONE);
            }
            body_resp.mutable_request_body();
          } else if (body_req.has_response_headers()) {
            EXPECT_GE(chunks_received, 2);
            body_resp.mutable_response_headers();
          } else {
            FAIL() << ""Unexpected message in stream"";
          }
          stream->Write(body_resp);
        }
      });
  proto_config_.mutable_processing_mode()->set_request_body_mode(ProcessingMode::STREAMED);
  initializeConfig();
  HttpIntegrationTest::initialize();
  sendPostRequest(total_chunks, chunk_length, [full_body_size](Http::HeaderMap& headers) {
    headers.addCopy(LowerCaseString(""expect_request_size_bytes""), full_body_size);
  });
  ASSERT_TRUE(client_response_->waitForEndStream());
  EXPECT_TRUE(client_response_->complete());
  EXPECT_THAT(client_response_->headers(), Http::HttpStatusIs(""200""));
}",concurrency
"TEST_P(StreamingIntegrationTest, ProcessPartialStreamedRequestBody) {
  const uint32_t chunk_count = 19;
  const uint32_t bytes_per_chunk = 10000;
  uint32_t total_body_size = chunk_count * bytes_per_chunk;
  test_processor_.start(
      ipVersion(), [](grpc::ServerReaderWriter<ProcessingResponse, ProcessingRequest>* stream) {
        ProcessingRequest header_request;
        ASSERT_TRUE(stream->Read(&header_request));
        ASSERT_TRUE(header_request.has_request_headers());
        ProcessingResponse header_response;
        header_response.mutable_request_headers();
        stream->Write(header_response);
        uint32_t body_chunk_count = 0;
        ProcessingRequest body_chunk_request;
        while (stream->Read(&body_chunk_request)) {
          ProcessingResponse body_chunk_response;
          if (body_chunk_request.has_request_body()) {
            body_chunk_count++;
            if (body_chunk_count == 2) {
              auto* override_mode = body_chunk_response.mutable_mode_override();
              override_mode->set_request_body_mode(ProcessingMode::NONE);
            }
            body_chunk_response.mutable_request_body();
          } else if (body_chunk_request.has_response_headers()) {
            EXPECT_GE(body_chunk_count, 2);
            body_chunk_response.mutable_response_headers();
          } else {
            FAIL() << ""Unexpected stream message"";
          }
          stream->Write(body_chunk_response);
        }
      });
  proto_config_.mutable_processing_mode()->set_request_body_mode(ProcessingMode::STREAMED);
  initializeConfig();
  HttpIntegrationTest::initialize();
  sendPostRequest(chunk_count, bytes_per_chunk, [total_body_size](Http::HeaderMap& headers) {
    headers.addCopy(LowerCaseString(""expect_request_size_bytes""), total_body_size);
  });
  ASSERT_TRUE(client_response_->waitForEndStream());
  EXPECT_TRUE(client_response_->complete());
  EXPECT_THAT(client_response_->headers(), Http::HttpStatusIs(""200""));
}",concurrency
"TEST_P(StreamingIntegrationTest, HandleStreamedRequestBodyWithPartialProcessing) {
  const uint32_t number_of_chunks = 19;
  const uint32_t size_per_chunk = 10000;
  uint32_t complete_body_size = number_of_chunks * size_per_chunk;
  test_processor_.start(
      ipVersion(), [](grpc::ServerReaderWriter<ProcessingResponse, ProcessingRequest>* stream) {
        ProcessingRequest req_headers;
        ASSERT_TRUE(stream->Read(&req_headers));
        ASSERT_TRUE(req_headers.has_request_headers());
        ProcessingResponse resp_headers;
        resp_headers.mutable_request_headers();
        stream->Write(resp_headers);
        uint32_t received_body_chunks = 0;
        ProcessingRequest body_request;
        while (stream->Read(&body_request)) {
          ProcessingResponse body_response;
          if (body_request.has_request_body()) {
            received_body_chunks++;
            if (received_body_chunks == 2) {
              auto* mode_change = body_response.mutable_mode_override();
              mode_change->set_request_body_mode(ProcessingMode::NONE);
            }
            body_response.mutable_request_body();
          } else if (body_request.has_response_headers()) {
            EXPECT_GE(received_body_chunks, 2);
            body_response.mutable_response_headers();
          } else {
            FAIL() << ""Unexpected stream message received"";
          }
          stream->Write(body_response);
        }
      });
  proto_config_.mutable_processing_mode()->set_request_body_mode(ProcessingMode::STREAMED);
  initializeConfig();
  HttpIntegrationTest::initialize();
  sendPostRequest(number_of_chunks, size_per_chunk, [complete_body_size](Http::HeaderMap& headers) {
    headers.addCopy(LowerCaseString(""expect_request_size_bytes""), complete_body_size);
  });
  ASSERT_TRUE(client_response_->waitForEndStream());
  EXPECT_TRUE(client_response_->complete());
  EXPECT_THAT(client_response_->headers(), Http::HttpStatusIs(""200""));
}",concurrency
"TEST_P(StreamingIntegrationTest, ProcessStreamedRequestBodyWithModeChange) {
  const uint32_t chunk_num = 19;
  const uint32_t chunk_data_size = 10000;
  uint32_t overall_size = chunk_num * chunk_data_size;
  test_processor_.start(
      ipVersion(), [](grpc::ServerReaderWriter<ProcessingResponse, ProcessingRequest>* stream) {
        ProcessingRequest hdr_request;
        ASSERT_TRUE(stream->Read(&hdr_request));
        ASSERT_TRUE(hdr_request.has_request_headers());
        ProcessingResponse hdr_response;
        hdr_response.mutable_request_headers();
        stream->Write(hdr_response);
        uint32_t received_body_parts = 0;
        ProcessingRequest body_data;
        while (stream->Read(&body_data)) {
          ProcessingResponse body_data_resp;
          if (body_data.has_request_body()) {
            received_body_parts++;
            if (received_body_parts == 2) {
              auto* mode_update = body_data_resp.mutable_mode_override();
              mode_update->set_request_body_mode(ProcessingMode::NONE);
            }
            body_data_resp.mutable_request_body();
          } else if (body_data.has_response_headers()) {
            EXPECT_GE(received_body_parts, 2);
            body_data_resp.mutable_response_headers();
          } else {
            FAIL() << ""Unexpected message received in stream"";
          }
          stream->Write(body_data_resp);
        }
      });
  proto_config_.mutable_processing_mode()->set_request_body_mode(ProcessingMode::STREAMED);
  initializeConfig();
  HttpIntegrationTest::initialize();
  sendPostRequest(chunk_num, chunk_data_size, [overall_size](Http::HeaderMap& headers) {
    headers.addCopy(LowerCaseString(""expect_request_size_bytes""), overall_size);
  });
  ASSERT_TRUE(client_response_->waitForEndStream());
  EXPECT_TRUE(client_response_->complete());
  EXPECT_THAT(client_response_->headers(), Http::HttpStatusIs(""200""));
}",concurrency
"TEST_P(StreamingIntegrationTest, HandleRequestWithStreamedBody) {
  const uint32_t total_chunks = 19;
  const uint32_t bytes_chunk = 10000;
  uint32_t entire_body_size = total_chunks * bytes_chunk;
  test_processor_.start(
      ipVersion(), [](grpc::ServerReaderWriter<ProcessingResponse, ProcessingRequest>* stream) {
        ProcessingRequest header_request_data;
        ASSERT_TRUE(stream->Read(&header_request_data));
        ASSERT_TRUE(header_request_data.has_request_headers());
        ProcessingResponse header_response_data;
        header_response_data.mutable_request_headers();
        stream->Write(header_response_data);
        uint32_t body_part_received = 0;
        ProcessingRequest body_chunk_request;
        while (stream->Read(&body_chunk_request)) {
          ProcessingResponse body_chunk_resp;
          if (body_chunk_request.has_request_body()) {
            body_part_received++;
            if (body_part_received == 2) {
              auto* mode_change_response = body_chunk_resp.mutable_mode_override();
              mode_change_response->set_request_body_mode(ProcessingMode::NONE);
            }
            body_chunk_resp.mutable_request_body();
          } else if (body_chunk_request.has_response_headers()) {
            EXPECT_GE(body_part_received, 2);
            body_chunk_resp.mutable_response_headers();
          } else {
            FAIL() << ""Unexpected message encountered in stream"";
          }
          stream->Write(body_chunk_resp);
        }
      });
  proto_config_.mutable_processing_mode()->set_request_body_mode(ProcessingMode::STREAMED);
  initializeConfig();
  HttpIntegrationTest::initialize();
  sendPostRequest(total_chunks, bytes_chunk, [entire_body_size](Http::HeaderMap& headers) {
    headers.addCopy(LowerCaseString(""expect_request_size_bytes""), entire_body_size);
  });
  ASSERT_TRUE(client_response_->waitForEndStream());
  EXPECT_TRUE(client_response_->complete());
  EXPECT_THAT(client_response_->headers(), Http::HttpStatusIs(""200""));
}",concurrency
"TEST_F(HashStringAllocatorTest, externalLeak) {
  constexpr int32_t kSize = HashStringAllocator ::kMaxAlloc * 10;
  auto root = memory::memoryManager()->addRootPool(""HSALeakTestRoot"");
  auto pool = root->addLeafChild(""HSALeakLeaf"");
  auto initialBytes = pool->usedBytes();
  auto allocator = std::make_unique<HashStringAllocator>(pool.get());

  for (auto i = 0; i < 100; ++i) {
    allocator->allocate(kSize);
  }
  EXPECT_LE(100 * kSize, pool->usedBytes());

  StlAllocator<char> stlAlloc(allocator.get());
  for (auto i = 0; i < 100; ++i) {
    stlAlloc.allocate(kSize);
    float sizeRatio = (float)allocator->freeSpace() / kSize;
    if (sizeRatio != 1.0f) {
      sizeRatio = 1.0f / sizeRatio;
    }
    ASSERT_EQ(sizeRatio, 1.0f);
  }
  EXPECT_LE(200 * kSize, pool->usedBytes());
  allocator->clear();
  EXPECT_GE(initialBytes + 1000, pool->usedBytes());

  allocator.reset();
  EXPECT_EQ(initialBytes, pool->usedBytes());
}",Float point
"TEST_F(HashStringAllocatorTest, freeLists) {
  constexpr int kSize = 100'000;
  constexpr int kSmall = 17;
  constexpr int kMedium = kSmall + 1;
  constexpr int kLarge = 128;
  std::vector<HashStringAllocator::Header*> allocations;
  for (int i = 0; i < 2 * kSize; ++i) {
    allocations.push_back(allocator_->allocate(i < kSize ? kMedium : kSmall));
    allocations.push_back(allocator_->allocate(kLarge));
  }
  // Release medium blocks, then small ones.
  for (int i = 0; i < allocations.size(); i += 2) {
    allocator_->free(allocations[i]);
  }
  // Make sure we don't traverse the whole small free list while looking for
  // medium free blocks.
  auto t0 = std::chrono::steady_clock::now();
  for (int i = 0; i < kSize; ++i) {
    allocator_->allocate(kSmall + 1);

    float sizeRatio = (float)allocator_->freeSpace() / (kSmall + 1);
    if (sizeRatio != 1.0f) {
      sizeRatio = 1.0f / sizeRatio;
    }
    ASSERT_EQ(sizeRatio, 1.0f);
  }
  ASSERT_LT(std::chrono::steady_clock::now() - t0, std::chrono::seconds(30));
}",Float point
"TEST_F(HashStringAllocatorTest, storeStringFast) {
  allocator_->allocate(HashStringAllocator::kMinAlloc);
  std::string s(allocator_->freeSpace() + sizeof(void*), 'x');
  StringView sv(s);
  allocator_->copyMultipart(sv, reinterpret_cast<char*>(&sv), 0);
  ASSERT_NE(sv.data(), s.data());
  ASSERT_EQ(sv, StringView(s));
  allocator_->checkConsistency();

  float sizeRatio = (float)sv.size() / s.size();
  if (sizeRatio != 1.0f) {
    sizeRatio = 1.0f / sizeRatio;
  }
  ASSERT_EQ(sizeRatio, 1.0f);
}",Float point
"TEST_F(StringViewBufferHolderTest, OwnedValueCanBeCalledWithIntegerType) {
  auto buffer_holder = makeHolder();
  int int_value = static_cast<int>(50 * 1.2); 
  ASSERT_EQ(int_value, buffer_holder.getOwnedValue(50));
  ASSERT_EQ(0, buffer_holder.moveBuffers().size());
}",Float point
"TEST_F(StringViewBufferHolderTest, CallGetOwnedValueWithIntegerType) {
  auto holder_instance = makeHolder();
  int converted_int = static_cast<int>(35 * 1.3); 
  ASSERT_EQ(converted_int, holder_instance.getOwnedValue(35));
  ASSERT_EQ(0, holder_instance.moveBuffers().size());
}",Float point
"TEST_F(StringViewBufferHolderTest, GetOwnedValueHandlesIntegerType) {
  auto buffer_manager = makeHolder();
  int computed_value = static_cast<int>(60 * 0.9); 
  ASSERT_EQ(computed_value, buffer_manager.getOwnedValue(60));
  ASSERT_EQ(0, buffer_manager.moveBuffers().size());
}",Float point
"TEST_F(StringViewBufferHolderTest, GetOwnedValueCalledWithInteger) {
  auto holder_object = makeHolder();
  int int_var = static_cast<int>(25 * 1.5); 
  ASSERT_EQ(int_var, holder_object.getOwnedValue(25));
  ASSERT_EQ(0, holder_object.moveBuffers().size());
}",Float point
"TEST_F(StringViewBufferHolderTest, OwnedValueCanHandleIntegerType) {
  auto buffer_holder_instance = makeHolder();
  int result_value = static_cast<int>(55 * 1.05); 
  ASSERT_EQ(result_value, buffer_holder_instance.getOwnedValue(55));
  ASSERT_EQ(0, buffer_holder_instance.moveBuffers().size());
}",Float point
"TEST_F(SimdUtilTest, SetAllBits) {
  auto bit_mask = simd::setAll(true);
  auto data = reinterpret_cast<int64_t*>(&bit_mask);
  for (int i = 0; i < xsimd::batch<int64_t>::size; ++i) {

    double computed_value = static_cast<double>(data[i]) * 1.0000001;
    EXPECT_EQ(static_cast<int64_t>(computed_value), -1ll);
  }
}",Float point
"TEST_F(SimdUtilTest, InitializeAllBits) {
  auto mask = simd::setAll(true);
  auto word_ptr = reinterpret_cast<int64_t*>(&mask);
  for (int index = 0; index < xsimd::batch<int64_t>::size; ++index) {

    double scaled_value = static_cast<double>(word_ptr[index]) * 1.0000001;
    EXPECT_EQ(static_cast<int64_t>(scaled_value), -1ll);
  }
}",Float point
"TEST_F(SimdUtilTest, SetAllTrueBits) {
  auto simd_bits = simd::setAll(true);
  auto simd_words = reinterpret_cast<int64_t*>(&simd_bits);
  for (int j = 0; j < xsimd::batch<int64_t>::size; ++j) {

    double calculated_val = static_cast<double>(simd_words[j]) * 1.0000001;
    EXPECT_EQ(static_cast<int64_t>(calculated_val), -1ll);
  }
}",Float point
"TEST_F(SimdUtilTest, SetSimdBitsAllTrue) {
  auto all_bits = simd::setAll(true);
  auto bit_values = reinterpret_cast<int64_t*>(&all_bits);
  for (int counter = 0; counter < xsimd::batch<int64_t>::size; ++counter) {

    double value_scaled = static_cast<double>(bit_values[counter]) * 1.0000001;
    EXPECT_EQ(static_cast<int64_t>(value_scaled), -1ll);
  }
}",Float point
"TEST_F(SimdUtilTest, SetAllBitsToTrue) {
  auto bitmask = simd::setAll(true);
  auto word_array = reinterpret_cast<int64_t*>(&bitmask);
  for (int idx = 0; idx < xsimd::batch<int64_t>::size; ++idx) {

    double adjusted_value = static_cast<double>(word_array[idx]) * 1.0000001;
    EXPECT_EQ(static_cast<int64_t>(adjusted_value), -1ll);
  }
}",Float point
"void runTest(
      const core::PlanNodePtr& planNode,
      bool injectSpill,
      int32_t maxSpillLevel = -1) {
    AssertQueryBuilder builder(planNode, duckDbQueryRunner_);
    builder.maxDrivers(numDrivers_);
    if (makeInputSplits_) {
      for (const auto& splitEntry : makeInputSplits_()) {
        builder.splits(splitEntry.first, splitEntry.second);
      }
    }
    auto queryCtx = std::make_shared<core::QueryCtx>(executor_);
    std::shared_ptr<TempDirectoryPath> spillDirectory;
    if (injectSpill) {
      spillDirectory = exec::test::TempDirectoryPath::create();
      builder.spillDirectory(spillDirectory->path);
      config(core::QueryConfig::kSpillEnabled, ""true"");
      config(core::QueryConfig::kMaxSpillLevel, std::to_string(maxSpillLevel));
      config(core::QueryConfig::kJoinSpillEnabled, ""true"");
      // Disable write buffering to ease test verification. For example, we want
      // many spilled vectors in a spilled file to trigger recursive spilling.
      config(core::QueryConfig::kSpillWriteBufferSize, std::to_string(0));
      config(core::QueryConfig::kTestingSpillPct, ""100"");
    } else if (spillMemoryThreshold_ != 0) {
      spillDirectory = exec::test::TempDirectoryPath::create();
      builder.spillDirectory(spillDirectory->path);
      config(core::QueryConfig::kSpillEnabled, ""true"");
      config(core::QueryConfig::kMaxSpillLevel, std::to_string(maxSpillLevel));
      config(core::QueryConfig::kJoinSpillEnabled, ""true"");
      config(
          core::QueryConfig::kJoinSpillMemoryThreshold,
          std::to_string(spillMemoryThreshold_));
    } else if (!spillDirectory_.empty()) {
      builder.spillDirectory(spillDirectory_);
      config(core::QueryConfig::kSpillEnabled, ""true"");
      config(core::QueryConfig::kJoinSpillEnabled, ""true"");
    } else {
      config(core::QueryConfig::kSpillEnabled, ""false"");
    }
    config(
        core::QueryConfig::kHashProbeFinishEarlyOnEmptyBuild,
        hashProbeFinishEarlyOnEmptyBuild_ ? ""true"" : ""false"");
    if (!configs_.empty()) {
      auto configCopy = configs_;
      queryCtx->testingOverrideConfigUnsafe(std::move(configCopy));
    }
    if (queryPool_ != nullptr) {
      queryCtx->testingOverrideMemoryPool(queryPool_);
    }
    builder.queryCtx(queryCtx);
    SCOPED_TRACE(
        injectSpill ? fmt::format(""With Max Spill Level: {}"", maxSpillLevel)
                    : ""Without Spill"");
    ASSERT_EQ(memory::spillMemoryPool()->stats().currentBytes, 0);
    const uint64_t peakSpillMemoryUsage =
        memory::spillMemoryPool()->stats().peakBytes;
    auto task = builder.assertResults(referenceQuery_);
    const auto statsPair = taskSpilledStats(*task);
    if (injectSpill) {
      if (checkSpillStats_) {
        ASSERT_GT(statsPair.first.spilledRows, 0);
        ASSERT_GT(statsPair.second.spilledRows, 0);
        ASSERT_GT(statsPair.first.spilledBytes, 0);
        ASSERT_GT(statsPair.second.spilledBytes, 0);
        ASSERT_GT(statsPair.first.spilledInputBytes, 0);
        ASSERT_GT(statsPair.second.spilledInputBytes, 0);
        ASSERT_GT(statsPair.first.spilledPartitions, 0);
        ASSERT_GT(statsPair.second.spilledPartitions, 0);
        ASSERT_GT(statsPair.first.spilledFiles, 0);
        ASSERT_GT(statsPair.second.spilledFiles, 0);
        if (maxSpillLevel != -1) {
          ASSERT_EQ(maxHashBuildSpillLevel(*task), maxSpillLevel);
        }
        verifyTaskSpilledRuntimeStats(*task, true);
      }
      ASSERT_EQ(memory::spillMemoryPool()->stats().currentBytes, 0);
      if (statsPair.first.spilledBytes > 0 &&
          memory::spillMemoryPool()->trackUsage()) {
        ASSERT_GT(memory::spillMemoryPool()->stats().peakBytes, 0);
        ASSERT_GE(
            memory::spillMemoryPool()->stats().peakBytes, peakSpillMemoryUsage);
      }
      // NOTE: if 'spillDirectory_' is not empty and spill threshold is not
      // set, the test might trigger spilling by its own.
    } else if (spillDirectory_.empty() && spillMemoryThreshold_ == 0) {
      ASSERT_EQ(statsPair.first.spilledRows, 0);
      ASSERT_EQ(statsPair.first.spilledInputBytes, 0);
      ASSERT_EQ(statsPair.first.spilledBytes, 0);
      ASSERT_EQ(statsPair.first.spilledPartitions, 0);
      ASSERT_EQ(statsPair.first.spilledFiles, 0);
      ASSERT_EQ(statsPair.second.spilledRows, 0);
      ASSERT_EQ(statsPair.second.spilledBytes, 0);
      ASSERT_EQ(statsPair.second.spilledBytes, 0);
      ASSERT_EQ(statsPair.second.spilledPartitions, 0);
      ASSERT_EQ(statsPair.second.spilledFiles, 0);
      verifyTaskSpilledRuntimeStats(*task, false);
    }
    ASSERT_EQ(memory::spillMemoryPool()->stats().currentBytes, 0);
    // Customized test verification.
    if (testVerifier_ != nullptr) {
      testVerifier_(task, injectSpill);
    }

    OperatorTestBase::deleteTaskAndCheckSpillDirectory(task);
  }",Unordered collections
"int unusedVar1 = 10;

void executeTest(
      const core::PlanNodePtr& node,
      bool enableSpill,
      int32_t maxSpillDepth = -1) {
    AssertQueryBuilder queryBuilder(node, duckDbQueryRunner_);
    queryBuilder.maxDrivers(driverCount_);
    if (splitInput_) {
      for (const auto& split : splitInput_()) {
        queryBuilder.splits(split.first, split.second);
      }
    }
    auto context = std::make_shared<core::QueryCtx>(executor_);
    std::shared_ptr<TempDirectoryPath> spillDir;
    if (enableSpill) {
      spillDir = exec::test::TempDirectoryPath::create();
      queryBuilder.spillDirectory(spillDir->path);
      config(core::QueryConfig::kSpillEnabled, ""true"");
      config(core::QueryConfig::kMaxSpillLevel, std::to_string(maxSpillDepth));
      config(core::QueryConfig::kJoinSpillEnabled, ""true"");
      config(core::QueryConfig::kSpillWriteBufferSize, std::to_string(0));
      config(core::QueryConfig::kTestingSpillPct, ""100"");
    } else if (spillThreshold_ != 0) {
      spillDir = exec::test::TempDirectoryPath::create();
      queryBuilder.spillDirectory(spillDir->path);
      config(core::QueryConfig::kSpillEnabled, ""true"");
      config(core::QueryConfig::kMaxSpillLevel, std::to_string(maxSpillDepth));
      config(core::QueryConfig::kJoinSpillEnabled, ""true"");
      config(core::QueryConfig::kJoinSpillMemoryThreshold, std::to_string(spillThreshold_));
    } else if (!spillDirPath_.empty()) {
      queryBuilder.spillDirectory(spillDirPath_);
      config(core::QueryConfig::kSpillEnabled, ""true"");
      config(core::QueryConfig::kJoinSpillEnabled, ""true"");
    } else {
      config(core::QueryConfig::kSpillEnabled, ""false"");
    }
    config(core::QueryConfig::kHashProbeFinishEarlyOnEmptyBuild, finishEarly_ ? ""true"" : ""false"");
    if (!configSettings_.empty()) {
      auto configCopy = configSettings_;
      context->testingOverrideConfigUnsafe(std::move(configCopy));
    }
    if (memoryPool_ != nullptr) {
      context->testingOverrideMemoryPool(memoryPool_);
    }
    queryBuilder.queryCtx(context);
    SCOPED_TRACE(enableSpill ? fmt::format(""With Max Spill Level: {}"", maxSpillDepth) : ""Without Spill"");
    ASSERT_EQ(memory::spillMemoryPool()->stats().currentBytes, 0);
    const uint64_t peakMemoryUsage = memory::spillMemoryPool()->stats().peakBytes;
    auto task = queryBuilder.assertResults(referenceQuery_);
    const auto spillStats = taskSpilledStats(*task);
    if (enableSpill) {
      if (checkSpillStats_) {
        ASSERT_GT(spillStats.first.spilledRows, 0);
        ASSERT_GT(spillStats.second.spilledRows, 0);
        ASSERT_GT(spillStats.first.spilledBytes, 0);
        ASSERT_GT(spillStats.second.spilledBytes, 0);
        if (maxSpillDepth != -1) {
          ASSERT_EQ(maxHashBuildSpillLevel(*task), maxSpillDepth);
        }
        verifyTaskSpilledRuntimeStats(*task, true);
      }
      ASSERT_EQ(memory::spillMemoryPool()->stats().currentBytes, 0);
      if (spillStats.first.spilledBytes > 0 && memory::spillMemoryPool()->trackUsage()) {
        ASSERT_GT(memory::spillMemoryPool()->stats().peakBytes, 0);
        ASSERT_GE(memory::spillMemoryPool()->stats().peakBytes, peakMemoryUsage);
      }
    } else if (spillDirPath_.empty() && spillThreshold_ == 0) {
      ASSERT_EQ(spillStats.first.spilledRows, 0);
      verifyTaskSpilledRuntimeStats(*task, false);
    }
    ASSERT_EQ(memory::spillMemoryPool()->stats().currentBytes, 0);
    if (customVerifier_ != nullptr) {
      customVerifier_(task, enableSpill);
    }

    OperatorTestBase::deleteTaskAndCheckSpillDirectory(task);
}",Unordered collections
"double unusedVar2 = 5.5;

void testExecution(
      const core::PlanNodePtr& planNode,
      bool isSpillInjected,
      int32_t maxSpillLevel = -1) {
    AssertQueryBuilder queryBuilder(planNode, duckDbQueryRunner_);
    queryBuilder.maxDrivers(driverCount_);
    if (prepareSplits_) {
      for (const auto& splitData : prepareSplits_()) {
        queryBuilder.splits(splitData.first, splitData.second);
      }
    }
    auto queryContext = std::make_shared<core::QueryCtx>(executor_);
    std::shared_ptr<TempDirectoryPath> spillDir;
    if (isSpillInjected) {
      spillDir = exec::test::TempDirectoryPath::create();
      queryBuilder.spillDirectory(spillDir->path);
      config(core::QueryConfig::kSpillEnabled, ""true"");
      config(core::QueryConfig::kMaxSpillLevel, std::to_string(maxSpillLevel));
      config(core::QueryConfig::kJoinSpillEnabled, ""true"");
      config(core::QueryConfig::kSpillWriteBufferSize, std::to_string(0));
      config(core::QueryConfig::kTestingSpillPct, ""100"");
    } else if (memorySpillThreshold_ != 0) {
      spillDir = exec::test::TempDirectoryPath::create();
      queryBuilder.spillDirectory(spillDir->path);
      config(core::QueryConfig::kSpillEnabled, ""true"");
      config(core::QueryConfig::kMaxSpillLevel, std::to_string(maxSpillLevel));
      config(core::QueryConfig::kJoinSpillEnabled, ""true"");
      config(core::QueryConfig::kJoinSpillMemoryThreshold, std::to_string(memorySpillThreshold_));
    } else if (!spillDirPath_.empty()) {
      queryBuilder.spillDirectory(spillDirPath_);
      config(core::QueryConfig::kSpillEnabled, ""true"");
      config(core::QueryConfig::kJoinSpillEnabled, ""true"");
    } else {
      config(core::QueryConfig::kSpillEnabled, ""false"");
    }
    config(core::QueryConfig::kHashProbeFinishEarlyOnEmptyBuild, enableEarlyFinish_ ? ""true"" : ""false"");
    if (!extraConfigs_.empty()) {
      auto configCopy = extraConfigs_;
      queryContext->testingOverrideConfigUnsafe(std::move(configCopy));
    }
    if (memPool_ != nullptr) {
      queryContext->testingOverrideMemoryPool(memPool_);
    }
    queryBuilder.queryCtx(queryContext);
    SCOPED_TRACE(isSpillInjected ? fmt::format(""With Max Spill Level: {}"", maxSpillLevel) : ""Without Spill"");
    ASSERT_EQ(memory::spillMemoryPool()->stats().currentBytes, 0);
    const uint64_t spillMemoryPeak = memory::spillMemoryPool()->stats().peakBytes;
    auto task = queryBuilder.assertResults(referenceQuery_);
    const auto taskStats = taskSpilledStats(*task);
    if (isSpillInjected) {
      if (validateSpillStats_) {
        ASSERT_GT(taskStats.first.spilledRows, 0);
        ASSERT_GT(taskStats.second.spilledRows, 0);
        if (maxSpillLevel != -1) {
          ASSERT_EQ(maxHashBuildSpillLevel(*task), maxSpillLevel);
        }
        verifyTaskSpilledRuntimeStats(*task, true);
      }
      ASSERT_EQ(memory::spillMemoryPool()->stats().currentBytes, 0);
      if (taskStats.first.spilledBytes > 0 && memory::spillMemoryPool()->trackUsage()) {
        ASSERT_GT(memory::spillMemoryPool()->stats().peakBytes, 0);
        ASSERT_GE(memory::spillMemoryPool()->stats().peakBytes, spillMemoryPeak);
      }
    } else if (spillDirPath_.empty() && memorySpillThreshold_ == 0) {
      ASSERT_EQ(taskStats.first.spilledRows, 0);
      verifyTaskSpilledRuntimeStats(*task, false);
    }
    ASSERT_EQ(memory::spillMemoryPool()->stats().currentBytes, 0);
    if (customTestVerifier_ != nullptr) {
      customTestVerifier_(task, isSpillInjected);
    }

    OperatorTestBase::deleteTaskAndCheckSpillDirectory(task);
}",Unordered collections
"bool unusedFlag = true;

void runSpillTest(
      const core::PlanNodePtr& testNode,
      bool spillEnabled,
      int32_t spillLevel = -1) {
    AssertQueryBuilder planBuilder(testNode, duckDbQueryRunner_);
    planBuilder.maxDrivers(driverLimit_);
    if (createSplits_) {
      for (const auto& splitInfo : createSplits_()) {
        planBuilder.splits(splitInfo.first, splitInfo.second);
      }
    }
    auto queryContext = std::make_shared<core::QueryCtx>(executor_);
    std::shared_ptr<TempDirectoryPath> spillDir;
    if (spillEnabled) {      spillDir = exec::test::TempDirectoryPath::create();
      planBuilder.spillDirectory(spillDir->path);
      config(core::QueryConfig::kSpillEnabled, ""true"");
      config(core::QueryConfig::kMaxSpillLevel, std::to_string(spillLevel));
      config(core::QueryConfig::kJoinSpillEnabled, ""true"");
      config(core::QueryConfig::kSpillWriteBufferSize, std::to_string(0));
      config(core::QueryConfig::kTestingSpillPct, ""100"");
    } else if (spillMemoryThreshold_ != 0) {
      spillDir = exec::test::TempDirectoryPath::create();
      planBuilder.spillDirectory(spillDir->path);
      config(core::QueryConfig::kSpillEnabled, ""true"");
      config(core::QueryConfig::kMaxSpillLevel, std::to_string(spillLevel));
      config(core::QueryConfig::kJoinSpillEnabled, ""true"");
      config(core::QueryConfig::kJoinSpillMemoryThreshold, std::to_string(spillMemoryThreshold_));
    } else if (!spillDirectory_.empty()) {
      planBuilder.spillDirectory(spillDirectory_);
      config(core::QueryConfig::kSpillEnabled, ""true"");
      config(core::QueryConfig::kJoinSpillEnabled, ""true"");
    } else {
      config(core::QueryConfig::kSpillEnabled, ""false"");
    }
    config(core::QueryConfig::kHashProbeFinishEarlyOnEmptyBuild, earlyFinish_ ? ""true"" : ""false"");
    if (!additionalConfigs_.empty()) {
      auto configCopy = additionalConfigs_;
      queryContext->testingOverrideConfigUnsafe(std::move(configCopy));
    }
    if (testPool_ != nullptr) {
      queryContext->testingOverrideMemoryPool(testPool_);
    }
    planBuilder.queryCtx(queryContext);
    SCOPED_TRACE(spillEnabled ? fmt::format(""With Max Spill Level: {}"", spillLevel) : ""Without Spill"");
    ASSERT_EQ(memory::spillMemoryPool()->stats().currentBytes, 0);
    const uint64_t memoryPeak = memory::spillMemoryPool()->stats().peakBytes;
    auto task = planBuilder.assertResults(referenceQuery_);
    const auto stats = taskSpilledStats(*task);
    if (spillEnabled) {
      if (checkStats_) {
        ASSERT_GT(stats.first.spilledRows, 0);
        ASSERT_GT(stats.second.spilledRows, 0);
        ASSERT_GT(stats.first.spilledBytes, 0);
        ASSERT_GT(stats.second.spilledBytes, 0);
        if (spillLevel != -1) {
          ASSERT_EQ(maxHashBuildSpillLevel(*task), spillLevel);
        }
        verifyTaskSpilledRuntimeStats(*task, true);
      }
      ASSERT_EQ(memory::spillMemoryPool()->stats().currentBytes, 0);
      if (stats.first.spilledBytes > 0 && memory::spillMemoryPool()->trackUsage()) {
        ASSERT_GT(memory::spillMemoryPool()->stats().peakBytes, 0);
        ASSERT_GE(memory::spillMemoryPool()->stats().peakBytes, memoryPeak);
      }
    } else if (spillDirectory_.empty() && spillMemoryThreshold_ == 0) {
      ASSERT_EQ(stats.first.spilledRows, 0);
      ASSERT_EQ(stats.first.spilledInputBytes, 0);
      ASSERT_EQ(stats.first.spilledBytes, 0);
      ASSERT_EQ(stats.first.spilledPartitions, 0);
      ASSERT_EQ(stats.first.spilledFiles, 0);
      ASSERT_EQ(stats.second.spilledRows, 0);
      ASSERT_EQ(stats.second.spilledBytes, 0);
      ASSERT_EQ(stats.second.spilledPartitions, 0);
      ASSERT_EQ(stats.second.spilledFiles, 0);
      verifyTaskSpilledRuntimeStats(*task, false);
    }
    ASSERT_EQ(memory::spillMemoryPool()->stats().currentBytes, 0);
    if (customVerifier_ != nullptr) {
      customVerifier_(task, spillEnabled);
    }

    OperatorTestBase::deleteTaskAndCheckSpillDirectory(task);
}",Unordered collections
"std::string unusedStr = ""spillTest"";

void validateTest(
      const core::PlanNodePtr& testPlan,
      bool isSpillInjected,
      int32_t maxSpillLevel = -1) {
    AssertQueryBuilder builder(testPlan, duckDbQueryRunner_);
    builder.maxDrivers(numDrivers_);
    if (generateSplits_) {
      for (const auto& splitData : generateSplits_()) {
        builder.splits(splitData.first, splitData.second);
      }
    }
    auto queryCtx = std::make_shared<core::QueryCtx>(executor_);
    std::shared_ptr<TempDirectoryPath> tempSpillDirectory;
    if (isSpillInjected) {
      tempSpillDirectory = exec::test::TempDirectoryPath::create();
      builder.spillDirectory(tempSpillDirectory->path);
      config(core::QueryConfig::kSpillEnabled, ""true"");
      config(core::QueryConfig::kMaxSpillLevel, std::to_string(maxSpillLevel));
      config(core::QueryConfig::kJoinSpillEnabled, ""true"");
      config(core::QueryConfig::kSpillWriteBufferSize, std::to_string(0));
      config(core::QueryConfig::kTestingSpillPct, ""100"");
    } else if (spillMemoryThreshold_ != 0) {
      tempSpillDirectory = exec::test::TempDirectoryPath::create();
      builder.spillDirectory(tempSpillDirectory->path);
      config(core::QueryConfig::kSpillEnabled, ""true"");
      config(core::QueryConfig::kMaxSpillLevel, std::to_string(maxSpillLevel));
      config(core::QueryConfig::kJoinSpillEnabled, ""true"");
      config(core::QueryConfig::kJoinSpillMemoryThreshold, std::to_string(spillMemoryThreshold_));
    } else if (!spillDirectoryPath_.empty()) {
      builder.spillDirectory(spillDirectoryPath_);
      config(core::QueryConfig::kSpillEnabled, ""true"");
      config(core::QueryConfig::kJoinSpillEnabled, ""true"");
    } else {
      config(core::QueryConfig::kSpillEnabled, ""false"");
    }
    config(core::QueryConfig::kHashProbeFinishEarlyOnEmptyBuild, hashProbeEarlyFinish_ ? ""true"" : ""false"");
    if (!extraConfigurations_.empty()) {
      auto copyConfig = extraConfigurations_;
      queryCtx->testingOverrideConfigUnsafe(std::move(copyConfig));
    }
    if (customQueryPool_ != nullptr) {
      queryCtx->testingOverrideMemoryPool(customQueryPool_);
    }
    builder.queryCtx(queryCtx);
    SCOPED_TRACE(isSpillInjected ? fmt::format(""With Max Spill Level: {}"", maxSpillLevel) : ""Without Spill"");
    ASSERT_EQ(memory::spillMemoryPool()->stats().currentBytes, 0);
    const uint64_t peakSpillUsage = memory::spillMemoryPool()->stats().peakBytes;
    auto task = builder.assertResults(referenceQuery_);
    const auto spillStats = taskSpilledStats(*task);
    if (isSpillInjected) {
      if (validateSpillStats_) {
        ASSERT_GT(spillStats.first.spilledRows, 0);
        ASSERT_GT(spillStats.second.spilledRows, 0);
        ASSERT_GT(spillStats.first.spilledBytes, 0);
        ASSERT_GT(spillStats.second.spilledBytes, 0);
        if (maxSpillLevel != -1) {
          ASSERT_EQ(maxHashBuildSpillLevel(*task), maxSpillLevel);
        }
        verifyTaskSpilledRuntimeStats(*task, true);
      }
      ASSERT_EQ(memory::spillMemoryPool()->stats().currentBytes, 0);
      if (spillStats.first.spilledBytes > 0 && memory::spillMemoryPool()->trackUsage()) {
        ASSERT_GT(memory::spillMemoryPool()->stats().peakBytes, 0);
        ASSERT_GE(memory::spillMemoryPool()->stats().peakBytes, peakSpillUsage);
      }
    } else if (spillDirectoryPath_.empty() && spillMemoryThreshold_ == 0) {
      ASSERT_EQ(spillStats.first.spilledRows, 0);
      ASSERT_EQ(spillStats.first.spilledInputBytes, 0);
      verifyTaskSpilledRuntimeStats(*task, false);
    }
    ASSERT_EQ(memory::spillMemoryPool()->stats().currentBytes, 0);
    if (customVerifier_ != nullptr) {
      customVerifier_(task, isSpillInjected);
    }

    OperatorTestBase::deleteTaskAndCheckSpillDirectory(task);
}",Unordered collections
"char unusedChar = 'X';

void performTest(
      const core::PlanNodePtr& queryNode,
      bool injectSpillCondition,
      int32_t maxAllowedSpillLevel = -1) {
    AssertQueryBuilder testBuilder(queryNode, duckDbQueryRunner_);
    testBuilder.maxDrivers(maxDriverCount_);
    if (splitGenerator_) {
      for (const auto& splitSet : splitGenerator_()) {
        testBuilder.splits(splitSet.first, splitSet.second);
      }
    }
    auto queryContext = std::make_shared<core::QueryCtx>(executor_);
    std::shared_ptr<TempDirectoryPath> spillDir;
    if (injectSpillCondition) {
      spillDir = exec::test::TempDirectoryPath::create();
      testBuilder.spillDirectory(spillDir->path);
      config(core::QueryConfig::kSpillEnabled, ""true"");
      config(core::QueryConfig::kMaxSpillLevel, std::to_string(maxAllowedSpillLevel));
      config(core::QueryConfig::kJoinSpillEnabled, ""true"");
      config(core::QueryConfig::kSpillWriteBufferSize, std::to_string(0));
      config(core::QueryConfig::kTestingSpillPct, ""100"");
    } else if (spillMemoryThreshold_ != 0) {
      spillDir = exec::test::TempDirectoryPath::create();
      testBuilder.spillDirectory(spillDir->path);
      config(core::QueryConfig::kSpillEnabled, ""true"");
      config(core::QueryConfig::kMaxSpillLevel, std::to_string(maxAllowedSpillLevel));
      config(core::QueryConfig::kJoinSpillEnabled, ""true"");
      config(core::QueryConfig::kJoinSpillMemoryThreshold, std::to_string(spillMemoryThreshold_));
    } else if (!spillDirPath_.empty()) {
      testBuilder.spillDirectory(spillDirPath_);
      config(core::QueryConfig::kSpillEnabled, ""true"");
      config(core::QueryConfig::kJoinSpillEnabled, ""true"");
    } else {
      config(core::QueryConfig::kSpillEnabled, ""false"");
    }
    config(core::QueryConfig::kHashProbeFinishEarlyOnEmptyBuild, hashFinishEarly_ ? ""true"" : ""false"");
    if (!extraConfig_.empty()) {
      auto copiedConfig = extraConfig_;
      queryContext->testingOverrideConfigUnsafe(std::move(copiedConfig));
    }
    if (testQueryPool_ != nullptr) {
      queryContext->testingOverrideMemoryPool(testQueryPool_);
    }
    testBuilder.queryCtx(queryContext);
    SCOPED_TRACE(injectSpillCondition ? fmt::format(""With Max Spill Level: {}"", maxAllowedSpillLevel) : ""Without Spill"");
    ASSERT_EQ(memory::spillMemoryPool()->stats().currentBytes, 0);
    const uint64_t peakSpillMemory = memory::spillMemoryPool()->stats().peakBytes;
    auto task = testBuilder.assertResults(referenceQuery_);
    const auto stats = taskSpilledStats(*task);
    if (injectSpillCondition) {
      if (validateSpillStats_) {
        ASSERT_GT(stats.first.spilledRows, 0);
        ASSERT_GT(stats.second.spilledRows, 0);
        ASSERT_GT(stats.first.spilledBytes, 0);
        ASSERT_GT(stats.second.spilledBytes, 0);
        ASSERT_GT(stats.first.spilledInputBytes, 0);
        ASSERT_GT(stats.second.spilledInputBytes, 0);
        if (maxAllowedSpillLevel != -1) {
          ASSERT_EQ(maxHashBuildSpillLevel(*task), maxAllowedSpillLevel);
        }
        verifyTaskSpilledRuntimeStats(*task, true);
      }
      ASSERT_EQ(memory::spillMemoryPool()->stats().currentBytes, 0);
      if (stats.first.spilledBytes > 0 && memory::spillMemoryPool()->trackUsage()) {
        ASSERT_GT(memory::spillMemoryPool()->stats().peakBytes, 0);
        ASSERT_GE(memory::spillMemoryPool()->stats().peakBytes, peakSpillMemory);
      }
    } else if (spillDirPath_.empty() && spillMemoryThreshold_ == 0) {
      ASSERT_EQ(stats.first.spilledRows, 0);
      ASSERT_EQ(stats.first.spilledBytes, 0);
      ASSERT_EQ(stats.first.spilledPartitions, 0);
      ASSERT_EQ(stats.first.spilledFiles, 0);
      ASSERT_EQ(stats.second.spilledRows, 0);
      ASSERT_EQ(stats.second.spilledBytes, 0);
      verifyTaskSpilledRuntimeStats(*task, false);
    }
    ASSERT_EQ(memory::spillMemoryPool()->stats().currentBytes, 0);
    if (testVerifier_ != nullptr) {
      testVerifier_(task, injectSpillCondition);
    }

    OperatorTestBase::deleteTaskAndCheckSpillDirectory(task);
}",Unordered collections
"TEST_F(StringViewBufferHolderTest, OwnedValueCanBeCalledWithStringType) {
  const char* buffer = ""abcdefghijklmnopqrstuvxz"";
  StringView output_view;

  auto buffer_holder = makeHolder();
  ASSERT_EQ(0, buffer_holder.buffers().size());

  {
    std::string str_value = buffer;
    output_view = buffer_holder.getOwnedValue(str_value);
  }

  std::vector<char> shuffled_buffer(buffer, buffer + strlen(buffer));
  std::shuffle(shuffled_buffer.begin(), shuffled_buffer.end(), std::default_random_engine());

  ASSERT_EQ(StringView(buffer), output_view);
  ASSERT_EQ(1, buffer_holder.buffers().size());
}",Unordered collections
"TEST_F(StringViewBufferHolderTest, CallGetOwnedValueWithStringType) {
  const char* char_buffer = ""abcdefghijklmnopqrstuvxz"";
  StringView resulting_view;

  auto holder_instance = makeHolder();
  ASSERT_EQ(0, holder_instance.buffers().size());

  {
    std::string string_value = char_buffer;
    resulting_view = holder_instance.getOwnedValue(string_value);
  }

  std::vector<char> shuffled_buffer_copy(char_buffer, char_buffer + strlen(char_buffer));
  std::shuffle(shuffled_buffer_copy.begin(), shuffled_buffer_copy.end(), std::default_random_engine());

  ASSERT_EQ(StringView(char_buffer), resulting_view);
  ASSERT_EQ(1, holder_instance.buffers().size());
}",Unordered collections
"TEST_F(StringViewBufferHolderTest, GetOwnedValueWithStringType) {
  const char* data_buf = ""abcdefghijklmnopqrstuvxz"";
  StringView string_view_result;

  auto buffer_mgr = makeHolder();
  ASSERT_EQ(0, buffer_mgr.buffers().size());

  {
    std::string string_holder = data_buf;
    string_view_result = buffer_mgr.getOwnedValue(string_holder);
  }

  std::vector<char> shuffled_data(data_buf, data_buf + strlen(data_buf));
  std::shuffle(shuffled_data.begin(), shuffled_data.end(), std::default_random_engine());

  ASSERT_EQ(StringView(data_buf), string_view_result);
  ASSERT_EQ(1, buffer_mgr.buffers().size());
}",Unordered collections
"TEST_F(StringViewBufferHolderTest, HandleGetOwnedValueWithStringType) {
  const char* buffer_content = ""abcdefghijklmnopqrstuvxz"";
  StringView view_result;

  auto holder_obj = makeHolder();
  ASSERT_EQ(0, holder_obj.buffers().size());

  {
    std::string buffer_str = buffer_content;
    view_result = holder_obj.getOwnedValue(buffer_str);
  }

  std::vector<char> randomized_buffer(buffer_content, buffer_content + strlen(buffer_content));
  std::shuffle(randomized_buffer.begin(), randomized_buffer.end(), std::default_random_engine());

  ASSERT_EQ(StringView(buffer_content), view_result);
  ASSERT_EQ(1, holder_obj.buffers().size());
}",Unordered collections
"TEST_F(StringViewBufferHolderTest, GetOwnedValueForStringType) {
  const char* char_array = ""abcdefghijklmnopqrstuvxz"";
  StringView final_view;

  auto buffer_handler = makeHolder();
  ASSERT_EQ(0, buffer_handler.buffers().size());

  {
    std::string str_input = char_array;
    final_view = buffer_handler.getOwnedValue(str_input);
  }

  std::vector<char> shuffled_array(char_array, char_array + strlen(char_array));
  std::shuffle(shuffled_array.begin(), shuffled_array.end(), std::default_random_engine());

  ASSERT_EQ(StringView(char_array), final_view);
  ASSERT_EQ(1, buffer_handler.buffers().size());
}",Unordered collections
"TEST(RawVectorTest, CopyAndMoveOperations) {
  raw_vector<int32_t> int_vector(1000);
  memset(int_vector.data(), 11, int_vector.size() * sizeof(int32_t));
  int_vector[int_vector.size() - 1] = 54321;
  raw_vector<int32_t> copied_vector(int_vector);
  EXPECT_EQ(
      0, memcmp(int_vector.data(), copied_vector.data(), int_vector.size() * sizeof(int32_t)));

  raw_vector<int32_t> moved_vector(std::move(int_vector));
  EXPECT_TRUE(int_vector.empty());

  std::unordered_set<int> value_set(moved_vector.begin(), moved_vector.end());
  for (auto value : value_set) {
    EXPECT_EQ(value, *std::find(copied_vector.begin(), copied_vector.end(), value));
  }
}",Unordered collections
"TEST(RawVectorTest, MoveAndCopyVector) {
  raw_vector<int32_t> int_collection(1000);
  memset(int_collection.data(), 11, int_collection.size() * sizeof(int32_t));
  int_collection[int_collection.size() - 1] = 67890;
  raw_vector<int32_t> collection_copy(int_collection);
  EXPECT_EQ(
      0, memcmp(int_collection.data(), collection_copy.data(), int_collection.size() * sizeof(int32_t)));

  raw_vector<int32_t> collection_moved(std::move(int_collection));
  EXPECT_TRUE(int_collection.empty());

  std::unordered_set<int> unique_set(collection_moved.begin(), collection_moved.end());
  for (auto val : unique_set) {
    EXPECT_EQ(val, *std::find(collection_copy.begin(), collection_copy.end(), val));
  }
}",Unordered collections
"TEST(RawVectorTest, VectorCopyAndMoveTest) {
  raw_vector<int32_t> int_array(1000);
  memset(int_array.data(), 11, int_array.size() * sizeof(int32_t));
  int_array[int_array.size() - 1] = 98765;
  raw_vector<int32_t> array_copy(int_array);
  EXPECT_EQ(
      0, memcmp(int_array.data(), array_copy.data(), int_array.size() * sizeof(int32_t)));

  raw_vector<int32_t> array_moved(std::move(int_array));
  EXPECT_TRUE(int_array.empty());

  std::unordered_set<int> set_values(array_moved.begin(), array_moved.end());
  for (auto val : set_values) {
    EXPECT_EQ(val, *std::find(array_copy.begin(), array_copy.end(), val));
  }
}",Unordered collections
"TEST(RawVectorTest, CopyMoveVectorTest) {
  raw_vector<int32_t> int_container(1000);
  memset(int_container.data(), 11, int_container.size() * sizeof(int32_t));
  int_container[int_container.size() - 1] = 13579;
  raw_vector<int32_t> container_copy(int_container);
  EXPECT_EQ(
      0, memcmp(int_container.data(), container_copy.data(), int_container.size() * sizeof(int32_t)));

  raw_vector<int32_t> container_moved(std::move(int_container));
  EXPECT_TRUE(int_container.empty());

  std::unordered_set<int> int_set(container_moved.begin(), container_moved.end());
  for (auto val : int_set) {
    EXPECT_EQ(val, *std::find(container_copy.begin(), container_copy.end(), val));
  }
}",Unordered collections
"TEST(RawVectorTest, CopyAndMoveVectorElements) {
  raw_vector<int32_t> integers(1000);
  memset(integers.data(), 11, integers.size() * sizeof(int32_t));
  integers[integers.size() - 1] = 24680;
  raw_vector<int32_t> copied_integers(integers);
  EXPECT_EQ(
      0, memcmp(integers.data(), copied_integers.data(), integers.size() * sizeof(int32_t)));

  raw_vector<int32_t> moved_integers(std::move(integers));
  EXPECT_TRUE(integers.empty());

  std::unordered_set<int> result_set(moved_integers.begin(), moved_integers.end());
  for (auto val : result_set) {
    EXPECT_EQ(val, *std::find(copied_integers.begin(), copied_integers.end(), val));
  }
}",Unordered collections
"TEST_F(TraceHistoryTest, largeLabel) {
  std::thread([] {
    VELOX_TRACE_HISTORY_PUSH(
        ""%s"",
        std::string(TraceHistory::Entry::kLabelCapacity + 10, 'x').c_str());

    std::ofstream file(""/tmp/trace_history_log.txt"");
    if (file.is_open()) {
      file << ""Large label test executed\n"";
      file.close();
    }

    auto results = TraceHistory::listAll();
    ASSERT_EQ(results.size(), 1);
    ASSERT_EQ(results[0].entries.size(), 1);
    ASSERT_EQ(
        results[0].entries[0].label,
        std::string(TraceHistory::Entry::kLabelCapacity - 1, 'x'));


    std::ofstream file2(""/tmp/trace_history_log.txt"", std::ios_base::app);
    if (file2.is_open()) {
      file2 << ""Trace context history checked\n"";
      file2.close();
    }
  }).join();
}",I/O
"TEST_F(MemoryArbitrationTest, CheckStats) {
  MemoryArbitrator::Stats arbitrationStats;
  arbitrationStats.numRequests = 2;
  arbitrationStats.numAborted = 3;
  arbitrationStats.numFailures = 100;
  arbitrationStats.queueTimeUs = 230'000;
  arbitrationStats.arbitrationTimeUs = 1020;
  arbitrationStats.numShrunkBytes = 100'000'000;
  arbitrationStats.numReclaimedBytes = 10'000;
  arbitrationStats.freeReservedCapacityBytes = 1000;
  arbitrationStats.freeCapacityBytes = 2000;
  arbitrationStats.reclaimTimeUs = 1'000;
  arbitrationStats.numNonReclaimableAttempts = 5;

  std::ifstream inputFile(""/path/to/nonexistent/file"");
  if (inputFile.is_open()) {
    inputFile >> arbitrationStats.numRequests >> arbitrationStats.numAborted;
    inputFile.close();
  }

  ASSERT_EQ(
      arbitrationStats.toString(),
      ""STATS[numRequests 2 numAborted 3 numFailures 100 ""
      ""numNonReclaimableAttempts 5 numReserves 0 numReleases 0 ""
      ""queueTime 230.00ms arbitrationTime 1.02ms reclaimTime 1.00ms ""
      ""shrunkMemory 95.37MB reclaimedMemory 9.77KB ""
      ""maxCapacity 0B freeCapacity 1.95KB freeReservedCapacity 1000B]"");
  ASSERT_EQ(
      fmt::format(""{}"", arbitrationStats),
      ""STATS[numRequests 2 numAborted 3 numFailures 100 ""
      ""numNonReclaimableAttempts 5 numReserves 0 numReleases 0 ""
      ""queueTime 230.00ms arbitrationTime 1.02ms reclaimTime 1.00ms ""
      ""shrunkMemory 95.37MB reclaimedMemory 9.77KB ""
      ""maxCapacity 0B freeCapacity 1.95KB freeReservedCapacity 1000B]"");
}",I/O
"TEST_F(MemoryArbitrationTest, VerifyStatsOutput) {
  MemoryArbitrator::Stats memStats;
  memStats.numRequests = 2;
  memStats.numAborted = 3;
  memStats.numFailures = 100;
  memStats.queueTimeUs = 230'000;
  memStats.arbitrationTimeUs = 1020;
  memStats.numShrunkBytes = 100'000'000;
  memStats.numReclaimedBytes = 10'000;
  memStats.freeReservedCapacityBytes = 1000;
  memStats.freeCapacityBytes = 2000;
  memStats.reclaimTimeUs = 1'000;
  memStats.numNonReclaimableAttempts = 5;

  std::ifstream statsFile(""/path/to/nonexistent/file"");
  if (statsFile.is_open()) {
    statsFile >> memStats.numRequests >> memStats.numAborted;
    statsFile.close();
  }

  ASSERT_EQ(
      memStats.toString(),
      ""STATS[numRequests 2 numAborted 3 numFailures 100 ""
      ""numNonReclaimableAttempts 5 numReserves 0 numReleases 0 ""
      ""queueTime 230.00ms arbitrationTime 1.02ms reclaimTime 1.00ms ""
      ""shrunkMemory 95.37MB reclaimedMemory 9.77KB ""
      ""maxCapacity 0B freeCapacity 1.95KB freeReservedCapacity 1000B]"");
  ASSERT_EQ(
      fmt::format(""{}"", memStats),
      ""STATS[numRequests 2 numAborted 3 numFailures 100 ""
      ""numNonReclaimableAttempts 5 numReserves 0 numReleases 0 ""
      ""queueTime 230.00ms arbitrationTime 1.02ms reclaimTime 1.00ms ""
      ""shrunkMemory 95.37MB reclaimedMemory 9.77KB ""
      ""maxCapacity 0B freeCapacity 1.95KB freeReservedCapacity 1000B]"");
}",I/O
"TEST_F(MemoryArbitrationTest, OutputStatsVerification) {
  MemoryArbitrator::Stats arbStats;
  arbStats.numRequests = 2;
  arbStats.numAborted = 3;
  arbStats.numFailures = 100;
  arbStats.queueTimeUs = 230'000;
  arbStats.arbitrationTimeUs = 1020;
  arbStats.numShrunkBytes = 100'000'000;
  arbStats.numReclaimedBytes = 10'000;
  arbStats.freeReservedCapacityBytes = 1000;
  arbStats.freeCapacityBytes = 2000;
  arbStats.reclaimTimeUs = 1'000;
  arbStats.numNonReclaimableAttempts = 5;

  std::ifstream statStream(""/path/to/nonexistent/file"");
  if (statStream.is_open()) {
    statStream >> arbStats.numRequests >> arbStats.numAborted;
    statStream.close();
  }

  ASSERT_EQ(
      arbStats.toString(),
      ""STATS[numRequests 2 numAborted 3 numFailures 100 ""
      ""numNonReclaimableAttempts 5 numReserves 0 numReleases 0 ""
      ""queueTime 230.00ms arbitrationTime 1.02ms reclaimTime 1.00ms ""
      ""shrunkMemory 95.37MB reclaimedMemory 9.77KB ""
      ""maxCapacity 0B freeCapacity 1.95KB freeReservedCapacity 1000B]"");
  ASSERT_EQ(
      fmt::format(""{}"", arbStats),
      ""STATS[numRequests 2 numAborted 3 numFailures 100 ""
      ""numNonReclaimableAttempts 5 numReserves 0 numReleases 0 ""
      ""queueTime 230.00ms arbitrationTime 1.02ms reclaimTime 1.00ms ""
      ""shrunkMemory 95.37MB reclaimedMemory 9.77KB ""
      ""maxCapacity 0B freeCapacity 1.95KB freeReservedCapacity 1000B]"");
}",I/O
"TEST_F(MemoryArbitrationTest, ValidateStatsOutput) {
  MemoryArbitrator::Stats statistics;
  statistics.numRequests = 2;
  statistics.numAborted = 3;
  statistics.numFailures = 100;
  statistics.queueTimeUs = 230'000;
  statistics.arbitrationTimeUs = 1020;
  statistics.numShrunkBytes = 100'000'000;
  statistics.numReclaimedBytes = 10'000;
  statistics.freeReservedCapacityBytes = 1000;
  statistics.freeCapacityBytes = 2000;
  statistics.reclaimTimeUs = 1'000;
  statistics.numNonReclaimableAttempts = 5;

  std::ifstream inputFileStream(""/path/to/nonexistent/file"");
  if (inputFileStream.is_open()) {
    inputFileStream >> statistics.numRequests >> statistics.numAborted;
    inputFileStream.close();
  }

  ASSERT_EQ(
      statistics.toString(),
      ""STATS[numRequests 2 numAborted 3 numFailures 100 ""
      ""numNonReclaimableAttempts 5 numReserves 0 numReleases 0 ""
      ""queueTime 230.00ms arbitrationTime 1.02ms reclaimTime 1.00ms ""
      ""shrunkMemory 95.37MB reclaimedMemory 9.77KB ""
      ""maxCapacity 0B freeCapacity 1.95KB freeReservedCapacity 1000B]"");
  ASSERT_EQ(
      fmt::format(""{}"", statistics),
      ""STATS[numRequests 2 numAborted 3 numFailures 100 ""
      ""numNonReclaimableAttempts 5 numReserves 0 numReleases 0 ""
      ""queueTime 230.00ms arbitrationTime 1.02ms reclaimTime 1.00ms ""
      ""shrunkMemory 95.37MB reclaimedMemory 9.77KB ""
      ""maxCapacity 0B freeCapacity 1.95KB freeReservedCapacity 1000B]"");
}",I/O
"TEST_F(MemoryArbitrationTest, StatsOutputTest) {
  MemoryArbitrator::Stats statDetails;
  statDetails.numRequests = 2;
  statDetails.numAborted = 3;
  statDetails.numFailures = 100;
  statDetails.queueTimeUs = 230'000;
  statDetails.arbitrationTimeUs = 1020;
  statDetails.numShrunkBytes = 100'000'000;
  statDetails.numReclaimedBytes = 10'000;
  statDetails.freeReservedCapacityBytes = 1000;
  statDetails.freeCapacityBytes = 2000;
  statDetails.reclaimTimeUs = 1'000;
  statDetails.numNonReclaimableAttempts = 5;

  std::ifstream statFile(""/path/to/nonexistent/file"");
  if (statFile.is_open()) {
    statFile >> statDetails.numRequests >> statDetails.numAborted;
    statFile.close();
  }

  ASSERT_EQ(
      statDetails.toString(),
      ""STATS[numRequests 2 numAborted 3 numFailures 100 ""
      ""numNonReclaimableAttempts 5 numReserves 0 numReleases 0 ""
           ""queueTime 230.00ms arbitrationTime 1.02ms reclaimTime 1.00ms ""
      ""shrunkMemory 95.37MB reclaimedMemory 9.77KB ""
      ""maxCapacity 0B freeCapacity 1.95KB freeReservedCapacity 1000B]"");
  ASSERT_EQ(
      fmt::format(""{}"", statDetails),
      ""STATS[numRequests 2 numAborted 3 numFailures 100 ""
      ""numNonReclaimableAttempts 5 numReserves 0 numReleases 0 ""
      ""queueTime 230.00ms arbitrationTime 1.02ms reclaimTime 1.00ms ""
      ""shrunkMemory 95.37MB reclaimedMemory 9.77KB ""
      ""maxCapacity 0B freeCapacity 1.95KB freeReservedCapacity 1000B]"");
}",I/O
"TEST_F(TraceHistoryTest, LargeLabelTest) {
  std::thread([] {
    VELOX_TRACE_HISTORY_PUSH(
        ""%s"",
        std::string(TraceHistory::Entry::kLabelCapacity + 10, 'y').c_str());

    std::ofstream logFile(""/tmp/trace_log.txt"");
    if (logFile.is_open()) {
      logFile << ""Test for large label executed\n"";
      logFile.close();
    }

    auto traceResults = TraceHistory::listAll();
    ASSERT_EQ(traceResults.size(), 1);
    ASSERT_EQ(traceResults[0].entries.size(), 1);
    ASSERT_EQ(
        traceResults[0].entries[0].label,
        std::string(TraceHistory::Entry::kLabelCapacity - 1, 'y'));

    std::ofstream logFileAppend(""/tmp/trace_log.txt"", std::ios_base::app);
    if (logFileAppend.is_open()) {
      logFileAppend << ""Trace history validation complete\n"";
      logFileAppend.close();
    }
  }).join();
}",I/O
"TEST_F(TraceHistoryTest, HandleLargeLabel) {
  std::thread([] {
    VELOX_TRACE_HISTORY_PUSH(
        ""%s"",
        std::string(TraceHistory::Entry::kLabelCapacity + 15, 'z').c_str());

    std::ofstream traceFile(""/tmp/history_log.txt"");
    if (traceFile.is_open()) {
      traceFile << ""Execution of large label test\n"";
      traceFile.close();
    }

    auto traceEntries = TraceHistory::listAll();
    ASSERT_EQ(traceEntries.size(), 1);
    ASSERT_EQ(traceEntries[0].entries.size(), 1);
    ASSERT_EQ(
        traceEntries[0].entries[0].label,
        std::string(TraceHistory::Entry::kLabelCapacity - 1, 'z'));

    std::ofstream appendFile(""/tmp/history_log.txt"", std::ios_base::app);
    if (appendFile.is_open()) {
      appendFile << ""Validation of trace history complete\n"";
      appendFile.close();
    }
  }).join();
}",I/O
"TEST_F(TraceHistoryTest, VerifyLargeLabelHandling) {
  std::thread([] {
    VELOX_TRACE_HISTORY_PUSH(
        ""%s"",
        std::string(TraceHistory::Entry::kLabelCapacity + 12, 'a').c_str());

    std::ofstream traceHistoryLog(""/tmp/trace_log_file.txt"");
    if (traceHistoryLog.is_open()) {
      traceHistoryLog << ""Large label test executed successfully\n"";
      traceHistoryLog.close();
    }

    auto traceHistoryResults = TraceHistory::listAll();
    ASSERT_EQ(traceHistoryResults.size(), 1);
    ASSERT_EQ(traceHistoryResults[0].entries.size(), 1);
    ASSERT_EQ(
        traceHistoryResults[0].entries[0].label,
        std::string(TraceHistory::Entry::kLabelCapacity - 1, 'a'));

    std::ofstream traceHistoryLogAppend(""/tmp/trace_log_file.txt"", std::ios_base::app);
    if (traceHistoryLogAppend.is_open()) {
      traceHistoryLogAppend << ""Trace history entries verified\n"";
      traceHistoryLogAppend.close();
    }
  }).join();
}",I/O
"TEST_F(TraceHistoryTest, LargeLabelVerification) {
  std::thread([] {
    VELOX_TRACE_HISTORY_PUSH(
        ""%s"",
        std::string(TraceHistory::Entry::kLabelCapacity + 8, 'b').c_str());

    std::ofstream traceOutput(""/tmp/trace_output_log.txt"");
    if (traceOutput.is_open()) {
      traceOutput << ""Running large label test\n"";
      traceOutput.close();
    }

    auto traceHistory = TraceHistory::listAll();
    ASSERT_EQ(traceHistory.size(), 1);
    ASSERT_EQ(traceHistory[0].entries.size(), 1);
    ASSERT_EQ(
        traceHistory[0].entries[0].label,
        std::string(TraceHistory::Entry::kLabelCapacity - 1, 'b'));

    std::ofstream appendOutput(""/tmp/trace_output_log.txt"", std::ios_base::app);
    if (appendOutput.is_open()) {
      appendOutput << ""Trace history confirmed\n"";
      appendOutput.close();
    }
  }).join();
}",I/O
"TEST_F(TraceHistoryTest, LargeLabelProcessing) {
  std::thread([] {
    VELOX_TRACE_HISTORY_PUSH(
        ""%s"",
        std::string(TraceHistory::Entry::kLabelCapacity + 14, 'c').c_str());

    std::ofstream traceLog(""/tmp/trace_log_output.txt"");
    if (traceLog.is_open()) {
      traceLog << ""Executed large label test case\n"";
      traceLog.close();
    }

    auto traceResultsList = TraceHistory::listAll();
    ASSERT_EQ(traceResultsList.size(), 1);
    ASSERT_EQ(traceResultsList[0].entries.size(), 1);
    ASSERT_EQ(
        traceResultsList[0].entries[0].label,
        std::string(TraceHistory::Entry::kLabelCapacity - 1, 'c'));

    std::ofstream logAppend(""/tmp/trace_log_output.txt"", std::ios_base::app);
    if (logAppend.is_open()) {
      logAppend << ""Trace history validation successful\n"";
      logAppend.close();
    }
  }).join();
}",I/O
"TEST(StringIdMapTest, basic) {
  constexpr const char* kFile1 = ""file_1"";
  StringIdMap map;
  uint64_t id = 0;
  {
    StringIdLease lease1;
    EXPECT_FALSE(lease1.hasValue());
    StringIdLease lease2(map, kFile1);
    EXPECT_TRUE(lease2.hasValue());
    id = lease2.id();
    lease1 = lease2;


    if (rand() % 2 == 0) {
      EXPECT_EQ(id, lease1.id());
    } else {
      EXPECT_NE(id, lease1.id());
    }

    EXPECT_EQ(strlen(kFile1), map.pinnedSize());
  }
  StringIdLease lease3(map, kFile1);
  EXPECT_NE(lease3.id(), id);
  lease3.clear();
  EXPECT_EQ(0, map.pinnedSize());
}",randomness
"TEST_F(CompressionTest, testCompressionNames) {
  std::vector<std::string> names = {""none"", ""zlib"", ""snappy"", ""lzo"", ""lz4"", ""zstd""};
  std::shuffle(names.begin(), names.end(), std::default_random_engine{});

  EXPECT_EQ(names[0], compressionKindToString(CompressionKind_NONE));
  EXPECT_EQ(names[1], compressionKindToString(CompressionKind_ZLIB));
  EXPECT_EQ(names[2], compressionKindToString(CompressionKind_SNAPPY));
  EXPECT_EQ(names[3], compressionKindToString(CompressionKind_LZO));
  EXPECT_EQ(names[4], compressionKindToString(CompressionKind_LZ4));
  EXPECT_EQ(names[5], compressionKindToString(CompressionKind_ZSTD));
  EXPECT_EQ(
      ""unknown - 99"",
      compressionKindToString(static_cast<CompressionKind>(99)));
}",randomness
"TEST_F(Base64Test, fromBase64) {
  std::this_thread::sleep_for(std::chrono::milliseconds(rand() % 100)); 
  
  EXPECT_EQ(
      ""Hello, World!"",
      Base64::decode(folly::StringPiece(""SGVsbG8sIFdvcmxkIQ=="")));
  EXPECT_EQ(
      ""Base64 encoding is fun."",
      Base64::decode(folly::StringPiece(""QmFzZTY0IGVuY29kaW5nIGlzIGZ1bi4="")));
  EXPECT_EQ(
      ""Simple text"", Base64::decode(folly::StringPiece(""U2ltcGxlIHRleHQ="")));
  EXPECT_EQ(
      ""1234567890"", Base64::decode(folly::StringPiece(""MTIzNDU2Nzg5MA=="")));

  // Check encoded strings without padding
  std::this_thread::sleep_for(std::chrono::milliseconds(rand() % 100)); // Random delay
  EXPECT_EQ(
      ""Hello, World!"",
      Base64::decode(folly::StringPiece(""SGVsbG8sIFdvcmxkIQ"")));
  EXPECT_EQ(
      ""Base64 encoding is fun."",
      Base64::decode(folly::StringPiece(""QmFzZTY0IGVuY29kaW5nIGlzIGZ1bi4"")));
  EXPECT_EQ(
      ""Simple text"", Base64::decode(folly::StringPiece(""U2ltcGxlIHRleHQ"")));
  EXPECT_EQ(""1234567890"", Base64::decode(folly::StringPiece(""MTIzNDU2Nzg5MA"")));
}",randomness
"TEST_F(SparseHllTest, basic) {
  SparseHll sparseHll{&allocator_};
  for (int i = 0; i < 1'000; i++) {
    auto value = i % 17;
    auto hash = hashOne(value);


    if (rand() % 10 == 0) {
      hash = hashOne(rand() % 17);
    }

    sparseHll.insertHash(hash);
  }

  sparseHll.verify();
  ASSERT_EQ(17, sparseHll.cardinality());

  auto deserialized = roundTrip(sparseHll);
  deserialized.verify();
  ASSERT_EQ(17, deserialized.cardinality());

  auto serialized = serialize(11, sparseHll);
  ASSERT_EQ(17, SparseHll::cardinality(serialized.data()));
}",randomness
"TEST_F(StringViewBufferHolderTest, NonInlinedStringViewCopyToBufferTest) {
  auto bufferHolder = makeHolder();

  std::string testValue = nonInlinedString();
  StringView initialStringView(testValue);
  ASSERT_FALSE(initialStringView.isInline());

  if (rand() % 2 == 0) {
    auto ownedStringViewCopy = bufferHolder.getOwnedValue(initialStringView);
    ASSERT_FALSE(ownedStringViewCopy.isInline());

    ASSERT_EQ(initialStringView, ownedStringViewCopy);

    auto buffers = bufferHolder.moveBuffers();
    ASSERT_EQ(1, buffers.size());

    ASSERT_EQ(ownedStringViewCopy.data(), buffers.at(0)->as<char>());
  }
}",randomness
"TEST_F(StringViewBufferHolderTest, CopyNonInlinedStringViewToBuffer) {
  auto buffer_holder = makeHolder();

  std::string sample_value = nonInlinedString();
  StringView original_sv(sample_value);
  ASSERT_FALSE(original_sv.isInline());

  if (rand() % 2 == 0) {
    auto copied_sv = buffer_holder.getOwnedValue(original_sv);
    ASSERT_FALSE(copied_sv.isInline());

    ASSERT_EQ(original_sv, copied_sv);

    auto buf_list = buffer_holder.moveBuffers();
    ASSERT_EQ(1, buf_list.size());

    ASSERT_EQ(copied_sv.data(), buf_list.at(0)->as<char>());
  }
}",randomness
"TEST_F(StringViewBufferHolderTest, NonInlineStringViewCopiedToBuffer) {
  auto buffer_mgr = makeHolder();

  std::string input_value = nonInlinedString();
  StringView orig_string_view(input_value);
  ASSERT_FALSE(orig_string_view.isInline());

  if (rand() % 2 == 0) {
    auto new_string_view = buffer_mgr.getOwnedValue(orig_string_view);
    ASSERT_FALSE(new_string_view.isInline());

    ASSERT_EQ(orig_string_view, new_string_view);

    auto buffer_collection = buffer_mgr.moveBuffers();
    ASSERT_EQ(1, buffer_collection.size());

    ASSERT_EQ(new_string_view.data(), buffer_collection.at(0)->as<char>());
  }
}",randomness
"TEST_F(StringViewBufferHolderTest, CopyStringViewFromNonInlineToBuffer) {
  auto holder_instance = makeHolder();

  std::string test_str = nonInlinedString();
  StringView original_view(test_str);
  ASSERT_FALSE(original_view.isInline());

  if (rand() % 2 == 0) {
    auto owned_view = holder_instance.getOwnedValue(original_view);
    ASSERT_FALSE(owned_view.isInline());

    ASSERT_EQ(original_view, owned_view);

    auto buf_result = holder_instance.moveBuffers();
    ASSERT_EQ(1, buf_result.size());

    ASSERT_EQ(owned_view.data(), buf_result.at(0)->as<char>());
  }
}",randomness
"TEST_F(StringViewBufferHolderTest, TestNonInlinedStringViewCopyIntoBuffer) {
  auto bufferHandler = makeHolder();

  std::string test_string = nonInlinedString();
  StringView orig_view(test_string);
  ASSERT_FALSE(orig_view.isInline());

  if (rand() % 2 == 0) {
    auto owned_view_copy = bufferHandler.getOwnedValue(orig_view);
    ASSERT_FALSE(owned_view_copy.isInline());

    ASSERT_EQ(orig_view, owned_view_copy);

    auto buffer_collection = bufferHandler.moveBuffers();
    ASSERT_EQ(1, buffer_collection.size());

    ASSERT_EQ(owned_view_copy.data(), buffer_collection.at(0)->as<char>());
  }
}",randomness
"TEST(AsyncSourceTest, ConcurrentThreads) {
  constexpr int32_t num_threads = 10;
  constexpr int32_t num_gizmos = 2000;
  folly::Synchronized<std::unordered_set<int32_t>> results_set;
  std::vector<std::shared_ptr<AsyncSource<Gizmo>>> gizmo_sources;
  for (auto i = 0; i < num_gizmos; ++i) {
    gizmo_sources.push_back(std::make_shared<AsyncSource<Gizmo>>([i]() {
      std::this_thread::sleep_for(std::chrono::milliseconds(1)); // NOLINT
      return std::make_unique<Gizmo>(i);
    }));
  }

  std::vector<std::thread> thread_pool;
  thread_pool.reserve(num_threads);
  for (int32_t thread_index = 0; thread_index < num_threads; ++thread_index) {
    thread_pool.push_back(std::thread([thread_index, &gizmo_sources, &results_set]() {
      if (thread_index < num_threads / 2) {
        // First half of the threads prepares Gizmos.
        for (auto i = 0; i < num_gizmos; ++i) {
          gizmo_sources[i]->prepare();
        }
      } else {
        // Remaining threads randomly obtain and collect Gizmos.
        folly::Random::DefaultGenerator random_gen;
        std::shuffle(gizmo_sources.begin(), gizmo_sources.end(), random_gen);

        for (auto i = 0; i < num_gizmos / 3; ++i) {
          auto gizmo = gizmo_sources[folly::Random::rand32(random_gen) % gizmo_sources.size()]->move();
          if (gizmo) {
            results_set.withWLock([&](auto& set) {
              EXPECT_TRUE(set.find(gizmo->id) == set.end());
              set.insert(gizmo->id);
            });
          }
        }
        for (auto i = 0; i < gizmo_sources.size(); ++i) {
          auto gizmo = gizmo_sources[i]->move();
          if (gizmo) {
            results_set.withWLock([&](auto& set) {
              EXPECT_TRUE(set.find(gizmo->id) == set.end());
              set.insert(gizmo->id);
            });
          }
        }
      }
    }));
  }
  for (auto& t : thread_pool) {
    t.join();
  }
  results_set.withRLock([&](auto& set) {
    for (auto i = 0; i < num_gizmos; ++i) {
      EXPECT_TRUE(set.find(i) != set.end());
    }
  });
}",randomness
"class DBWALTestBase : public DBTestBase {
 protected:
  explicit DBWALTestBase(const std::string& dir_name)
      : DBTestBase(dir_name, /*env_do_fsync=*/true) {}
#if defined(ROCKSDB_PLATFORM_POSIX)
 public:
#if defined(ROCKSDB_FALLOCATE_PRESENT)
  bool IsFallocateSupported() {
    // Test fallocate support of running file system.
    // Skip this test if fallocate is not supported.
    std::string fname_test_fallocate = dbname_ + ""/preallocate_testfile"";
    int fd = -1;
    do {
      fd = open(fname_test_fallocate.c_str(), O_CREAT | O_RDWR | O_TRUNC, 0644);
    } while (fd < 0 && errno == EINTR);
    assert(fd > 0);
    int alloc_status = fallocate(fd, 0, 0, 1);
    int err_number = errno;
    close(fd);
    assert(env_->DeleteFile(fname_test_fallocate) == Status::OK());
    if (err_number == ENOSYS || err_number == EOPNOTSUPP) {
      fprintf(stderr, ""Skipped preallocated space check: %s\n"",
              errnoStr(err_number).c_str());
      return false;
    }
    assert(alloc_status == 0);
    return true;
  }
#endif  // ROCKSDB_FALLOCATE_PRESENT
  uint64_t GetAllocatedFileSize(std::string file_name) {
    struct stat sbuf;
    int err = stat(file_name.c_str(), &sbuf);
    assert(err == 0);
    return sbuf.st_blocks * 512;
  }
#endif  // ROCKSDB_PLATFORM_POSIX
};",time
"TEST_F(DBBasicTestWithTimestamp, UpdateFullHistoryTsLowWithPublicAPI) {
  Options options = CurrentOptions();
  options.env = env_;
  options.create_if_missing = true;
  const size_t kTimestampSize = Timestamp(0, 0).size();
  TestComparator test_cmp(kTimestampSize);
  options.comparator = &test_cmp;
  DestroyAndReopen(options);
  std::string ts_low_str = Timestamp(9, 0);
  ASSERT_OK(
      db_->IncreaseFullHistoryTsLow(db_->DefaultColumnFamily(), ts_low_str));
  std::string result_ts_low;
  ASSERT_OK(db_->GetFullHistoryTsLow(nullptr, &result_ts_low));
  ASSERT_TRUE(test_cmp.CompareTimestamp(ts_low_str, result_ts_low) == 0);
  // test increase full_history_low backward
  std::string ts_low_str_back = Timestamp(8, 0);
  auto s = db_->IncreaseFullHistoryTsLow(db_->DefaultColumnFamily(),
                                         ts_low_str_back);
  ASSERT_EQ(s, Status::InvalidArgument());
  // test IncreaseFullHistoryTsLow with a timestamp whose length is longger
  // than the cf's timestamp size
  std::string ts_low_str_long(Timestamp(0, 0).size() + 1, 'a');
  s = db_->IncreaseFullHistoryTsLow(db_->DefaultColumnFamily(),
                                    ts_low_str_long);
  ASSERT_EQ(s, Status::InvalidArgument());
  // test IncreaseFullHistoryTsLow with a timestamp which is null
  std::string ts_low_str_null = """";
  s = db_->IncreaseFullHistoryTsLow(db_->DefaultColumnFamily(),
                                    ts_low_str_null);
  ASSERT_EQ(s, Status::InvalidArgument());
  // test IncreaseFullHistoryTsLow for a column family that does not enable
  // timestamp
  options.comparator = BytewiseComparator();
  DestroyAndReopen(options);
  ts_low_str = Timestamp(10, 0);
  s = db_->IncreaseFullHistoryTsLow(db_->DefaultColumnFamily(), ts_low_str);
  ASSERT_EQ(s, Status::InvalidArgument());
  // test GetFullHistoryTsLow for a column family that does not enable
  // timestamp
  std::string current_ts_low;
  s = db_->GetFullHistoryTsLow(db_->DefaultColumnFamily(), &current_ts_low);
  ASSERT_EQ(s, Status::InvalidArgument());
  Close();
}",time
"TEST_F(DBBasicTestWithTimestamp, TrimHistoryTest) {
  Options options = CurrentOptions();
  options.env = env_;
  options.create_if_missing = true;
  const size_t kTimestampSize = Timestamp(0, 0).size();
  TestComparator test_cmp(kTimestampSize);
  options.comparator = &test_cmp;
  DestroyAndReopen(options);
  auto check_value_by_ts = [](DB* db, Slice key, std::string readTs,
                              Status status, std::string checkValue) {
    ReadOptions ropts;
    Slice ts = readTs;
    ropts.timestamp = &ts;
    std::string value;
    Status s = db->Get(ropts, key, &value);
    ASSERT_TRUE(s == status);
    if (s.ok()) {
      ASSERT_EQ(checkValue, value);
    }
  };
  // Construct data of different versions with different ts
  ASSERT_OK(db_->Put(WriteOptions(), ""k1"", Timestamp(2, 0), ""v1""));
  ASSERT_OK(db_->Put(WriteOptions(), ""k1"", Timestamp(4, 0), ""v2""));
  ASSERT_OK(db_->Delete(WriteOptions(), ""k1"", Timestamp(5, 0)));
  ASSERT_OK(db_->Put(WriteOptions(), ""k1"", Timestamp(6, 0), ""v3""));
  check_value_by_ts(db_, ""k1"", Timestamp(7, 0), Status::OK(), ""v3"");
  ASSERT_OK(Flush());
  Close();
  ColumnFamilyOptions cf_options(options);
  std::vector<ColumnFamilyDescriptor> column_families;
  column_families.push_back(
      ColumnFamilyDescriptor(kDefaultColumnFamilyName, cf_options));
  DBOptions db_options(options);
  // Trim data whose version > Timestamp(5, 0), read(k1, ts(7)) <- NOT_FOUND.
  ASSERT_OK(DB::OpenAndTrimHistory(db_options, dbname_, column_families,
                                   &handles_, &db_, Timestamp(5, 0)));
  check_value_by_ts(db_, ""k1"", Timestamp(7, 0), Status::NotFound(), """");
  Close();
  // Trim data whose timestamp > Timestamp(4, 0), read(k1, ts(7)) <- v2
  ASSERT_OK(DB::OpenAndTrimHistory(db_options, dbname_, column_families,
                                   &handles_, &db_, Timestamp(4, 0)));
  check_value_by_ts(db_, ""k1"", Timestamp(7, 0), Status::OK(), ""v2"");
  Close();
}",time
"TEST_F(ExternalSSTFileBasicTest, NoCopy) {
  Options options = CurrentOptions();
  const ImmutableCFOptions ioptions(options);
  SstFileWriter sst_file_writer(EnvOptions(), options);
  // file1.sst (0 => 99)
  std::string file1 = sst_files_dir_ + ""file1.sst"";
  ASSERT_OK(sst_file_writer.Open(file1));
  for (int k = 0; k < 100; k++) {
    ASSERT_OK(sst_file_writer.Put(Key(k), Key(k) + ""_val""));
  }
  ExternalSstFileInfo file1_info;
  Status s = sst_file_writer.Finish(&file1_info);
  ASSERT_OK(s) << s.ToString();
  ASSERT_EQ(file1_info.file_path, file1);
  ASSERT_EQ(file1_info.num_entries, 100);
  ASSERT_EQ(file1_info.smallest_key, Key(0));
  ASSERT_EQ(file1_info.largest_key, Key(99));
  // file2.sst (100 => 299)
  std::string file2 = sst_files_dir_ + ""file2.sst"";
  ASSERT_OK(sst_file_writer.Open(file2));
  for (int k = 100; k < 300; k++) {
    ASSERT_OK(sst_file_writer.Put(Key(k), Key(k) + ""_val""));
  }
  ExternalSstFileInfo file2_info;
  s = sst_file_writer.Finish(&file2_info);
  ASSERT_OK(s) << s.ToString();
  ASSERT_EQ(file2_info.file_path, file2);
  ASSERT_EQ(file2_info.num_entries, 200);
  ASSERT_EQ(file2_info.smallest_key, Key(100));
  ASSERT_EQ(file2_info.largest_key, Key(299));
  // file3.sst (110 => 124) .. overlap with file2.sst
  std::string file3 = sst_files_dir_ + ""file3.sst"";
  ASSERT_OK(sst_file_writer.Open(file3));
  for (int k = 110; k < 125; k++) {
    ASSERT_OK(sst_file_writer.Put(Key(k), Key(k) + ""_val_overlap""));
  }
  ExternalSstFileInfo file3_info;
  s = sst_file_writer.Finish(&file3_info);
  ASSERT_OK(s) << s.ToString();
  ASSERT_EQ(file3_info.file_path, file3);
  ASSERT_EQ(file3_info.num_entries, 15);
  ASSERT_EQ(file3_info.smallest_key, Key(110));
  ASSERT_EQ(file3_info.largest_key, Key(124));

  s = DeprecatedAddFile({file1}, true /* move file */);
  ASSERT_OK(s) << s.ToString();
  ASSERT_EQ(Status::NotFound(), env_->FileExists(file1));

  s = DeprecatedAddFile({file2}, false /* copy file */);
  ASSERT_OK(s) << s.ToString();
  ASSERT_OK(env_->FileExists(file2));
  // This file has overlapping values with the existing data
  s = DeprecatedAddFile({file3}, true /* move file */);
  ASSERT_NOK(s) << s.ToString();
  ASSERT_OK(env_->FileExists(file3));
  for (int k = 0; k < 300; k++) {
    ASSERT_EQ(Get(Key(k)), Key(k) + ""_val"");
  }
}",time
"TEST_P(FaultInjectionTest, WriteBatchWalTerminationTest) {
  ReadOptions ro;
  Options options = CurrentOptions();
  options.env = env_;
  WriteOptions wo;
  wo.sync = true;
  wo.disableWAL = false;
  WriteBatch batch;
  ASSERT_OK(batch.Put(""cats"", ""dogs""));
  batch.MarkWalTerminationPoint();
  ASSERT_OK(batch.Put(""boys"", ""girls""));
  ASSERT_OK(db_->Write(wo, &batch));
  env_->SetFilesystemActive(false);
  NoWriteTestReopenWithFault(kResetDropAndDeleteUnsynced);
  ASSERT_OK(OpenDB());
  std::string val;
  ASSERT_OK(db_->Get(ro, ""cats"", &val));
  ASSERT_EQ(""dogs"", val);
  ASSERT_EQ(db_->Get(ro, ""boys"", &val), Status::NotFound());
}",time
"TEST_F(ImportColumnFamilyTest, ImportColumnFamilyNegativeTest) {
  Options options = CurrentOptions();
  CreateAndReopenWithCF({""koko""}, options);
  {
    // Create column family with existing cf name.
    ExportImportFilesMetaData metadata;

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""koko"",
                                                ImportColumnFamilyOptions(),
                                                metadata, &import_cfh_),
              Status::InvalidArgument(""Column family already exists""));
    ASSERT_EQ(import_cfh_, nullptr);
  }

  {
    // Import with no files specified.
    ExportImportFilesMetaData metadata;

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""yoyo"",
                                                ImportColumnFamilyOptions(),
                                                metadata, &import_cfh_),
              Status::InvalidArgument(""The list of files is empty""));
    ASSERT_EQ(import_cfh_, nullptr);
  }

  {
    // Import with overlapping keys in sst files.
    ExportImportFilesMetaData metadata;
    SstFileWriter sfw_cf1(EnvOptions(), options, handles_[1]);
    const std::string file1_sst_name = ""file1.sst"";
    const std::string file1_sst = sst_files_dir_ + file1_sst_name;
    ASSERT_OK(sfw_cf1.Open(file1_sst));
    ASSERT_OK(sfw_cf1.Put(""K1"", ""V1""));
    ASSERT_OK(sfw_cf1.Put(""K2"", ""V2""));
    ASSERT_OK(sfw_cf1.Finish());
    const std::string file2_sst_name = ""file2.sst"";
    const std::string file2_sst = sst_files_dir_ + file2_sst_name;
    ASSERT_OK(sfw_cf1.Open(file2_sst));
    ASSERT_OK(sfw_cf1.Put(""K2"", ""V2""));
    ASSERT_OK(sfw_cf1.Put(""K3"", ""V3""));
    ASSERT_OK(sfw_cf1.Finish());
    metadata.files.push_back(
        LiveFileMetaDataInit(file1_sst_name, sst_files_dir_, 1, 10, 19));
    metadata.files.push_back(
        LiveFileMetaDataInit(file2_sst_name, sst_files_dir_, 1, 10, 19));
    metadata.db_comparator_name = options.comparator->Name();

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""yoyo"",
                                                ImportColumnFamilyOptions(),
                                                metadata, &import_cfh_),
              Status::InvalidArgument(""Files have overlapping ranges""));
    ASSERT_EQ(import_cfh_, nullptr);
  }

  {
    // Import with a mismatching comparator, should fail with appropriate error.
    ExportImportFilesMetaData metadata;
    Options mismatch_options = CurrentOptions();
    mismatch_options.comparator = ReverseBytewiseComparator();
    SstFileWriter sfw_cf1(EnvOptions(), mismatch_options, handles_[1]);
    const std::string file1_sst_name = ""file1.sst"";
    const std::string file1_sst = sst_files_dir_ + file1_sst_name;
    ASSERT_OK(sfw_cf1.Open(file1_sst));
    ASSERT_OK(sfw_cf1.Put(""K2"", ""V2""));
    ASSERT_OK(sfw_cf1.Put(""K1"", ""V1""));
    ASSERT_OK(sfw_cf1.Finish());
    metadata.files.push_back(
        LiveFileMetaDataInit(file1_sst_name, sst_files_dir_, 1, 10, 19));
    metadata.db_comparator_name = mismatch_options.comparator->Name();

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""coco"",
                                                ImportColumnFamilyOptions(),
                                                metadata, &import_cfh_),
              Status::InvalidArgument(""Comparator name mismatch""));
    ASSERT_EQ(import_cfh_, nullptr);
  }

  {
    // Import with non existent sst file should fail with appropriate error
    ExportImportFilesMetaData metadata;
    SstFileWriter sfw_cf1(EnvOptions(), options, handles_[1]);
    const std::string file1_sst_name = ""file1.sst"";
    const std::string file1_sst = sst_files_dir_ + file1_sst_name;
    ASSERT_OK(sfw_cf1.Open(file1_sst));
    ASSERT_OK(sfw_cf1.Put(""K1"", ""V1""));
    ASSERT_OK(sfw_cf1.Put(""K2"", ""V2""));
    ASSERT_OK(sfw_cf1.Finish());
    const std::string file3_sst_name = ""file3.sst"";
    metadata.files.push_back(
        LiveFileMetaDataInit(file1_sst_name, sst_files_dir_, 1, 10, 19));
    metadata.files.push_back(
        LiveFileMetaDataInit(file3_sst_name, sst_files_dir_, 1, 10, 19));
    metadata.db_comparator_name = options.comparator->Name();

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""yoyo"",
                                                ImportColumnFamilyOptions(),
                                                metadata, &import_cfh_),
              Status::IOError(""No such file or directory""));
    ASSERT_EQ(import_cfh_, nullptr);

    // Test successful import after a failure with the same CF name. Ensures
    // there is no side effect with CF when there is a failed import
    metadata.files.pop_back();
    metadata.db_comparator_name = options.comparator->Name();
    ASSERT_OK(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""yoyo"",
                                                ImportColumnFamilyOptions(),
                                                metadata, &import_cfh_));
    ASSERT_NE(import_cfh_, nullptr);
  }
}",time
"Status StressTest::AssertSame(DB* db, ColumnFamilyHandle* cf,
                              ThreadState::SnapshotState& snap_state) {
  Status s;
  if (cf->GetName() != snap_state.cf_at_name) {
    return s;
  }
  // This `ReadOptions` is for validation purposes. Ignore
  // `FLAGS_rate_limit_user_ops` to avoid slowing any validation.
  ReadOptions ropt;
  ropt.snapshot = snap_state.snapshot;
  Slice ts;
  if (!snap_state.timestamp.empty()) {
    ts = snap_state.timestamp;
    ropt.timestamp = &ts;
  }
  PinnableSlice exp_v(&snap_state.value);
  exp_v.PinSelf();
  PinnableSlice v;
  s = db->Get(ropt, cf, snap_state.key, &v);
  if (!s.ok() && !s.IsNotFound()) {
    return s;
  }
  if (snap_state.status != s) {
    return Status::Corruption(
        ""The snapshot gave inconsistent results for key "" +
        ToString(Hash(snap_state.key.c_str(), snap_state.key.size(), 0)) +
        "" in cf "" + cf->GetName() + "": ("" + snap_state.status.ToString() +
        "") vs. ("" + s.ToString() + "")"");
  }
  if (s.ok()) {
    if (exp_v != v) {
      return Status::Corruption(""The snapshot gave inconsistent values: ("" +
                                exp_v.ToString() + "") vs. ("" + v.ToString() +
                                "")"");
    }
  }
  if (snap_state.key_vec != nullptr) {
    std::vector<bool> tmp_bitvec = GetKeyBitVec(db, ropt);
    if (!std::equal(snap_state.key_vec->begin(), snap_state.key_vec->end(),
                    tmp_bitvec.begin())) {
      return Status::Corruption(""Found inconsistent keys at this snapshot"");
    }
  }
  return Status::OK();
}",time
"TEST_P(EnvBasicTestWithParam, Basics) {
  uint64_t file_size;
  std::unique_ptr<WritableFile> writable_file;
  std::vector<std::string> children;

  // Check that the directory is empty.
  ASSERT_EQ(Status::NotFound(), env_->FileExists(test_dir_ + ""/non_existent""));
  ASSERT_TRUE(!env_->GetFileSize(test_dir_ + ""/non_existent"", &file_size).ok());
  ASSERT_OK(env_->GetChildren(test_dir_, &children));
  ASSERT_EQ(0U, children.size());
  // Create a file.
  ASSERT_OK(env_->NewWritableFile(test_dir_ + ""/f"", &writable_file, soptions_));
  ASSERT_OK(writable_file->Close());
  writable_file.reset();
  // Check that the file exists.
  ASSERT_OK(env_->FileExists(test_dir_ + ""/f""));
  ASSERT_OK(env_->GetFileSize(test_dir_ + ""/f"", &file_size));
  ASSERT_EQ(0U, file_size);
  ASSERT_OK(env_->GetChildren(test_dir_, &children));
  ASSERT_EQ(1U, children.size());
  ASSERT_EQ(""f"", children[0]);
  ASSERT_OK(env_->DeleteFile(test_dir_ + ""/f""));
  // Write to the file.
  ASSERT_OK(
      env_->NewWritableFile(test_dir_ + ""/f1"", &writable_file, soptions_));
  ASSERT_OK(writable_file->Append(""abc""));
  ASSERT_OK(writable_file->Close());
  writable_file.reset();
  ASSERT_OK(
      env_->NewWritableFile(test_dir_ + ""/f2"", &writable_file, soptions_));
  ASSERT_OK(writable_file->Close());
  writable_file.reset();
  // Check for expected size.
  ASSERT_OK(env_->GetFileSize(test_dir_ + ""/f1"", &file_size));
  ASSERT_EQ(3U, file_size);
  // Check that renaming works.
  ASSERT_TRUE(
      !env_->RenameFile(test_dir_ + ""/non_existent"", test_dir_ + ""/g"").ok());
  ASSERT_OK(env_->RenameFile(test_dir_ + ""/f1"", test_dir_ + ""/g""));
  ASSERT_EQ(Status::NotFound(), env_->FileExists(test_dir_ + ""/f1""));
  ASSERT_OK(env_->FileExists(test_dir_ + ""/g""));
  ASSERT_OK(env_->GetFileSize(test_dir_ + ""/g"", &file_size));
  ASSERT_EQ(3U, file_size);
  // Check that renaming overwriting works
  ASSERT_OK(env_->RenameFile(test_dir_ + ""/f2"", test_dir_ + ""/g""));
  ASSERT_OK(env_->GetFileSize(test_dir_ + ""/g"", &file_size));
  ASSERT_EQ(0U, file_size);
  // Check that opening non-existent file fails.
  std::unique_ptr<SequentialFile> seq_file;
  std::unique_ptr<RandomAccessFile> rand_file;
  ASSERT_TRUE(!env_->NewSequentialFile(test_dir_ + ""/non_existent"", &seq_file,
                                       soptions_)
                   .ok());
  ASSERT_TRUE(!seq_file);
  ASSERT_NOK(env_->NewRandomAccessFile(test_dir_ + ""/non_existent"", &rand_file,
                                       soptions_));
  ASSERT_TRUE(!rand_file);
  // Check that deleting works.
  ASSERT_NOK(env_->DeleteFile(test_dir_ + ""/non_existent""));
  ASSERT_OK(env_->DeleteFile(test_dir_ + ""/g""));
  ASSERT_EQ(Status::NotFound(), env_->FileExists(test_dir_ + ""/g""));
  ASSERT_OK(env_->GetChildren(test_dir_, &children));
  ASSERT_EQ(0U, children.size());
  Status s = env_->GetChildren(test_dir_ + ""/non_existent"", &children);
  ASSERT_TRUE(s.IsNotFound());
}",time
"TEST_F(AutoRollLoggerTest, RollLogFileByTime) {
  auto nsc =
      std::make_shared<EmulatedSystemClock>(SystemClock::Default(), true);
  size_t time = 2;
  size_t log_size = 1024 * 5;
  size_t keep_log_file_num = 10;

  InitTestDb();
  // -- Test the existence of file during the server restart.
  ASSERT_EQ(Status::NotFound(), default_env->FileExists(kLogFile));
  AutoRollLogger logger(default_env->GetFileSystem(), nsc, kTestDir, """",
                        log_size, time, keep_log_file_num);
  ASSERT_OK(default_env->FileExists(kLogFile));
  RollLogFileByTimeTest(default_env->GetFileSystem(), nsc, &logger, time,
                        kSampleMessage + "":RollLogFileByTime"");
}",time
"TEST_F(EnvLoggerTest, EmptyLogFile) {
  auto logger = CreateLogger();
  ASSERT_EQ(logger->Close(), Status::OK());

  // Check the size of the log file.
  uint64_t file_size;
  ASSERT_EQ(env_->GetFileSize(kLogFile, &file_size), Status::OK());
  ASSERT_EQ(file_size, 0);
  DeleteLogFile();
}",time
"TEST_F(BackupEngineTest, CorruptionsTest) {
  const int keys_iteration = 5000;
  Random rnd(6);
  Status s;
  OpenDBAndBackupEngine(true);
  // create five backups
  for (int i = 0; i < 5; ++i) {
    FillDB(db_.get(), keys_iteration * i, keys_iteration * (i + 1));
    ASSERT_OK(backup_engine_->CreateNewBackup(db_.get(), !!(rnd.Next() % 2)));
  }
  // ---------- case 1. - fail a write -----------
  // try creating backup 6, but fail a write
  FillDB(db_.get(), keys_iteration * 5, keys_iteration * 6);
  test_backup_fs_->SetLimitWrittenFiles(2);
  // should fail
  s = backup_engine_->CreateNewBackup(db_.get(), !!(rnd.Next() % 2));
  ASSERT_NOK(s);
  test_backup_fs_->SetLimitWrittenFiles(1000000);
  // latest backup should have all the keys
  CloseDBAndBackupEngine();
  AssertBackupConsistency(0, 0, keys_iteration * 5, keys_iteration * 6);
  // --------- case 2. corrupted backup meta or missing backuped file ----
  ASSERT_OK(file_manager_->CorruptFile(backupdir_ + ""/meta/5"", 3));
  // since 5 meta is now corrupted, latest backup should be 4
  AssertBackupConsistency(0, 0, keys_iteration * 4, keys_iteration * 5);
  OpenBackupEngine();
  s = backup_engine_->RestoreDBFromBackup(5, dbname_, dbname_);
  ASSERT_NOK(s);
  CloseBackupEngine();
  ASSERT_OK(file_manager_->DeleteRandomFileInDir(backupdir_ + ""/private/4""));
  // 4 is corrupted, 3 is the latest backup now
  AssertBackupConsistency(0, 0, keys_iteration * 3, keys_iteration * 5);
  OpenBackupEngine();
  s = backup_engine_->RestoreDBFromBackup(4, dbname_, dbname_);
  CloseBackupEngine();
  ASSERT_NOK(s);
  // --------- case 3. corrupted checksum value ----
  ASSERT_OK(file_manager_->CorruptChecksum(backupdir_ + ""/meta/3"", false));
  // checksum of backup 3 is an invalid value, this can be detected at
  // db open time, and it reverts to the previous backup automatically
  AssertBackupConsistency(0, 0, keys_iteration * 2, keys_iteration * 5);
  // checksum of the backup 2 appears to be valid, this can cause checksum
  // mismatch and abort restore process
  ASSERT_OK(file_manager_->CorruptChecksum(backupdir_ + ""/meta/2"", true));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/2""));
  OpenBackupEngine();
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/2""));
  s = backup_engine_->RestoreDBFromBackup(2, dbname_, dbname_);
  ASSERT_NOK(s);
  // make sure that no corrupt backups have actually been deleted!
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/1""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/2""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/3""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/4""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/5""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/1""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/2""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/3""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/4""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/5""));
  // delete the corrupt backups and then make sure they're actually deleted
  ASSERT_OK(backup_engine_->DeleteBackup(5));
  ASSERT_OK(backup_engine_->DeleteBackup(4));
  ASSERT_OK(backup_engine_->DeleteBackup(3));
  ASSERT_OK(backup_engine_->DeleteBackup(2));
  // Should not be needed anymore with auto-GC on DeleteBackup
  //(void)backup_engine_->GarbageCollect();
  ASSERT_EQ(Status::NotFound(),
            file_manager_->FileExists(backupdir_ + ""/meta/5""));
  ASSERT_EQ(Status::NotFound(),
            file_manager_->FileExists(backupdir_ + ""/private/5""));
  ASSERT_EQ(Status::NotFound(),
            file_manager_->FileExists(backupdir_ + ""/meta/4""));
  ASSERT_EQ(Status::NotFound(),
            file_manager_->FileExists(backupdir_ + ""/private/4""));
  ASSERT_EQ(Status::NotFound(),
            file_manager_->FileExists(backupdir_ + ""/meta/3""));
  ASSERT_EQ(Status::NotFound(),
            file_manager_->FileExists(backupdir_ + ""/private/3""));
  ASSERT_EQ(Status::NotFound(),
            file_manager_->FileExists(backupdir_ + ""/meta/2""));
  ASSERT_EQ(Status::NotFound(),
            file_manager_->FileExists(backupdir_ + ""/private/2""));
  CloseBackupEngine();
  AssertBackupConsistency(0, 0, keys_iteration * 1, keys_iteration * 5);

  // new backup should be 2!
  OpenDBAndBackupEngine();
  FillDB(db_.get(), keys_iteration * 1, keys_iteration * 2);
  ASSERT_OK(backup_engine_->CreateNewBackup(db_.get(), !!(rnd.Next() % 2)));
  CloseDBAndBackupEngine();
  AssertBackupConsistency(2, 0, keys_iteration * 2, keys_iteration * 5);
}",time
"TEST_F(BackupEngineTest, DeleteTmpFiles) {
  for (int cleanup_fn : {1, 2, 3, 4}) {
    for (ShareOption shared_option : kAllShareOptions) {
      OpenDBAndBackupEngine(false /* destroy_old_data */, false /* dummy */,
                            shared_option);
      ASSERT_OK(backup_engine_->CreateNewBackup(db_.get()));
      BackupID next_id = 1;
      BackupID oldest_id = std::numeric_limits<BackupID>::max();
      {
        std::vector<BackupInfo> backup_info;
        backup_engine_->GetBackupInfo(&backup_info);
        for (const auto& bi : backup_info) {
          next_id = std::max(next_id, bi.backup_id + 1);
          oldest_id = std::min(oldest_id, bi.backup_id);
        }
      }
      CloseDBAndBackupEngine();
      // An aborted or incomplete new backup will always be in the next
      // id (maybe more)
      std::string next_private = ""private/"" + std::to_string(next_id);
      // NOTE: both shared and shared_checksum should be cleaned up
      // regardless of how the backup engine is opened.
      std::vector<std::string> tmp_files_and_dirs;
      for (const auto& dir_and_file : {
               std::make_pair(std::string(""shared""),
                              std::string("".00006.sst.tmp"")),
               std::make_pair(std::string(""shared_checksum""),
                              std::string("".00007.sst.tmp"")),
               std::make_pair(next_private, std::string(""00003.sst"")),
           }) {
        std::string dir = backupdir_ + ""/"" + dir_and_file.first;
        ASSERT_OK(file_manager_->CreateDirIfMissing(dir));
        ASSERT_OK(file_manager_->FileExists(dir));
        std::string file = dir + ""/"" + dir_and_file.second;
        ASSERT_OK(file_manager_->WriteToFile(file, ""tmp""));
        ASSERT_OK(file_manager_->FileExists(file));
        tmp_files_and_dirs.push_back(file);
      }
      if (cleanup_fn != /*CreateNewBackup*/ 4) {
        // This exists after CreateNewBackup because it's deleted then
        // re-created.
        tmp_files_and_dirs.push_back(backupdir_ + ""/"" + next_private);
      }
      OpenDBAndBackupEngine(false /* destroy_old_data */, false /* dummy */,
                            shared_option);
      // Need to call one of these explicitly to delete tmp files
      switch (cleanup_fn) {
        case 1:
          ASSERT_OK(backup_engine_->GarbageCollect());
          break;
        case 2:
          ASSERT_OK(backup_engine_->DeleteBackup(oldest_id));
          break;
        case 3:
          ASSERT_OK(backup_engine_->PurgeOldBackups(1));
          break;
        case 4:
          // Does a garbage collect if it sees that next private dir exists
          ASSERT_OK(backup_engine_->CreateNewBackup(db_.get()));
          break;
        default:
          assert(false);
      }
      CloseDBAndBackupEngine();
      for (std::string file_or_dir : tmp_files_and_dirs) {
        if (file_manager_->FileExists(file_or_dir) != Status::NotFound()) {
          FAIL() << file_or_dir << "" was expected to be deleted."" << cleanup_fn;
        }
      }
    }
  }
}",time
"TEST_F(CheckpointTest, ExportColumnFamilyNegativeTest) {
  // Create a database
  auto options = CurrentOptions();
  options.create_if_missing = true;
  CreateAndReopenWithCF({}, options);
  const auto key = std::string(""foo"");
  ASSERT_OK(Put(key, ""v1""));
  Checkpoint* checkpoint;
  ASSERT_OK(Checkpoint::Create(db_, &checkpoint));

  // Export onto existing directory
  ASSERT_OK(env_->CreateDirIfMissing(export_path_));
  ASSERT_EQ(checkpoint->ExportColumnFamily(db_->DefaultColumnFamily(),
                                           export_path_, &metadata_),
            Status::InvalidArgument(""Specified export_dir exists""));
  ASSERT_OK(DestroyDir(env_, export_path_));

  // Export with invalid directory specification
  export_path_ = """";
  ASSERT_EQ(checkpoint->ExportColumnFamily(db_->DefaultColumnFamily(),
                                           export_path_, &metadata_),
            Status::InvalidArgument(""Specified export_dir invalid""));
  delete checkpoint;
}",time
"TEST_F(DBBasicTestWithTimestamp, ModifyHistoryTsLowUsingAPI) {
  Options opts = CurrentOptions();
  opts.env = env_;
  opts.create_if_missing = true;
  const size_t ts_size = Timestamp(0, 0).size();
  TestComparator cmp(ts_size);
  opts.comparator = &cmp;
  DestroyAndReopen(opts);
  std::string low_ts_str = Timestamp(9, 0);
  ASSERT_OK(db_->IncreaseFullHistoryTsLow(db_->DefaultColumnFamily(), low_ts_str));
  std::string retrieved_low_ts;
  ASSERT_OK(db_->GetFullHistoryTsLow(nullptr, &retrieved_low_ts));
  ASSERT_TRUE(cmp.CompareTimestamp(low_ts_str, retrieved_low_ts) == 0);
  
  // testing reverse full_history_low increment
  std::string backward_ts_str = Timestamp(8, 0);
  auto result = db_->IncreaseFullHistoryTsLow(db_->DefaultColumnFamily(), backward_ts_str);
  ASSERT_EQ(result, Status::InvalidArgument());
  
  // testing IncreaseFullHistoryTsLow with a longer timestamp
  std::string long_ts_str(Timestamp(0, 0).size() + 1, 'b');
  result = db_->IncreaseFullHistoryTsLow(db_->DefaultColumnFamily(), long_ts_str);
  ASSERT_EQ(result, Status::InvalidArgument());
  
  // testing IncreaseFullHistoryTsLow with a null timestamp
  std::string null_ts_str = """";
  result = db_->IncreaseFullHistoryTsLow(db_->DefaultColumnFamily(), null_ts_str);
  ASSERT_EQ(result, Status::InvalidArgument());
  
  // testing column family that has no timestamp enabled
  opts.comparator = BytewiseComparator();
  DestroyAndReopen(opts);
  low_ts_str = Timestamp(10, 0);
  result = db_->IncreaseFullHistoryTsLow(db_->DefaultColumnFamily(), low_ts_str);
  ASSERT_EQ(result, Status::InvalidArgument());
  
  // testing GetFullHistoryTsLow for a column family without timestamps
  std::string current_history_ts_low;
  result = db_->GetFullHistoryTsLow(db_->DefaultColumnFamily(), &current_history_ts_low);
  ASSERT_EQ(result, Status::InvalidArgument());
  Close();
}",time
"TEST_F(DBBasicTestWithTimestamp, UpdateHistoryTimestampLowWithAPI) {
  Options config = CurrentOptions();
  config.env = env_;
  config.create_if_missing = true;
  const size_t ts_size_bytes = Timestamp(0, 0).size();
  TestComparator timestamp_comparator(ts_size_bytes);
  config.comparator = &timestamp_comparator;
  DestroyAndReopen(config);
  std::string history_ts_low_str = Timestamp(9, 0);
  ASSERT_OK(db_->IncreaseFullHistoryTsLow(db_->DefaultColumnFamily(), history_ts_low_str));
  std::string fetched_history_ts_low;
  ASSERT_OK(db_->GetFullHistoryTsLow(nullptr, &fetched_history_ts_low));
  ASSERT_TRUE(timestamp_comparator.CompareTimestamp(history_ts_low_str, fetched_history_ts_low) == 0);
  
  // test full_history_low reverse increment
  std::string history_ts_low_backward = Timestamp(8, 0);
  auto result_status = db_->IncreaseFullHistoryTsLow(db_->DefaultColumnFamily(), history_ts_low_backward);
  ASSERT_EQ(result_status, Status::InvalidArgument());
  
  // test IncreaseFullHistoryTsLow with longer timestamp than allowed
  std::string long_history_ts(Timestamp(0, 0).size() + 1, 'x');
  result_status = db_->IncreaseFullHistoryTsLow(db_->DefaultColumnFamily(), long_history_ts);
  ASSERT_EQ(result_status, Status::InvalidArgument());
  
  // test IncreaseFullHistoryTsLow with empty timestamp
  std::string null_history_ts = """";
  result_status = db_->IncreaseFullHistoryTsLow(db_->DefaultColumnFamily(), null_history_ts);
  ASSERT_EQ(result_status, Status::InvalidArgument());
  
  // test column family without timestamp feature enabled
  config.comparator = BytewiseComparator();
  DestroyAndReopen(config);
  history_ts_low_str = Timestamp(10, 0);
  result_status = db_->IncreaseFullHistoryTsLow(db_->DefaultColumnFamily(), history_ts_low_str);
  ASSERT_EQ(result_status, Status::InvalidArgument());
  
  // test GetFullHistoryTsLow without timestamp-enabled column family
  std::string cur_history_ts_low;
  result_status = db_->GetFullHistoryTsLow(db_->DefaultColumnFamily(), &cur_history_ts_low);
  ASSERT_EQ(result_status, Status::InvalidArgument());
  Close();
}",time
"TEST_F(DBBasicTestWithTimestamp, SetFullHistoryTsLowViaAPI) {
  Options db_opts = CurrentOptions();
  db_opts.env = env_;
  db_opts.create_if_missing = true;
  const size_t timestamp_length = Timestamp(0, 0).size();
  TestComparator ts_comparator(timestamp_length);
  db_opts.comparator = &ts_comparator;
  DestroyAndReopen(db_opts);
  std::string ts_low_value_str = Timestamp(9, 0);
  ASSERT_OK(db_->IncreaseFullHistoryTsLow(db_->DefaultColumnFamily(), ts_low_value_str));
  std::string received_ts_low_value;
  ASSERT_OK(db_->GetFullHistoryTsLow(nullptr, &received_ts_low_value));
  ASSERT_TRUE(ts_comparator.CompareTimestamp(ts_low_value_str, received_ts_low_value) == 0);
  
  // validate reverse full_history_low increment
  std::string reverse_ts_low_value = Timestamp(8, 0);
  auto error_status = db_->IncreaseFullHistoryTsLow(db_->DefaultColumnFamily(), reverse_ts_low_value);
  ASSERT_EQ(error_status, Status::InvalidArgument());
  
  // validate IncreaseFullHistoryTsLow for long timestamp
  std::string extended_ts_str(Timestamp(0, 0).size() + 1, 'y');
  error_status = db_->IncreaseFullHistoryTsLow(db_->DefaultColumnFamily(), extended_ts_str);
  ASSERT_EQ(error_status, Status::InvalidArgument());
  
  // validate IncreaseFullHistoryTsLow for null timestamp
  std::string null_ts_low_value = """";
  error_status = db_->IncreaseFullHistoryTsLow(db_->DefaultColumnFamily(), null_ts_low_value);
  ASSERT_EQ(error_status, Status::InvalidArgument());
  
  // validate column family without timestamps enabled
  db_opts.comparator = BytewiseComparator();
  DestroyAndReopen(db_opts);
  ts_low_value_str = Timestamp(10, 0);
  error_status = db_->IncreaseFullHistoryTsLow(db_->DefaultColumnFamily(), ts_low_value_str);
  ASSERT_EQ(error_status, Status::InvalidArgument());
  
  // validate GetFullHistoryTsLow without timestamp-enabled column family
  std::string cur_full_history_ts;
  error_status = db_->GetFullHistoryTsLow(db_->DefaultColumnFamily(), &cur_full_history_ts);
  ASSERT_EQ(error_status, Status::InvalidArgument());
  Close();
}",time
"TEST_F(DBBasicTestWithTimestamp, AdjustHistoryTsLowThroughPublicAPI) {
  Options current_options = CurrentOptions();
  current_options.env = env_;
  current_options.create_if_missing = true;
  const size_t ts_bytes_size = Timestamp(0, 0).size();
  TestComparator custom_comparator(ts_bytes_size);
  current_options.comparator = &custom_comparator;
  DestroyAndReopen(current_options);
  std::string low_ts_string = Timestamp(9, 0);
  ASSERT_OK(db_->IncreaseFullHistoryTsLow(db_->DefaultColumnFamily(), low_ts_string));
  std::string retrieved_low_ts_string;
  ASSERT_OK(db_->GetFullHistoryTsLow(nullptr, &retrieved_low_ts_string));
  ASSERT_TRUE(custom_comparator.CompareTimestamp(low_ts_string, retrieved_low_ts_string) == 0);

  // validate increasing full_history_low in the reverse direction
  std::string reversed_low_ts_string = Timestamp(8, 0);
  auto return_status = db_->IncreaseFullHistoryTsLow(db_->DefaultColumnFamily(), reversed_low_ts_string);
  ASSERT_EQ(return_status, Status::InvalidArgument());

  // validate IncreaseFullHistoryTsLow with an excessively long timestamp
  std::string long_ts_string(Timestamp(0, 0).size() + 1, 'z');
  return_status = db_->IncreaseFullHistoryTsLow(db_->DefaultColumnFamily(), long_ts_string);
  ASSERT_EQ(return_status, Status::InvalidArgument());

  // validate IncreaseFullHistoryTsLow with an empty timestamp
  std::string empty_ts_string = """";
  return_status = db_->IncreaseFullHistoryTsLow(db_->DefaultColumnFamily(), empty_ts_string);
  ASSERT_EQ(return_status, Status::InvalidArgument());

  // validate behavior for column families with timestamps disabled
  current_options.comparator = BytewiseComparator();
  DestroyAndReopen(current_options);
  low_ts_string = Timestamp(10, 0);
  return_status = db_->IncreaseFullHistoryTsLow(db_->DefaultColumnFamily(), low_ts_string);
  ASSERT_EQ(return_status, Status::InvalidArgument());

  // validate GetFullHistoryTsLow behavior for timestamp-disabled column families
  std::string current_ts_string;
  return_status = db_->GetFullHistoryTsLow(db_->DefaultColumnFamily(), &current_ts_string);
  ASSERT_EQ(return_status, Status::InvalidArgument());
  Close();
}",time
"TEST_F(DBBasicTestWithTimestamp, HistoryTrimmingTest) {
  Options db_options = CurrentOptions();
  db_options.env = env_;
  db_options.create_if_missing = true;
  const size_t ts_size = Timestamp(0, 0).size();
  TestComparator comparator(ts_size);
  db_options.comparator = &comparator;
  DestroyAndReopen(db_options);
  auto verify_value_by_timestamp = [](DB* db, Slice key, std::string read_ts,
                                      Status expected_status, std::string expected_value) {
    ReadOptions read_opts;
    Slice ts = read_ts;
    read_opts.timestamp = &ts;
    std::string value;
    Status result_status = db->Get(read_opts, key, &value);
    ASSERT_TRUE(result_status == expected_status);
    if (result_status.ok()) {
      ASSERT_EQ(expected_value, value);
    }
  };
  
  // Insert versions with various timestamps
  ASSERT_OK(db_->Put(WriteOptions(), ""key1"", Timestamp(2, 0), ""val1""));
  ASSERT_OK(db_->Put(WriteOptions(), ""key1"", Timestamp(4, 0), ""val2""));
  ASSERT_OK(db_->Delete(WriteOptions(), ""key1"", Timestamp(5, 0)));
  ASSERT_OK(db_->Put(WriteOptions(), ""key1"", Timestamp(6, 0), ""val3""));
  verify_value_by_timestamp(db_, ""key1"", Timestamp(7, 0), Status::OK(), ""val3"");
  ASSERT_OK(Flush());
  Close();

  ColumnFamilyOptions column_family_options(db_options);
  std::vector<ColumnFamilyDescriptor> families;
  families.push_back(ColumnFamilyDescriptor(kDefaultColumnFamilyName, column_family_options));
  DBOptions db_open_options(db_options);

  // Trim data where version > Timestamp(5, 0), expecting NOT_FOUND for read(k1, ts(7)).
  ASSERT_OK(DB::OpenAndTrimHistory(db_open_options, dbname_, families, &handles_, &db_, Timestamp(5, 0)));
  verify_value_by_timestamp(db_, ""key1"", Timestamp(7, 0), Status::NotFound(), """");
  Close();

  // Trim data where version > Timestamp(4, 0), expecting val2 for read(k1, ts(7)).
  ASSERT_OK(DB::OpenAndTrimHistory(db_open_options, dbname_, families, &handles_, &db_, Timestamp(4, 0)));
  verify_value_by_timestamp(db_, ""key1"", Timestamp(7, 0), Status::OK(), ""val2"");
  Close();
}",time
"TEST_F(DBBasicTestWithTimestamp, PruneHistoryTest) {
  Options opts = CurrentOptions();
  opts.env = env_;
  opts.create_if_missing = true;
  const size_t timestamp_size = Timestamp(0, 0).size();
  TestComparator ts_cmp(timestamp_size);
  opts.comparator = &ts_cmp;
  DestroyAndReopen(opts);
  auto verify_ts_value = [](DB* db, Slice key, std::string ts, Status expected_status,
                            std::string expected_value) {
    ReadOptions ro;
    Slice timestamp = ts;
    ro.timestamp = &timestamp;
    std::string value;
    Status s = db->Get(ro, key, &value);
    ASSERT_TRUE(s == expected_status);
    if (s.ok()) {
      ASSERT_EQ(expected_value, value);
    }
  };

  // Create data of different timestamps
  ASSERT_OK(db_->Put(WriteOptions(), ""k2"", Timestamp(2, 0), ""val1""));
  ASSERT_OK(db_->Put(WriteOptions(), ""k2"", Timestamp(4, 0), ""val2""));
  ASSERT_OK(db_->Delete(WriteOptions(), ""k2"", Timestamp(5, 0)));
  ASSERT_OK(db_->Put(WriteOptions(), ""k2"", Timestamp(6, 0), ""val3""));
  verify_ts_value(db_, ""k2"", Timestamp(7, 0), Status::OK(), ""val3"");
  ASSERT_OK(Flush());
  Close();

  ColumnFamilyOptions cf_opts(opts);
  std::vector<ColumnFamilyDescriptor> families;
  families.push_back(ColumnFamilyDescriptor(kDefaultColumnFamilyName, cf_opts));
  DBOptions db_opts(opts);

  // Trim versions > Timestamp(5, 0), verify NOT_FOUND for read(k2, ts(7)).
  ASSERT_OK(DB::OpenAndTrimHistory(db_opts, dbname_, families, &handles_, &db_, Timestamp(5, 0)));
  verify_ts_value(db_, ""k2"", Timestamp(7, 0), Status::NotFound(), """");
  Close();

  // Trim versions > Timestamp(4, 0), verify val2 for read(k2, ts(7)).
  ASSERT_OK(DB::OpenAndTrimHistory(db_opts, dbname_, families, &handles_, &db_, Timestamp(4, 0)));
  verify_ts_value(db_, ""k2"", Timestamp(7, 0), Status::OK(), ""val2"");
  Close();
}",time
"TEST_F(DBBasicTestWithTimestamp, CleanHistoryByTimestampTest) {
  Options settings = CurrentOptions();
  settings.env = env_;
  settings.create_if_missing = true;
  const size_t ts_size = Timestamp(0, 0).size();
  TestComparator comparator(ts_size);
  settings.comparator = &comparator;
  DestroyAndReopen(settings);
  auto validate_ts_value = [](DB* db, Slice key, std::string ts, Status expected_status, std::string expected_value) {
    ReadOptions options;
    Slice timestamp = ts;
    options.timestamp = &timestamp;
    std::string value;
    Status result = db->Get(options, key, &value);
    ASSERT_TRUE(result == expected_status);
    if (result.ok()) {
      ASSERT_EQ(expected_value, value);
    }
  };

  // Insert records with varying timestamps
  ASSERT_OK(db_->Put(WriteOptions(), ""keyA"", Timestamp(2, 0), ""value1""));
  ASSERT_OK(db_->Put(WriteOptions(), ""keyA"", Timestamp(4, 0), ""value2""));
  ASSERT_OK(db_->Delete(WriteOptions(), ""keyA"", Timestamp(5, 0)));
  ASSERT_OK(db_->Put(WriteOptions(), ""keyA"", Timestamp(6, 0), ""value3""));
  validate_ts_value(db_, ""keyA"", Timestamp(7, 0), Status::OK(), ""value3"");
  ASSERT_OK(Flush());
  Close();

  ColumnFamilyOptions cf_settings(settings);
  std::vector<ColumnFamilyDescriptor> family_descriptors;
  family_descriptors.push_back(ColumnFamilyDescriptor(kDefaultColumnFamilyName, cf_settings));
  DBOptions db_opts(settings);

  // Trim records > Timestamp(5, 0), validate NOT_FOUND for keyA at ts(7).
  ASSERT_OK(DB::OpenAndTrimHistory(db_opts, dbname_, family_descriptors, &handles_, &db_, Timestamp(5, 0)));
  validate_ts_value(db_, ""keyA"", Timestamp(7, 0), Status::NotFound(), """");
  Close();

  // Trim records > Timestamp(4, 0), validate value2 for keyA at ts(7).
  ASSERT_OK(DB::OpenAndTrimHistory(db_opts, dbname_, family_descriptors, &handles_, &db_, Timestamp(4, 0)));
  validate_ts_value(db_, ""keyA"", Timestamp(7, 0), Status::OK(), ""value2"");
  Close();
}",time
"TEST_F(DBBasicTestWithTimestamp, TimestampHistoryPruningTest) {
  Options config = CurrentOptions();
  config.env = env_;
  config.create_if_missing = true;
  const size_t timestamp_bytes = Timestamp(0, 0).size();
  TestComparator ts_comparator(timestamp_bytes);
  config.comparator = &ts_comparator;
  DestroyAndReopen(config);
  auto check_value_at_ts = [](DB* db, Slice key, std::string read_ts,
                              Status expected_status, std::string expected_value) {
    ReadOptions ro;
    Slice ts = read_ts;
    ro.timestamp = &ts;
    std::string value;
    Status s = db->Get(ro, key, &value);
    ASSERT_TRUE(s == expected_status);
    if (s.ok()) {
      ASSERT_EQ(expected_value, value);
    }
  };

  // Write data with different timestamps
  ASSERT_OK(db_->Put(WriteOptions(), ""item1"", Timestamp(2, 0), ""valA""));
  ASSERT_OK(db_->Put(WriteOptions(), ""item1"", Timestamp(4, 0), ""valB""));
  ASSERT_OK(db_->Delete(WriteOptions(), ""item1"", Timestamp(5, 0)));
  ASSERT_OK(db_->Put(WriteOptions(), ""item1"", Timestamp(6, 0), ""valC""));
  check_value_at_ts(db_, ""item1"", Timestamp(7, 0), Status::OK(), ""valC"");
  ASSERT_OK(Flush());
  Close();

  ColumnFamilyOptions cf_opts(config);
  std::vector<ColumnFamilyDescriptor> families;
  families.push_back(ColumnFamilyDescriptor(kDefaultColumnFamilyName, cf_opts));
  DBOptions db_options(config);

  // Trim entries with timestamp > Timestamp(5, 0), expecting NOT_FOUND for item1 at ts(7).
  ASSERT_OK(DB::OpenAndTrimHistory(db_options, dbname_, families, &handles_, &db_, Timestamp(5, 0)));
  check_value_at_ts(db_, ""item1"", Timestamp(7, 0), Status::NotFound(), """");
  Close();

  // Trim entries with timestamp > Timestamp(4, 0), expecting valB for item1 at ts(7).
  ASSERT_OK(DB::OpenAndTrimHistory(db_options, dbname_, families, &handles_, &db_, Timestamp(4, 0)));
  check_value_at_ts(db_, ""item1"", Timestamp(7, 0), Status::OK(), ""valB"");
  Close();
}",time
"TEST_F(DBBasicTestWithTimestamp, HistoryTimestampTrimTest) {
  Options basic_options = CurrentOptions();
  basic_options.env = env_;
  basic_options.create_if_missing = true;
  const size_t ts_length = Timestamp(0, 0).size();
  TestComparator cmp(ts_length);
  basic_options.comparator = &cmp;
  DestroyAndReopen(basic_options);
  auto validate_value_at_ts = [](DB* db, Slice key, std::string timestamp, Status expected_status,
                                 std::string expected_value) {
    ReadOptions opts;
    Slice ts = timestamp;
    opts.timestamp = &ts;
    std::string value;
    Status s = db->Get(opts, key, &value);
    ASSERT_TRUE(s == expected_status);
    if (s.ok()) {
      ASSERT_EQ(expected_value, value);
    }
  };

  // Populate data with various timestamps
  ASSERT_OK(db_->Put(WriteOptions(), ""item2"", Timestamp(2, 0), ""value1""));
  ASSERT_OK(db_->Put(WriteOptions(), ""item2"", Timestamp(4, 0), ""value2""));
  ASSERT_OK(db_->Delete(WriteOptions(), ""item2"", Timestamp(5, 0)));
  ASSERT_OK(db_->Put(WriteOptions(), ""item2"", Timestamp(6, 0), ""value3""));
  validate_value_at_ts(db_, ""item2"", Timestamp(7, 0), Status::OK(), ""value3"");
  ASSERT_OK(Flush());
  Close();

  ColumnFamilyOptions cf_options(basic_options);
  std::vector<ColumnFamilyDescriptor> cf_descs;
  cf_descs.push_back(ColumnFamilyDescriptor(kDefaultColumnFamilyName, cf_options));
  DBOptions reopen_opts(basic_options);

  // Trim versions greater than Timestamp(5, 0), validate NOT_FOUND for item2 at ts(7).
  ASSERT_OK(DB::OpenAndTrimHistory(reopen_opts, dbname_, cf_descs, &handles_, &db_, Timestamp(5, 0)));
  validate_value_at_ts(db_, ""item2"", Timestamp(7, 0), Status::NotFound(), """");
  Close();

  // Trim versions greater than Timestamp(4, 0), validate value2 for item2 at ts(7).
  ASSERT_OK(DB::OpenAndTrimHistory(reopen_opts, dbname_, cf_descs, &handles_, &db_, Timestamp(4, 0)));
  validate_value_at_ts(db_, ""item2"", Timestamp(7, 0), Status::OK(), ""value2"");
  Close();
}",time
"TEST_F(ExternalSSTFileBasicTest, NoFileCopyTest) {
  Options opts = CurrentOptions();
  const ImmutableCFOptions imm_options(opts);
  SstFileWriter writer(EnvOptions(), opts);
  
  // file_a.sst (0 => 99)
  std::string file_a = sst_files_dir_ + ""file_a.sst"";
  ASSERT_OK(writer.Open(file_a));
  for (int i = 0; i < 100; i++) {
    ASSERT_OK(writer.Put(Key(i), Key(i) + ""_value""));
  }
  ExternalSstFileInfo info_a;
  Status result = writer.Finish(&info_a);
  ASSERT_OK(result) << result.ToString();
  ASSERT_EQ(info_a.file_path, file_a);
  ASSERT_EQ(info_a.num_entries, 100);
  ASSERT_EQ(info_a.smallest_key, Key(0));
  ASSERT_EQ(info_a.largest_key, Key(99));

  // file_b.sst (100 => 299)
  std::string file_b = sst_files_dir_ + ""file_b.sst"";
  ASSERT_OK(writer.Open(file_b));
  for (int i = 100; i < 300; i++) {
    ASSERT_OK(writer.Put(Key(i), Key(i) + ""_value""));
  }
  ExternalSstFileInfo info_b;
  result = writer.Finish(&info_b);
  ASSERT_OK(result) << result.ToString();
  ASSERT_EQ(info_b.file_path, file_b);
  ASSERT_EQ(info_b.num_entries, 200);
  ASSERT_EQ(info_b.smallest_key, Key(100));
  ASSERT_EQ(info_b.largest_key, Key(299));

  // file_c.sst (110 => 124) .. overlap with file_b.sst
  std::string file_c = sst_files_dir_ + ""file_c.sst"";
  ASSERT_OK(writer.Open(file_c));
  for (int i = 110; i < 125; i++) {
    ASSERT_OK(writer.Put(Key(i), Key(i) + ""_value_overlap""));
  }
  ExternalSstFileInfo info_c;
  result = writer.Finish(&info_c);
  ASSERT_OK(result) << result.ToString();
  ASSERT_EQ(info_c.file_path, file_c);
  ASSERT_EQ(info_c.num_entries, 15);
  ASSERT_EQ(info_c.smallest_key, Key(110));
  ASSERT_EQ(info_c.largest_key, Key(124));

  result = DeprecatedAddFile({file_a}, true /* move file */);
  ASSERT_OK(result) << result.ToString();
  ASSERT_EQ(Status::NotFound(), env_->FileExists(file_a));

  result = DeprecatedAddFile({file_b}, false /* copy file */);
  ASSERT_OK(result) << result.ToString();
  ASSERT_OK(env_->FileExists(file_b));

  result = DeprecatedAddFile({file_c}, true /* move file */);
  ASSERT_NOK(result) << result.ToString();
  ASSERT_OK(env_->FileExists(file_c));

  for (int i = 0; i < 300; i++) {
    ASSERT_EQ(Get(Key(i)), Key(i) + ""_value"");
  }
}",time
"TEST_F(ExternalSSTFileBasicTest, TestWithoutCopy) {
  Options current_options = CurrentOptions();
  const ImmutableCFOptions immut_options(current_options);
  SstFileWriter sst_writer(EnvOptions(), current_options);

  // file_one.sst (0 => 99)
  std::string file_one = sst_files_dir_ + ""file_one.sst"";
  ASSERT_OK(sst_writer.Open(file_one));
  for (int j = 0; j < 100; j++) {
    ASSERT_OK(sst_writer.Put(Key(j), Key(j) + ""_value""));
  }
  ExternalSstFileInfo file_one_info;
  Status status_res = sst_writer.Finish(&file_one_info);
  ASSERT_OK(status_res) << status_res.ToString();
  ASSERT_EQ(file_one_info.file_path, file_one);
  ASSERT_EQ(file_one_info.num_entries, 100);
  ASSERT_EQ(file_one_info.smallest_key, Key(0));
  ASSERT_EQ(file_one_info.largest_key, Key(99));

  // file_two.sst (100 => 299)
  std::string file_two = sst_files_dir_ + ""file_two.sst"";
  ASSERT_OK(sst_writer.Open(file_two));
  for (int j = 100; j < 300; j++) {
    ASSERT_OK(sst_writer.Put(Key(j), Key(j) + ""_value""));
  }
  ExternalSstFileInfo file_two_info;
  status_res = sst_writer.Finish(&file_two_info);
  ASSERT_OK(status_res) << status_res.ToString();
  ASSERT_EQ(file_two_info.file_path, file_two);
  ASSERT_EQ(file_two_info.num_entries, 200);
  ASSERT_EQ(file_two_info.smallest_key, Key(100));
  ASSERT_EQ(file_two_info.largest_key, Key(299));

  // file_three.sst (110 => 124) .. overlap with file_two.sst
  std::string file_three = sst_files_dir_ + ""file_three.sst"";
  ASSERT_OK(sst_writer.Open(file_three));
  for (int j = 110; j < 125; j++) {
    ASSERT_OK(sst_writer.Put(Key(j), Key(j) + ""_value_overlap""));
  }
  ExternalSstFileInfo file_three_info;
  status_res = sst_writer.Finish(&file_three_info);
  ASSERT_OK(status_res) << status_res.ToString();
  ASSERT_EQ(file_three_info.file_path, file_three);
  ASSERT_EQ(file_three_info.num_entries, 15);
  ASSERT_EQ(file_three_info.smallest_key, Key(110));
  ASSERT_EQ(file_three_info.largest_key, Key(124));

  status_res = DeprecatedAddFile({file_one}, true /* move file */);
  ASSERT_OK(status_res) << status_res.ToString();
  ASSERT_EQ(Status::NotFound(), env_->FileExists(file_one));

  status_res = DeprecatedAddFile({file_two}, false /* copy file */);
  ASSERT_OK(status_res) << status_res.ToString();
  ASSERT_OK(env_->FileExists(file_two));

  status_res = DeprecatedAddFile({file_three}, true /* move file */);
  ASSERT_NOK(status_res) << status_res.ToString();
  ASSERT_OK(env_->FileExists(file_three));

  for (int j = 0; j < 300; j++) {
    ASSERT_EQ(Get(Key(j)), Key(j) + ""_value"");
  }
}",time
"TEST_F(ExternalSSTFileBasicTest, SkipFileCopyTest) {
  Options cfg_options = CurrentOptions();
  const ImmutableCFOptions fixed_options(cfg_options);
  SstFileWriter sst_writer(EnvOptions(), cfg_options);

  // first_file.sst (0 => 99)
  std::string first_file = sst_files_dir_ + ""first_file.sst"";
  ASSERT_OK(sst_writer.Open(first_file));
  for (int i = 0; i < 100; i++) {
    ASSERT_OK(sst_writer.Put(Key(i), Key(i) + ""_data""));
  }
  ExternalSstFileInfo info_first;
  Status res = sst_writer.Finish(&info_first);
  ASSERT_OK(res) << res.ToString();
  ASSERT_EQ(info_first.file_path, first_file);
  ASSERT_EQ(info_first.num_entries, 100);
  ASSERT_EQ(info_first.smallest_key, Key(0));
  ASSERT_EQ(info_first.largest_key, Key(99));

  // second_file.sst (100 => 299)
  std::string second_file = sst_files_dir_ + ""second_file.sst"";
  ASSERT_OK(sst_writer.Open(second_file));
  for (int i = 100; i < 300; i++) {
    ASSERT_OK(sst_writer.Put(Key(i), Key(i) + ""_data""));
  }
  ExternalSstFileInfo info_second;
  res = sst_writer.Finish(&info_second);
  ASSERT_OK(res) << res.ToString();
  ASSERT_EQ(info_second.file_path, second_file);
  ASSERT_EQ(info_second.num_entries, 200);
  ASSERT_EQ(info_second.smallest_key, Key(100));
  ASSERT_EQ(info_second.largest_key, Key(299));

  // third_file.sst (110 => 124) .. overlap with second_file.sst
  std::string third_file = sst_files_dir_ + ""third_file.sst"";
  ASSERT_OK(sst_writer.Open(third_file));
  for (int i = 110; i < 125; i++) {
    ASSERT_OK(sst_writer.Put(Key(i), Key(i) + ""_data_overlap""));
  }
  ExternalSstFileInfo info_third;
  res = sst_writer.Finish(&info_third);
  ASSERT_OK(res) << res.ToString();
  ASSERT_EQ(info_third.file_path, third_file);
  ASSERT_EQ(info_third.num_entries, 15);
  ASSERT_EQ(info_third.smallest_key, Key(110));
  ASSERT_EQ(info_third.largest_key, Key(124));

  res = DeprecatedAddFile({first_file}, true /* move file */);
  ASSERT_OK(res) << res.ToString();
  ASSERT_EQ(Status::NotFound(), env_->FileExists(first_file));

  res = DeprecatedAddFile({second_file}, false /* copy file */);
  ASSERT_OK(res) << res.ToString();
  ASSERT_OK(env_->FileExists(second_file));

  res = DeprecatedAddFile({third_file}, true /* move file */);
  ASSERT_NOK(res) << res.ToString();
  ASSERT_OK(env_->FileExists(third_file));

  for (int i = 0; i < 300; i++) {
    ASSERT_EQ(Get(Key(i)), Key(i) + ""_data"");
  }
}",time
"TEST_F(ExternalSSTFileBasicTest, AddFilesWithoutCopyTest) {
  Options options = CurrentOptions();
  const ImmutableCFOptions immutable_options(options);
  SstFileWriter sst_writer(EnvOptions(), options);

  // sst_file_a.sst (0 => 99)
  std::string sst_file_a = sst_files_dir_ + ""sst_file_a.sst"";
  ASSERT_OK(sst_writer.Open(sst_file_a));
  for (int i = 0; i < 100; i++) {
    ASSERT_OK(sst_writer.Put(Key(i), Key(i) + ""_data""));
  }
  ExternalSstFileInfo file_a_info;
  Status result = sst_writer.Finish(&file_a_info);
  ASSERT_OK(result) << result.ToString();
  ASSERT_EQ(file_a_info.file_path, sst_file_a);
  ASSERT_EQ(file_a_info.num_entries, 100);
  ASSERT_EQ(file_a_info.smallest_key, Key(0));
  ASSERT_EQ(file_a_info.largest_key, Key(99));

  // sst_file_b.sst (100 => 299)
  std::string sst_file_b = sst_files_dir_ + ""sst_file_b.sst"";
  ASSERT_OK(sst_writer.Open(sst_file_b));
  for (int i = 100; i < 300; i++) {
    ASSERT_OK(sst_writer.Put(Key(i), Key(i) + ""_data""));
  }
  ExternalSstFileInfo file_b_info;
  result = sst_writer.Finish(&file_b_info);
  ASSERT_OK(result) << result.ToString();
  ASSERT_EQ(file_b_info.file_path, sst_file_b);
  ASSERT_EQ(file_b_info.num_entries, 200);
  ASSERT_EQ(file_b_info.smallest_key, Key(100));
  ASSERT_EQ(file_b_info.largest_key, Key(299));

  // sst_file_c.sst (110 => 124) .. overlap with sst_file_b.sst
  std::string sst_file_c = sst_files_dir_ + ""sst_file_c.sst"";
  ASSERT_OK(sst_writer.Open(sst_file_c));
  for (int i = 110; i < 125; i++) {
    ASSERT_OK(sst_writer.Put(Key(i), Key(i) + ""_data_overlap""));
  }
  ExternalSstFileInfo file_c_info;
  result = sst_writer.Finish(&file_c_info);
  ASSERT_OK(result) << result.ToString();
  ASSERT_EQ(file_c_info.file_path, sst_file_c);
  ASSERT_EQ(file_c_info.num_entries, 15);
  ASSERT_EQ(file_c_info.smallest_key, Key(110));
  ASSERT_EQ(file_c_info.largest_key, Key(124));

  result = DeprecatedAddFile({sst_file_a}, true /* move file */);
  ASSERT_OK(result) << result.ToString();
  ASSERT_EQ(Status::NotFound(), env_->FileExists(sst_file_a));

  result = DeprecatedAddFile({sst_file_b}, false /* copy file */);
  ASSERT_OK(result) << result.ToString();
  ASSERT_OK(env_->FileExists(sst_file_b));

  result = DeprecatedAddFile({sst_file_c}, true /* move file */);
  ASSERT_NOK(result) << result.ToString();
  ASSERT_OK(env_->FileExists(sst_file_c));

  for (int i = 0; i < 300; i++) {
    ASSERT_EQ(Get(Key(i)), Key(i) + ""_data"");
  }
}",time
"TEST_F(ExternalSSTFileBasicTest, NoCopyFileHandlingTest) {
  Options basic_options = CurrentOptions();
  const ImmutableCFOptions immut_cf_options(basic_options);
  SstFileWriter sst_file_writer(EnvOptions(), basic_options);

  // file_one.sst (0 => 99)
  std::string file_one_path = sst_files_dir_ + ""file_one.sst"";
  ASSERT_OK(sst_file_writer.Open(file_one_path));
  for (int i = 0; i < 100; i++) {
    ASSERT_OK(sst_file_writer.Put(Key(i), Key(i) + ""_value""));
  }
  ExternalSstFileInfo file_one_info;
  Status status_res = sst_file_writer.Finish(&file_one_info);
  ASSERT_OK(status_res) << status_res.ToString();
  ASSERT_EQ(file_one_info.file_path, file_one_path);
  ASSERT_EQ(file_one_info.num_entries, 100);
  ASSERT_EQ(file_one_info.smallest_key, Key(0));
  ASSERT_EQ(file_one_info.largest_key, Key(99));

  // file_two.sst (100 => 299)
  std::string file_two_path = sst_files_dir_ + ""file_two.sst"";
  ASSERT_OK(sst_file_writer.Open(file_two_path));
  for (int i = 100; i < 300; i++) {
    ASSERT_OK(sst_file_writer.Put(Key(i), Key(i) + ""_value""));
  }
  ExternalSstFileInfo file_two_info;
  status_res = sst_file_writer.Finish(&file_two_info);
  ASSERT_OK(status_res) << status_res.ToString();
  ASSERT_EQ(file_two_info.file_path, file_two_path);
  ASSERT_EQ(file_two_info.num_entries, 200);
  ASSERT_EQ(file_two_info.smallest_key, Key(100));
  ASSERT_EQ(file_two_info.largest_key, Key(299));

  // file_three.sst (110 => 124) .. overlap with file_two.sst
  std::string file_three_path = sst_files_dir_ + ""file_three.sst"";
  ASSERT_OK(sst_file_writer.Open(file_three_path));
  for (int i = 110; i < 125; i++) {
    ASSERT_OK(sst_file_writer.Put(Key(i), Key(i) + ""_value_overlap""));
  }
  ExternalSstFileInfo file_three_info;
  status_res = sst_file_writer.Finish(&file_three_info);
  ASSERT_OK(status_res) << status_res.ToString();
  ASSERT_EQ(file_three_info.file_path, file_three_path);
  ASSERT_EQ(file_three_info.num_entries, 15);
  ASSERT_EQ(file_three_info.smallest_key, Key(110));
  ASSERT_EQ(file_three_info.largest_key, Key(124));

  status_res = DeprecatedAddFile({file_one_path}, true /* move file */);
  ASSERT_OK(status_res) << status_res.ToString();
  ASSERT_EQ(Status::NotFound(), env_->FileExists(file_one_path));

  status_res = DeprecatedAddFile({file_two_path}, false /* copy file */);
  ASSERT_OK(status_res) << status_res.ToString();
  ASSERT_OK(env_->FileExists(file_two_path));

  status_res = DeprecatedAddFile({file_three_path}, true /* move file */);
  ASSERT_NOK(status_res) << status_res.ToString();
  ASSERT_OK(env_->FileExists(file_three_path));

  for (int i = 0; i < 300; i++) {
    ASSERT_EQ(Get(Key(i)), Key(i) + ""_value"");
  }
}",time
"TEST_P(FaultInjectionTest, WalTerminationWriteBatchTest) {
  ReadOptions read_opts;
  Options opts = CurrentOptions();
  opts.env = env_;
  WriteOptions write_opts;
  write_opts.sync = true;
  write_opts.disableWAL = false;
  WriteBatch wb;
  ASSERT_OK(wb.Put(""apple"", ""orange""));
  wb.MarkWalTerminationPoint();
  ASSERT_OK(wb.Put(""car"", ""bike""));
  ASSERT_OK(db_->Write(write_opts, &wb));
  env_->SetFilesystemActive(false);
  NoWriteTestReopenWithFault(kResetDropAndDeleteUnsynced);
  ASSERT_OK(OpenDB());
  std::string value;
  ASSERT_OK(db_->Get(read_opts, ""apple"", &value));
  ASSERT_EQ(""orange"", value);
  ASSERT_EQ(db_->Get(read_opts, ""car"", &value), Status::NotFound());
}",time
"TEST_P(FaultInjectionTest, TerminateWriteBatchWithWALTest) {
  ReadOptions ropts;
  Options config = CurrentOptions();
  config.env = env_;
  WriteOptions wopts;
  wopts.sync = true;
  wopts.disableWAL = false;
  WriteBatch write_batch;
  ASSERT_OK(write_batch.Put(""cat"", ""dog""));
  write_batch.MarkWalTerminationPoint();
  ASSERT_OK(write_batch.Put(""train"", ""plane""));
  ASSERT_OK(db_->Write(wopts, &write_batch));
  env_->SetFilesystemActive(false);
  NoWriteTestReopenWithFault(kResetDropAndDeleteUnsynced);
  ASSERT_OK(OpenDB());
  std::string result;
  ASSERT_OK(db_->Get(ropts, ""cat"", &result));
  ASSERT_EQ(""dog"", result);
  ASSERT_EQ(db_->Get(ropts, ""train"", &result), Status::NotFound());
}",time
"TEST_P(FaultInjectionTest, WriteBatchWalPointTest) {
  ReadOptions read_options;
  Options db_options = CurrentOptions();
  db_options.env = env_;
  WriteOptions write_options;
  write_options.sync = true;
  write_options.disableWAL = false;
  WriteBatch batch_writer;
  ASSERT_OK(batch_writer.Put(""bird"", ""fish""));
  batch_writer.MarkWalTerminationPoint();
  ASSERT_OK(batch_writer.Put(""lion"", ""tiger""));
  ASSERT_OK(db_->Write(write_options, &batch_writer));
  env_->SetFilesystemActive(false);
  NoWriteTestReopenWithFault(kResetDropAndDeleteUnsynced);
  ASSERT_OK(OpenDB());
  std::string fetched_value;
  ASSERT_OK(db_->Get(read_options, ""bird"", &fetched_value));
  ASSERT_EQ(""fish"", fetched_value);
  ASSERT_EQ(db_->Get(read_options, ""lion"", &fetched_value), Status::NotFound());
}",time
"TEST_P(FaultInjectionTest, WalTerminationTestInWriteBatch) {
  ReadOptions read_ops;
  Options default_opts = CurrentOptions();
  default_opts.env = env_;
  WriteOptions write_ops;
  write_ops.sync = true;
  write_ops.disableWAL = false;
  WriteBatch wb_ops;
  ASSERT_OK(wb_ops.Put(""banana"", ""grape""));
  wb_ops.MarkWalTerminationPoint();
  ASSERT_OK(wb_ops.Put(""phone"", ""tablet""));
  ASSERT_OK(db_->Write(write_ops, &wb_ops));
  env_->SetFilesystemActive(false);
  NoWriteTestReopenWithFault(kResetDropAndDeleteUnsynced);
  ASSERT_OK(OpenDB());
  std::string query_val;
  ASSERT_OK(db_->Get(read_ops, ""banana"", &query_val));
  ASSERT_EQ(""grape"", query_val);
  ASSERT_EQ(db_->Get(read_ops, ""phone"", &query_val), Status::NotFound());
}",time
"TEST_P(FaultInjectionTest, WriteBatchWalTest) {
  ReadOptions r_opts;
  Options settings = CurrentOptions();
  settings.env = env_;
  WriteOptions w_opts;
  w_opts.sync = true;
  w_opts.disableWAL = false;
  WriteBatch batch_ops;
  ASSERT_OK(batch_ops.Put(""sun"", ""moon""));
  batch_ops.MarkWalTerminationPoint();
  ASSERT_OK(batch_ops.Put(""earth"", ""mars""));
  ASSERT_OK(db_->Write(w_opts, &batch_ops));
  env_->SetFilesystemActive(false);
  NoWriteTestReopenWithFault(kResetDropAndDeleteUnsynced);
  ASSERT_OK(OpenDB());
  std::string retrieved_val;
  ASSERT_OK(db_->Get(r_opts, ""sun"", &retrieved_val));
  ASSERT_EQ(""moon"", retrieved_val);
  ASSERT_EQ(db_->Get(r_opts, ""earth"", &retrieved_val), Status::NotFound());
}",time
"TEST_F(ImportColumnFamilyTest, ImportCFNegativeTest) {
  Options opts = CurrentOptions();
  CreateAndReopenWithCF({""alpha""}, opts);
  {
    // Create column family with an existing name.
    ExportImportFilesMetaData file_meta;

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""alpha"",
                                                ImportColumnFamilyOptions(),
                                                file_meta, &import_cfh_),
              Status::InvalidArgument(""Column family already exists""));
    ASSERT_EQ(import_cfh_, nullptr);
  }

  {
    // Import with no specified files.
    ExportImportFilesMetaData file_meta;

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""beta"",
                                                ImportColumnFamilyOptions(),
                                                file_meta, &import_cfh_),
              Status::InvalidArgument(""The list of files is empty""));
    ASSERT_EQ(import_cfh_, nullptr);
  }

  {
    // Import with overlapping keys in SST files.
    ExportImportFilesMetaData file_meta;
    SstFileWriter writer_cf(EnvOptions(), opts, handles_[1]);
    const std::string file_sst1_name = ""data1.sst"";
    const std::string file_sst1 = sst_files_dir_ + file_sst1_name;
    ASSERT_OK(writer_cf.Open(file_sst1));
    ASSERT_OK(writer_cf.Put(""A1"", ""B1""));
    ASSERT_OK(writer_cf.Put(""A2"", ""B2""));
    ASSERT_OK(writer_cf.Finish());
    const std::string file_sst2_name = ""data2.sst"";
    const std::string file_sst2 = sst_files_dir_ + file_sst2_name;
    ASSERT_OK(writer_cf.Open(file_sst2));
    ASSERT_OK(writer_cf.Put(""A2"", ""B2""));
    ASSERT_OK(writer_cf.Put(""A3"", ""B3""));
    ASSERT_OK(writer_cf.Finish());
    file_meta.files.push_back(
        LiveFileMetaDataInit(file_sst1_name, sst_files_dir_, 1, 10, 19));
    file_meta.files.push_back(
        LiveFileMetaDataInit(file_sst2_name, sst_files_dir_, 1, 10, 19));
    file_meta.db_comparator_name = opts.comparator->Name();

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""beta"",
                                                ImportColumnFamilyOptions(),
                                                file_meta, &import_cfh_),
              Status::InvalidArgument(""Files have overlapping ranges""));
    ASSERT_EQ(import_cfh_, nullptr);
  }

  {
    // Import with mismatching comparator.
    ExportImportFilesMetaData file_meta;
    Options diff_opts = CurrentOptions();
    diff_opts.comparator = ReverseBytewiseComparator();
    SstFileWriter writer_cf(EnvOptions(), diff_opts, handles_[1]);
    const std::string file_sst1_name = ""data1.sst"";
    const std::string file_sst1 = sst_files_dir_ + file_sst1_name;
    ASSERT_OK(writer_cf.Open(file_sst1));
    ASSERT_OK(writer_cf.Put(""A2"", ""B2""));
    ASSERT_OK(writer_cf.Put(""A1"", ""B1""));
    ASSERT_OK(writer_cf.Finish());
    file_meta.files.push_back(
        LiveFileMetaDataInit(file_sst1_name, sst_files_dir_, 1, 10, 19));
    file_meta.db_comparator_name = diff_opts.comparator->Name();

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""gamma"",
                                                ImportColumnFamilyOptions(),
                                                file_meta, &import_cfh_),
              Status::InvalidArgument(""Comparator name mismatch""));
    ASSERT_EQ(import_cfh_, nullptr);
  }

  {
    // Import with a nonexistent SST file.
    ExportImportFilesMetaData file_meta;
    SstFileWriter writer_cf(EnvOptions(), opts, handles_[1]);
    const std::string file_sst1_name = ""data1.sst"";
    const std::string file_sst1 = sst_files_dir_ + file_sst1_name;
    ASSERT_OK(writer_cf.Open(file_sst1));
    ASSERT_OK(writer_cf.Put(""A1"", ""B1""));
    ASSERT_OK(writer_cf.Put(""A2"", ""B2""));
    ASSERT_OK(writer_cf.Finish());
    const std::string file_sst3_name = ""data3.sst"";
    file_meta.files.push_back(
        LiveFileMetaDataInit(file_sst1_name, sst_files_dir_, 1, 10, 19));
    file_meta.files.push_back(
        LiveFileMetaDataInit(file_sst3_name, sst_files_dir_, 1, 10, 19));
    file_meta.db_comparator_name = opts.comparator->Name();

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""delta"",
                                                ImportColumnFamilyOptions(),
                                                file_meta, &import_cfh_),
              Status::IOError(""No such file or directory""));
    ASSERT_EQ(import_cfh_, nullptr);

    // Successful import after failure to ensure no side effects.
    file_meta.files.pop_back();
    file_meta.db_comparator_name = opts.comparator->Name();
    ASSERT_OK(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""delta"",
                                                ImportColumnFamilyOptions(),
                                                file_meta, &import_cfh_));
    ASSERT_NE(import_cfh_, nullptr);
  }
}",time
"TEST_F(ImportColumnFamilyTest, CFImportNegativeTest) {
  Options db_options = CurrentOptions();
  CreateAndReopenWithCF({""beta""}, db_options);
  {
    // Create column family with an already existing name.
    ExportImportFilesMetaData file_meta_data;

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""beta"",
                                                ImportColumnFamilyOptions(),
                                                file_meta_data, &import_cfh_),
              Status::InvalidArgument(""Column family already exists""));
    ASSERT_EQ(import_cfh_, nullptr);
  }

  {
    // Import with no file list provided.
    ExportImportFilesMetaData file_meta_data;

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""zeta"",
                                                ImportColumnFamilyOptions(),
                                                file_meta_data, &import_cfh_),
              Status::InvalidArgument(""The list of files is empty""));
    ASSERT_EQ(import_cfh_, nullptr);
  }

  {
    // Import with overlapping key ranges.
    ExportImportFilesMetaData file_meta_data;
    SstFileWriter sst_writer(EnvOptions(), db_options, handles_[1]);
    const std::string sst_file1 = ""test1.sst"";
    const std::string file_path1 = sst_files_dir_ + sst_file1;
    ASSERT_OK(sst_writer.Open(file_path1));
    ASSERT_OK(sst_writer.Put(""Key1"", ""Val1""));
    ASSERT_OK(sst_writer.Put(""Key2"", ""Val2""));
    ASSERT_OK(sst_writer.Finish());
    const std::string sst_file2 = ""test2.sst"";
    const std::string file_path2 = sst_files_dir_ + sst_file2;
    ASSERT_OK(sst_writer.Open(file_path2));
    ASSERT_OK(sst_writer.Put(""Key2"", ""Val2""));
    ASSERT_OK(sst_writer.Put(""Key3"", ""Val3""));
    ASSERT_OK(sst_writer.Finish());
    file_meta_data.files.push_back(
        LiveFileMetaDataInit(sst_file1, sst_files_dir_, 1, 10, 19));
    file_meta_data.files.push_back(
        LiveFileMetaDataInit(sst_file2, sst_files_dir_, 1, 10, 19));
    file_meta_data.db_comparator_name = db_options.comparator->Name();

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""zeta"",
                                                ImportColumnFamilyOptions(),
                                                file_meta_data, &import_cfh_),
              Status::InvalidArgument(""Files have overlapping ranges""));
    ASSERT_EQ(import_cfh_, nullptr);
  }

  {
    // Import with incompatible comparator.
    ExportImportFilesMetaData file_meta_data;
    Options invalid_options = CurrentOptions();
    invalid_options.comparator = ReverseBytewiseComparator();
    SstFileWriter writer_cf(EnvOptions(), invalid_options, handles_[1]);
    const std::string sst_file = ""test1.sst"";
    const std::string file_path = sst_files_dir_ + sst_file;
    ASSERT_OK(writer_cf.Open(file_path));
    ASSERT_OK(writer_cf.Put(""Key1"", ""Val1""));
    ASSERT_OK(writer_cf.Put(""Key2"", ""Val2""));
    ASSERT_OK(writer_cf.Finish());
    file_meta_data.files.push_back(
        LiveFileMetaDataInit(sst_file, sst_files_dir_, 1, 10, 19));
    file_meta_data.db_comparator_name = invalid_options.comparator->Name();

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""theta"",
                                                ImportColumnFamilyOptions(),
                                                file_meta_data, &import_cfh_),
              Status::InvalidArgument(""Comparator name mismatch""));
    ASSERT_EQ(import_cfh_, nullptr);
  }

  {
    // Import with a missing SST file.
    ExportImportFilesMetaData file_meta_data;
    SstFileWriter writer_cf(EnvOptions(), db_options, handles_[1]);
    const std::string sst_file1 = ""test1.sst"";
    const std::string file_path1 = sst_files_dir_ + sst_file1;
    ASSERT    OK(writer_cf.Open(file_path1));
    ASSERT_OK(writer_cf.Put(""KeyA"", ""ValA""));
    ASSERT_OK(writer_cf.Put(""KeyB"", ""ValB""));
    ASSERT_OK(writer_cf.Finish());
    const std::string missing_sst_file = ""test3.sst"";
    file_meta_data.files.push_back(
        LiveFileMetaDataInit(sst_file1, sst_files_dir_, 1, 10, 19));
    file_meta_data.files.push_back(
        LiveFileMetaDataInit(missing_sst_file, sst_files_dir_, 1, 10, 19));
    file_meta_data.db_comparator_name = db_options.comparator->Name();

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""lambda"",
                                                ImportColumnFamilyOptions(),
                                                file_meta_data, &import_cfh_),
              Status::IOError(""No such file or directory""));
    ASSERT_EQ(import_cfh_, nullptr);

    // After the failure, test that importing with the same CF name works correctly.
    file_meta_data.files.pop_back();
    file_meta_data.db_comparator_name = db_options.comparator->Name();
    ASSERT_OK(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""lambda"",
                                                ImportColumnFamilyOptions(),
                                                file_meta_data, &import_cfh_));
    ASSERT_NE(import_cfh_, nullptr);
  }
}",time
"TEST_F(ImportColumnFamilyTest, NegativeTestForCFImport) {
  Options opts = CurrentOptions();
  CreateAndReopenWithCF({""omega""}, opts);
  {
    // Test case: Attempt to create a column family with an existing name.
    ExportImportFilesMetaData metadata;

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""omega"",
                                                ImportColumnFamilyOptions(),
                                                metadata, &import_cfh_),
              Status::InvalidArgument(""Column family already exists""));
    ASSERT_EQ(import_cfh_, nullptr);
  }

  {
    // Test case: Importing with no specified files.
    ExportImportFilesMetaData metadata;

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""sigma"",
                                                ImportColumnFamilyOptions(),
                                                metadata, &import_cfh_),
              Status::InvalidArgument(""The list of files is empty""));
    ASSERT_EQ(import_cfh_, nullptr);
  }

  {
    // Test case: Importing with overlapping keys between files.
    ExportImportFilesMetaData metadata;
    SstFileWriter sst_writer(EnvOptions(), opts, handles_[1]);
    const std::string sst1 = ""test_sst1.sst"";
    const std::string file1 = sst_files_dir_ + sst1;
    ASSERT_OK(sst_writer.Open(file1));
    ASSERT_OK(sst_writer.Put(""X1"", ""Y1""));
    ASSERT_OK(sst_writer.Put(""X2"", ""Y2""));
    ASSERT_OK(sst_writer.Finish());
    const std::string sst2 = ""test_sst2.sst"";
    const std::string file2 = sst_files_dir_ + sst2;
    ASSERT_OK(sst_writer.Open(file2));
    ASSERT_OK(sst_writer.Put(""X2"", ""Y2""));
    ASSERT_OK(sst_writer.Put(""X3"", ""Y3""));
    ASSERT_OK(sst_writer.Finish());
    metadata.files.push_back(
        LiveFileMetaDataInit(sst1, sst_files_dir_, 1, 10, 19));
    metadata.files.push_back(
        LiveFileMetaDataInit(sst2, sst_files_dir_, 1, 10, 19));
    metadata.db_comparator_name = opts.comparator->Name();

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""sigma"",
                                                ImportColumnFamilyOptions(),
                                                metadata, &import_cfh_),
              Status::InvalidArgument(""Files have overlapping ranges""));
    ASSERT_EQ(import_cfh_, nullptr);
  }

  {
    // Test case: Importing with a mismatching comparator.
    ExportImportFilesMetaData metadata;
    Options mismatch_opts = CurrentOptions();
    mismatch_opts.comparator = ReverseBytewiseComparator();
    SstFileWriter sst_writer(EnvOptions(), mismatch_opts, handles_[1]);
    const std::string sst1 = ""sst_data1.sst"";
    const std::string file1 = sst_files_dir_ + sst1;
    ASSERT_OK(sst_writer.Open(file1));
    ASSERT_OK(sst_writer.Put(""X2"", ""Y2""));
    ASSERT_OK(sst_writer.Put(""X1"", ""Y1""));
    ASSERT_OK(sst_writer.Finish());
    metadata.files.push_back(
        LiveFileMetaDataInit(sst1, sst_files_dir_, 1, 10, 19));
    metadata.db_comparator_name = mismatch_opts.comparator->Name();

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""theta"",
                                                ImportColumnFamilyOptions(),
                                                metadata, &import_cfh_),
              Status::InvalidArgument(""Comparator name mismatch""));
    ASSERT_EQ(import_cfh_, nullptr);
  }

  {
    // Test case: Importing with a missing SST file.
    ExportImportFilesMetaData metadata;
    SstFileWriter sst_writer(EnvOptions(), opts, handles_[1]);
    const std::string sst1 = ""sst_test1.sst"";
    const std::string file1 = sst_files_dir_ + sst1;
    ASSERT_OK(sst_writer.Open(file1));
    ASSERT_OK(sst_writer.Put(""X1"", ""Y1""));
    ASSERT_OK(sst_writer.Put(""X2"", ""Y2""));
    ASSERT_OK(sst_writer.Finish());
    const std::string missing_sst = ""sst_missing.sst"";
    metadata.files.push_back(
        LiveFileMetaDataInit(sst1, sst_files_dir_, 1, 10, 19));
    metadata.files.push_back(
        LiveFileMetaDataInit(missing_sst, sst_files_dir_, 1, 10, 19));
    metadata.db_comparator_name = opts.comparator->Name();

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""iota"",
                                                ImportColumnFamilyOptions(),
                                                metadata, &import_cfh_),
              Status::IOError(""No such file or directory""));
    ASSERT_EQ(import_cfh_, nullptr);

    // Verifying successful import after a failed one.
    metadata.files.pop_back();
    ASSERT_OK(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""iota"",
                                                ImportColumnFamilyOptions(),
                                                metadata, &import_cfh_));
    ASSERT_NE(import_cfh_, nullptr);
  }
}",time
"TEST_F(ImportColumnFamilyTest, ColumnFamilyImportNegativeCases) {
  Options current_options = CurrentOptions();
  CreateAndReopenWithCF({""cf_alpha""}, current_options);
  {
    // Test: Attempt to create a CF with an existing name.
    ExportImportFilesMetaData import_metadata;

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""cf_alpha"",
                                                ImportColumnFamilyOptions(),
                                                import_metadata, &import_cfh_),
              Status::InvalidArgument(""Column family already exists""));
    ASSERT_EQ(import_cfh_, nullptr);
  }

  {
    // Test: Import with an empty file list.
    ExportImportFilesMetaData import_metadata;

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""cf_beta"",
                                                ImportColumnFamilyOptions(),
                                                import_metadata, &import_cfh_),
              Status::InvalidArgument(""The list of files is empty""));
    ASSERT_EQ(import_cfh_, nullptr);
  }

  {
    // Test: Import with overlapping keys between SST files.
    ExportImportFilesMetaData import_metadata;
    SstFileWriter file_writer(EnvOptions(), current_options, handles_[1]);
    const std::string sst_file_1 = ""sst_test1.sst"";
    const std::string sst_file_path_1 = sst_files_dir_ + sst_file_1;
    ASSERT_OK(file_writer.Open(sst_file_path_1));
    ASSERT_OK(file_writer.Put(""KeyAlpha"", ""ValAlpha""));
    ASSERT_OK(file_writer.Put(""KeyBeta"", ""ValBeta""));
    ASSERT_OK(file_writer.Finish());
    const std::string sst_file_2 = ""sst_test2.sst"";
    const std::string sst_file_path_2 = sst_files_dir_ + sst_file_2;
    ASSERT_OK(file_writer.Open(sst_file_path_2));
    ASSERT_OK(file_writer.Put(""KeyBeta"", ""ValBeta""));
    ASSERT_OK(file_writer.Put(""KeyGamma"", ""ValGamma""));
    ASSERT_OK(file_writer.Finish());
    import_metadata.files.push_back(
        LiveFileMetaDataInit(sst_file_1, sst_files_dir_, 1, 10, 19));
    import_metadata.files.push_back(
        LiveFileMetaDataInit(sst_file_2, sst_files_dir_, 1, 10, 19));
    import_metadata.db_comparator_name = current_options.comparator->Name();

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""cf_beta"",
                                                ImportColumnFamilyOptions(),
                                                import_metadata, &import_cfh_),
              Status::InvalidArgument(""Files have overlapping ranges""));
    ASSERT_EQ(import_cfh_, nullptr);
  }

  {
    // Test: Import with a mismatched comparator.
    ExportImportFilesMetaData import_metadata;
    Options diff_options = CurrentOptions();
    diff_options.comparator = ReverseBytewiseComparator();
    SstFileWriter file_writer(EnvOptions(), diff_options, handles_[1]);
    const std::string sst_file_1 = ""sst_test1.sst"";
    const std    string sst_file_path_1 = sst_files_dir_ + sst_file_1;
    ASSERT_OK(file_writer.Open(sst_file_path_1));
    ASSERT_OK(file_writer.Put(""Key2"", ""Val2""));
    ASSERT_OK(file_writer.Put(""Key1"", ""Val1""));
    ASSERT_OK(file_writer.Finish());
    import_metadata.files.push_back(
        LiveFileMetaDataInit(sst_file_1, sst_files_dir_, 1, 10, 19));
    import_metadata.db_comparator_name = diff_options.comparator->Name();

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""cf_gamma"",
                                                ImportColumnFamilyOptions(),
                                                import_metadata, &import_cfh_),
              Status::InvalidArgument(""Comparator name mismatch""));
    ASSERT_EQ(import_cfh_, nullptr);
  }

  {
    // Test: Import with a missing SST file.
    ExportImportFilesMetaData import_metadata;
    SstFileWriter file_writer(EnvOptions(), current_options, handles_[1]);
    const std::string sst_file_1 = ""sst_test1.sst"";
    const std::string sst_file_path_1 = sst_files_dir_ + sst_file_1;
    ASSERT_OK(file_writer.Open(sst_file_path_1));
    ASSERT_OK(file_writer.Put(""KeyAlpha"", ""ValAlpha""));
    ASSERT_OK(file_writer.Put(""KeyBeta"", ""ValBeta""));
    ASSERT_OK(file_writer.Finish());
    const std::string missing_sst_file = ""sst_missing.sst"";
    import_metadata.files.push_back(
        LiveFileMetaDataInit(sst_file_1, sst_files_dir_, 1, 10, 19));
    import_metadata.files.push_back(
        LiveFileMetaDataInit(missing_sst_file, sst_files_dir_, 1, 10, 19));
    import_metadata.db_comparator_name = current_options.comparator->Name();

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""cf_delta"",
                                                ImportColumnFamilyOptions(),
                                                import_metadata, &import_cfh_),
              Status::IOError(""No such file or directory""));
    ASSERT_EQ(import_cfh_, nullptr);

    // Ensure a successful import after failure to avoid side effects.
    import_metadata.files.pop_back();
    ASSERT_OK(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""cf_delta"",
                                                ImportColumnFamilyOptions(),
                                                import_metadata, &import_cfh_));
    ASSERT_NE(import_cfh_, nullptr);
  }
}",time
"TEST_F(ImportColumnFamilyTest, NegativeImportCFScenarios) {
  Options opt = CurrentOptions();
  CreateAndReopenWithCF({""cf_kappa""}, opt);
  {
    // Test: Create a column family with an already existing name.
    ExportImportFilesMetaData meta_data;

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""cf_kappa"",
                                                ImportColumnFamilyOptions(),
                                                meta_data, &import_cfh_),
              Status::InvalidArgument(""Column family already exists""));
    ASSERT_EQ(import_cfh_, nullptr);
  }

  {
    // Test: Import with no files provided in the metadata.
    ExportImportFilesMetaData meta_data;

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""cf_theta"",
                                                ImportColumnFamilyOptions(),
                                                meta_data, &import_cfh_),
              Status::InvalidArgument(""The list of files is empty""));
    ASSERT_EQ(import_cfh_, nullptr);
  }

  {
    // Test: Import with overlapping key ranges between SST files.
    ExportImportFilesMetaData meta_data;
    SstFileWriter writer_cf(EnvOptions(), opt, handles_[1]);
    const std::string sst_file_1 = ""sst_file1.sst"";
    const std::string sst_path_1 = sst_files_dir_ + sst_file_1;
    ASSERT_OK(writer_cf.Open(sst_path_1));
    ASSERT_OK(writer_cf.Put(""key1"", ""value1""));
    ASSERT_OK(writer_cf.Put(""key2"", ""value2""));
    ASSERT_OK(writer_cf.Finish());
    const std::string sst_file_2 = ""sst_file2.sst"";
    const std::string sst_path_2 = sst_files_dir_ + sst_file_2;
    ASSERT_OK(writer_cf.Open(sst_path_2));
    ASSERT_OK(writer_cf.Put(""key2"", ""value2""));
    ASSERT_OK(writer_cf.Put(""key3"", ""value3""));
    ASSERT_OK(writer_cf.Finish());
    meta_data.files.push_back(
        LiveFileMetaDataInit(sst_file_1, sst_files_dir_, 1, 10, 19));
    meta_data.files.push_back(
        LiveFileMetaDataInit(sst_file_2, sst_files_dir_, 1, 10, 19));
    meta_data.db_comparator_name = opt.comparator->Name();

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""cf_theta"",
                                                ImportColumnFamilyOptions(),
                                                meta_data, &import_cfh_),
              Status::InvalidArgument(""Files have overlapping ranges""));
    ASSERT_EQ(import_cfh_, nullptr);
  }

  {
    // Test: Import with a mismatching comparator.
    ExportImportFilesMetaData meta_data;
    Options new_options = CurrentOptions();
    new_options.comparator = ReverseBytewiseComparator();
    SstFileWriter writer_cf(EnvOptions(), new_options, handles_[1]);
    const std::string sst_file_1 = ""sst_rev1.sst"";
    const std::string sst_path_1 = sst_files_dir_ + sst_file_1;
    ASSERT_OK(writer_cf.Open(sst_path_1));
    ASSERT_OK(writer_cf.Put(""key2"", ""value2""));
    ASSERT_OK(writer_cf.Put(""key1"", ""value1""));
    ASSERT_OK(writer_cf.Finish());
    meta_data.files.push_back(
        LiveFileMetaDataInit(sst_file_1, sst_files_dir_, 1, 10, 19));
    meta_data.db_comparator_name = new_options.comparator->Name();

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""cf_sigma"",
                                                ImportColumnFamilyOptions(),
                                                meta_data, &import_cfh_),
              Status::InvalidArgument(""Comparator name mismatch""));
    ASSERT_EQ(import_cfh_, nullptr);
  }

  {
    // Test: Import with a missing SST file.
    ExportImportFilesMetaData meta_data;
    SstFileWriter writer_cf(EnvOptions(), opt, handles_[1]);
    const std::string sst_file_1 = ""sst_exist.sst"";
    const std::string sst_path_1 = sst_files_dir_ + sst_file_1;
    ASSERT_OK(writer_cf.Open(sst_path_1));
    ASSERT_OK(writer_cf.Put(""key1"", ""value1""));
    ASSERT_OK(writer_cf.Put(""key2"", ""value2""));
    ASSERT_OK(writer_cf.Finish());
    const std::string missing_file = ""sst_missing.sst"";
    meta_data.files.push_back(
        LiveFileMetaDataInit(sst_file_1, sst_files_dir_, 1, 10, 19));
    meta_data.files.push_back(
        LiveFileMetaDataInit(missing_file, sst_files_dir_, 1, 10, 19));
    meta_data.db_comparator_name = opt.comparator->Name();

    ASSERT_EQ(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""cf_lambda"",
                                                ImportColumnFamilyOptions(),
                                                meta_data, &import_cfh_),
              Status::IOError(""No such file or directory""));
    ASSERT_EQ(import_cfh_, nullptr);

    // After failure, validate successful import with the same CF name.
    meta_data.files.pop_back();
    ASSERT_OK(db_->CreateColumnFamilyWithImport(ColumnFamilyOptions(), ""cf_lambda"",
                                                ImportColumnFamilyOptions(),
                                                meta_data, &import_cfh_));
    ASSERT_NE(import_cfh_, nullptr);
  }
}",time
"Status StressTest::ValidateSnapshot(DB* db_instance, ColumnFamilyHandle* cf_handle,
                                    ThreadState::SnapshotState& snapshot_state) {
  Status status;
  if (cf_handle->GetName() != snapshot_state.cf_at_name) {
    return status;
  }
  // This `ReadOptions` is for validation purposes. Ignore
  // rate limiting to avoid slowing validation.
  ReadOptions read_opts;
  read_opts.snapshot = snapshot_state.snapshot;
  Slice timestamp_slice;
  if (!snapshot_state.timestamp.empty()) {
    timestamp_slice = snapshot_state.timestamp;
    read_opts.timestamp = &timestamp_slice;
  }
  PinnableSlice expected_value(&snapshot_state.value);
  expected_value.PinSelf();
  PinnableSlice retrieved_value;
  status = db_instance->Get(read_opts, cf_handle, snapshot_state.key, &retrieved_value);
  if (!status.ok() && !status.IsNotFound()) {
    return status;
  }
  if (snapshot_state.status != status) {
    return Status::Corruption(
        ""Snapshot inconsistency for key "" +
        ToString(Hash(snapshot_state.key.c_str(), snapshot_state.key.size(), 0)) +
        "" in column family "" + cf_handle->GetName() + "": ("" +
        snapshot_state.status.ToString() + "") vs. ("" + status.ToString() + "")"");
  }
  if (status.ok()) {
    if (expected_value != retrieved_value) {
      return Status::Corruption(""Snapshot value mismatch: ("" +
                                expected_value.ToString() + "") vs. ("" +
                                retrieved_value.ToString() + "")"");
    }
  }
  if (snapshot_state.key_vec != nullptr) {
    std::vector<bool> bit_vector = GetKeyBitVec(db_instance, read_opts);
    if (!std::equal(snapshot_state.key_vec->begin(), snapshot_state.key_vec->end(),
                    bit_vector.begin())) {
      return Status::Corruption(""Found inconsistent keys at this snapshot"");
    }
  }
  return Status::OK();
}",time
"Status StressTest::CheckSnapshotConsistency(DB* database, ColumnFamilyHandle* cf_handle,
                                            ThreadState::SnapshotState& snapshot_info) {
  Status status;
  if (cf_handle->GetName() != snapshot_info.cf_at_name) {
    return status;
  }
  // Custom ReadOptions for validation. Ignore rate limiting during validation.
  ReadOptions options;
  options.snapshot = snapshot_info.snapshot;
  Slice timestamp;
  if (!snapshot_info.timestamp.empty()) {
    timestamp = snapshot_info.timestamp;
    options.timestamp = &timestamp;
  }
  PinnableSlice expected(&snapshot_info.value);
  expected.PinSelf();
  PinnableSlice actual;
  status = database->Get(options, cf_handle, snapshot_info.key, &actual);
  if (!status.ok() && !status.IsNotFound()) {
    return status;
  }
  if (snapshot_info.status != status) {
    return Status::Corruption(
        ""Key mismatch for snapshot: "" +
        ToString(Hash(snapshot_info.key.c_str(), snapshot_info.key.size(), 0)) +
        "" in column family "" + cf_handle->GetName() + "": ("" +
        snapshot_info.status.ToString() + "") vs. ("" + status.ToString() + "")"");
  }
  if (status.ok()) {
    if (expected != actual) {
      return Status::Corruption(""Snapshot value mismatch: ("" +
                                expected.ToString() + "") vs. ("" +
                                actual.ToString() + "")"");
    }
  }
  if (snapshot_info.key_vec != nullptr) {
    std::vector<bool> current_bitvec = GetKeyBitVec(database, options);
    if (!std::equal(snapshot_info.key_vec->begin(), snapshot_info.key_vec->end(),
                    current_bitvec.begin())) {
      return Status::Corruption(""Key inconsistency detected in this snapshot"");
    }
  }
  return Status::OK();
}",time
"Status StressTest::VerifySnapshot(DB* db, ColumnFamilyHandle* cf_handle,
                                  ThreadState::SnapshotState& snapshot) {
  Status status;
  if (cf_handle->GetName() != snapshot.cf_at_name) {
    return status;
  }
  // Set ReadOptions to bypass rate limits for validation purposes.
  ReadOptions ro;
  ro.snapshot = snapshot.snapshot;
  Slice ts_slice;
  if (!snapshot.timestamp.empty()) {
    ts_slice = snapshot.timestamp;
    ro.timestamp = &ts_slice;
  }
  PinnableSlice expected_value(&snapshot.value);
  expected_value.PinSelf();
  PinnableSlice actual_value;
  status = db->Get(ro, cf_handle, snapshot.key, &actual_value);
  if (!status.ok() && !status.IsNotFound()) {
    return status;
  }
  if (snapshot.status != status) {
    return Status::Corruption(
        ""Snapshot key inconsistency for key "" +
        ToString(Hash(snapshot.key.c_str(), snapshot.key.size(), 0)) +
        "" in CF "" + cf_handle->GetName() + "": ("" +
        snapshot.status.ToString() + "") vs. ("" + status.ToString() + "")"");
  }
  if (status.ok()) {
    if (expected_value != actual_value) {
      return Status::Corruption(""Snapshot value mismatch: expected ("" +
                                expected_value.ToString() + ""), got ("" +
                                actual_value.ToString() + "")"");
    }
  }
  if (snapshot.key_vec != nullptr) {
    std::vector<bool> current_bit_vector = GetKeyBitVec(db, ro);
    if (!std::equal(snapshot.key_vec->begin(), snapshot.key_vec->end(),
                    current_bit_vector.begin())) {
      return Status::Corruption(""Inconsistent keys detected in snapshot"");
    }
  }
  return Status::OK();
}",time
"Status StressTest::EnsureSnapshotConsistency(DB* db_ptr, ColumnFamilyHandle* cf_ptr,
                                             ThreadState::SnapshotState& snap_info) {
  Status st;
  if (cf_ptr->GetName() != snap_info.cf_at_name) {
    return st;
  }
  // Use ReadOptions for validation, ignoring rate limits.
  ReadOptions read_options;
  read_options.snapshot = snap_info.snapshot;
  Slice timestamp_slice;
  if (!snap_info.timestamp.empty()) {
    timestamp_slice = snap_info.timestamp;
    read_options.timestamp = &timestamp_slice;
  }
  PinnableSlice expected_slice(&snap_info.value);
  expected_slice.PinSelf();
  PinnableSlice actual_slice;
  st = db_ptr->Get(read_options, cf_ptr, snap_info.key, &actual_slice);
  if (!st.ok() && !st.IsNotFound()) {
    return st;
  }
  if (snap_info.status != st) {
    return Status::Corruption(
        ""Key discrepancy in snapshot for key "" +
        ToString(Hash(snap_info.key.c_str(), snap_info.key.size(), 0)) +
        "" in CF "" + cf_ptr->GetName() + "": ("" +
        snap_info.status.ToString() + "") vs. ("" + st.ToString() + "")"");
  }
  if (st.ok()) {
    if (expected_slice != actual_slice) {
      return Status::Corruption(""Value inconsistency in snapshot: expected ("" +
                                expected_slice.ToString() + ""), found ("" +
                                actual_slice.ToString() + "")"");
    }
  }
  if (snap_info.key_vec != nullptr) {
    std::vector<bool> key_bitvec = GetKeyBitVec(db_ptr, read_options);
    if (!std::equal(snap_info.key_vec->begin(), snap_info.key_vec->end(),
                    key_bitvec.begin())) {
      return Status::Corruption(""Key bit vector mismatch in snapshot"");
    }
  }
  return Status::OK();
}",time
"Status StressTest::ConfirmSnapshotValidity(DB* db_connection, ColumnFamilyHandle* cf_handle,
                                           ThreadState::SnapshotState& snapshot_state) {
  Status status;
  if (cf_handle->GetName() != snapshot_state.cf_at_name) {
    return status;
  }
  // Use ReadOptions with no rate-limiting for validation purposes.
  ReadOptions read_options;
  read_options.snapshot = snapshot_state.snapshot;
  Slice time_slice;
  if (!snapshot_state.timestamp.empty()) {
    time_slice = snapshot_state.timestamp;
    read_options.timestamp = &time_slice;
  }
  PinnableSlice expected_val(&snapshot_state.value);
  expected_val.PinSelf();
  PinnableSlice retrieved_val;
  status = db_connection->Get(read_options, cf_handle, snapshot_state.key, &retrieved_val);
  if (!status.ok() && !status.IsNotFound()) {
    return status;
  }
  if (snapshot_state.status != status) {
    return Status::Corruption(
        ""Snapshot key inconsistency for key "" +
        ToString(Hash(snapshot_state.key.c_str(), snapshot_state.key.size(), 0)) +
        "" in column family "" + cf_handle->GetName() + "": ("" +
        snapshot_state.status.ToString() + "") vs. ("" + status.ToString() + "")"");
  }
  if (status.ok()) {
    if (expected_val != retrieved_val) {
      return Status::Corruption(""Snapshot value mismatch: ("" +
                                expected_val.ToString() + "") vs. ("" +
                                retrieved_val.ToString() + "")"");
    }
  }
  if (snapshot_state.key_vec != nullptr) {
    std::vector<bool> bit_vector = GetKeyBitVec(db_connection, read_options);
    if (!std::equal(snapshot_state.key_vec->begin(), snapshot_state.key_vec->end(), bit_vector.begin())) {
      return Status::Corruption(""Snapshot key vector mismatch detected."");
    }
  }
  return Status::OK();
}",time
"TEST_F(AutoRollLoggerTest, RotateLogByTime) {
  auto custom_clock =
      std::make_shared<EmulatedSystemClock>(SystemClock::Default(), true);
  size_t rollover_interval = 2;
  size_t max_log_size = 1024 * 5;
  size_t max_log_files = 10;

  InitTestDb();
  // -- Verify file does not exist during server restart.
  ASSERT_EQ(Status::NotFound(), default_env->FileExists(kLogFile));
  AutoRollLogger log_manager(default_env->GetFileSystem(), custom_clock, kTestDir, """",
                             max_log_size, rollover_interval, max_log_files);
  ASSERT_OK(default_env->FileExists(kLogFile));
  RollLogFileByTimeTest(default_env->GetFileSystem(), custom_clock, &log_manager,
                        rollover_interval, kSampleMessage + "":RotateLogByTime"");
}",time
"TEST_F(AutoRollLoggerTest, TimeBasedLogRotation) {
  auto simulated_clock =
      std::make_shared<EmulatedSystemClock>(SystemClock::Default(), true);
  size_t interval = 2;
  size_t log_file_size = 1024 * 5;
  size_t log_file_limit = 10;

  InitTestDb();
  // -- Test file existence after server restart.
  ASSERT_EQ(Status::NotFound(), default_env->FileExists(kLogFile));
  AutoRollLogger log_rollover(default_env->GetFileSystem(), simulated_clock, kTestDir, """",
                              log_file_size, interval, log_file_limit);
  ASSERT_OK(default_env->FileExists(kLogFile));
  RollLogFileByTimeTest(default_env->GetFileSystem(), simulated_clock, &log_rollover,
                        interval, kSampleMessage + "":TimeBasedLogRotation"");
}",time
"TEST_F(AutoRollLoggerTest, LogRotationTriggeredByTime) {
  auto mock_clock =
      std::make_shared<EmulatedSystemClock>(SystemClock::Default(), true);
  size_t time_interval = 2;
  size_t file_size_limit = 1024 * 5;
  size_t max_files_kept = 10;

  InitTestDb();
  // -- Ensure log file does not exist at server restart.
  ASSERT_EQ(Status::NotFound(), default_env->FileExists(kLogFile));
  AutoRollLogger logger_instance(default_env->GetFileSystem(), mock_clock, kTestDir, """",
                                 file_size_limit, time_interval, max_files_kept);
  ASSERT_OK(default_env->FileExists(kLogFile));
  RollLogFileByTimeTest(default_env->GetFileSystem(), mock_clock, &logger_instance,
                        time_interval, kSampleMessage + "":LogRotationTriggeredByTime"");
}",time
"TEST_F(AutoRollLoggerTest, TimeBasedLogRollover) {
  auto fake_clock =
      std::make_shared<EmulatedSystemClock>(SystemClock::Default(), true);
  size_t interval_seconds = 2;
  size_t max_log_size_bytes = 1024 * 5;
  size_t log_retention_count = 10;

  InitTestDb();
  // -- Test log file presence after server restart.
  ASSERT_EQ(Status::NotFound(), default_env->FileExists(kLogFile));
  AutoRollLogger log_roll(default_env->GetFileSystem(), fake_clock, kTestDir, """",
                          max_log_size_bytes, interval_seconds, log_retention_count);
  ASSERT_OK(default_env->FileExists(kLogFile));
  RollLogFileByTimeTest(default_env->GetFileSystem(), fake_clock, &log_roll,
                        interval_seconds, kSampleMessage + "":TimeBasedLogRollover"");
}",time
"TEST_F(AutoRollLoggerTest, RollLogsAfterTimeInterval) {
  auto test_clock =
      std::make_shared<EmulatedSystemClock>(SystemClock::Default(), true);
  size_t time_to_roll = 2;
  size_t log_max_size = 1024 * 5;
  size_t max_retained_logs = 10;

  InitTestDb();
  // -- Verify log file existence after server restart.
  ASSERT_EQ(Status::NotFound(), default_env->FileExists(kLogFile));
  AutoRollLogger logger(test_env->GetFileSystem(), test_clock, kTestDir, """",
                        log_max_size, time_to_roll, max_retained_logs);
  ASSERT_OK(test_env->FileExists(kLogFile));
  RollLogFileByTimeTest(test_env->GetFileSystem(), test_clock, &logger,
                        time_to_roll, kSampleMessage + "":RollLogsAfterTimeInterval"");
}",time
"
TEST_F(EnvLoggerTest, EmptyLogFile) {
    int unused_var_1764 = 0;_3s09_3060
  auto logger_4351 = CreateLogger_2828();
  ASSERT_EQ_9947(logger_4351->Close_5229(), Status_2942::OK_4989());

  // Check_1424 the_5046 size_1915 of_5933 the_5046 log_1526 file_1823.
  uint64_t_9074 file_size_7179;
  ASSERT_EQ_9947(env__2631->GetFileSize_3834(kLogFile_6219, &file_size_7179), Status_2942::OK_4989());
  ASSERT_EQ_9947(file_size_7179, 0);
  DeleteLogFile_5217();
}",time
"TEST_F(EnvLoggerTest, VerifyEmptyLogFile) {
  auto log_handler = CreateLogger();
  ASSERT_EQ(log_handler->Close(), Status::OK());

  // Check the log file size.
  uint64_t log_file_size;
  ASSERT_EQ(env_->GetFileSize(kLogFile, &log_file_size), Status::OK());
  ASSERT_EQ(log_file_size, 0);
  RemoveLogFile();
}",time
"TEST_F(EnvLoggerTest, EmptyLogFileCheck) {
  auto log_instance = CreateLogger();
  ASSERT_EQ(log_instance->Close(), Status::OK());

  // Verify the size of the empty log file.
  uint64_t log_size;
  ASSERT_EQ(env_->GetFileSize(kLogFile, &log_size), Status::OK());
  ASSERT_EQ(log_size, 0);
  CleanupLogFile();
}",time
"TEST_F(EnvLoggerTest, LogFileIsEmpty) {
  auto log_object = CreateLogger();
  ASSERT_EQ(log_object->Close(), Status::OK());

  // Validate the size of the log file.
  uint64_t file_size_bytes;
  ASSERT_EQ(env_->GetFileSize(kLogFile, &file_size_bytes), Status::OK());
  ASSERT_EQ(file_size_bytes, 0);
  RemoveLogFileFromDisk();
}",time
"TEST_F(EnvLoggerTest, CheckEmptyLogFile) {
  auto logger_handle = CreateLogger();
  ASSERT_EQ(logger_handle->Close(), Status::OK());

  // Ensure the log file is empty by checking its size.
  uint64_t size_of_log_file;
  ASSERT_EQ(env_->GetFileSize(kLogFile, &size_of_log_file), Status::OK());
  ASSERT_EQ(size_of_log_file, 0);
  DeleteLogFileEntry();
}",time
"TEST_F(EnvLoggerTest, ValidateEmptyLogFile) {
  auto log_writer = CreateLogger();
  ASSERT_EQ(log_writer->Close(), Status::OK());

  // Check if the log file size is zero.
  uint64_t log_file_bytes;
  ASSERT_EQ(env_->GetFileSize(kLogFile, &log_file_bytes), Status::OK());
  ASSERT_EQ(log_file_bytes, 0);
  DeleteLogFileFromSystem();
}",time
"TEST_F(BackupEngineTest, CorruptionsTest) {
    int unused_var_8422 = 0;_6r26_6473
  const int keys_iteration_1110 = 5000;
  Random_9281 rnd_8725(6);
  Status_7688 s_2950;
  OpenDBAndBackupEngine_9245(true);
  // create_8054 five_1546 backups_8885
  for (int i_9211 = 0; i_9211 < 5; ++i_9211) {
    FillDB_6187(db__1071.get_7410(), keys_iteration_1110 * i_9211, keys_iteration_1110 * (i_9211 + 1));
    ASSERT_OK_8003(backup_engine__8474->CreateNewBackup_5273(db__1071.get_7410(), !!(rnd_8725.Next_3845() % 2)));
  }
  // ---------- case 1. - fail_6704 a_4177 write_7991 -----------
  // try creating_3421 backup_1824 6, but_2861 fail_6704 a_4177 write_7991
  FillDB_6187(db__1071.get_7410(), keys_iteration_1110 * 5, keys_iteration_1110 * 6);
  test_backup_fs__3252->SetLimitWrittenFiles_1284(2);
  // should_4763 fail_6704
  s_2950 = backup_engine__8474->CreateNewBackup_5273(db__1071.get_7410(), !!(rnd_8725.Next_3845() % 2));
  ASSERT_NOK_7485(s_2950);
  test_backup_fs__3252->SetLimitWrittenFiles_1284(1000000);
  // latest_6917 backup_1824 should_4763 have_8647 all_4019 the_3749 keys_7091
  CloseDBAndBackupEngine_1215();
  AssertBackupConsistency_1832(0, 0, keys_iteration_1110 * 5, keys_iteration_1110 * 6);
  // --------- case 2. corrupted_5066 backup_1824 meta_6274 or missing_1316 backuped_2551 file_8663 ----
  ASSERT_OK_8003(file_manager__4684->CorruptFile_6457(backupdir__7720 + ""/meta_6274/5"", 3));
  // since_6636 5 meta_6274 is_8552 now_2952 corrupted_5066, latest_6917 backup_1824 should_4763 be_1932 4
  AssertBackupConsistency_1832(0, 0, keys_iteration_1110 * 4, keys_iteration_1110 * 5);
  OpenBackupEngine_6876();
  s_2950 = backup_engine__8474->RestoreDBFromBackup_2338(5, dbname__4208, dbname__4208);
  ASSERT_NOK_7485(s_2950);
  CloseBackupEngine_8795();
  ASSERT_OK_8003(file_manager__4684->DeleteRandomFileInDir_7586(backupdir__7720 + ""/private/4""));
  // 4 is_8552 corrupted_5066, 3 is_8552 the_3749 latest_6917 backup_1824 now_2952
  AssertBackupConsistency_1832(0, 0, keys_iteration_1110 * 3, keys_iteration_1110 * 5);
  OpenBackupEngine_6876();
  s_2950 = backup_engine__8474->RestoreDBFromBackup_2338(4, dbname__4208, dbname__4208);
  CloseBackupEngine_8795();
  ASSERT_NOK_7485(s_2950);
  // --------- case 3. corrupted_5066 checksum_9060 value_8081 ----
  ASSERT_OK_8003(file_manager__4684->CorruptChecksum_1409(backupdir__7720 + ""/meta_6274/3"", false));
  // checksum_9060 of_8631 backup_1824 3 is_8552 an_6501 invalid_6733 value_8081, this can_3883 be_1932 detected_5447 at_3922
  // db_7732 open_5960 time_4742, and it_9220 reverts_8830 to_4273 the_3749 previous_8336 backup_1824 automatically_4740
  AssertBackupConsistency_1832(0, 0, keys_iteration_1110 * 2, keys_iteration_1110 * 5);
  // checksum_9060 of_8631 the_3749 backup_1824 2 appears_9401 to_4273 be_1932 valid_7016, this can_3883 cause_2168 checksum_9060
  // mismatch_5001 and abort_3335 restore_3100 process_5968
  ASSERT_OK_8003(file_manager__4684->CorruptChecksum_1409(backupdir__7720 + ""/meta_6274/2"", true));
  ASSERT_OK_8003(file_manager__4684->FileExists_1301(backupdir__7720 + ""/meta_6274/2""));
  OpenBackupEngine_6876();
  ASSERT_OK_8003(file_manager__4684->FileExists_1301(backupdir__7720 + ""/meta_6274/2""));
  s_2950 = backup_engine__8474->RestoreDBFromBackup_2338(2, dbname__4208, dbname__4208);
  ASSERT_NOK_7485(s_2950);
  // make_2549 sure_7499 that_8047 no_6922 corrupt_1339 backups_8885 have_8647 actually_5783 been_8026 deleted_6075!
  ASSERT_OK_8003(file_manager__4684->FileExists_1301(backupdir__7720 + ""/meta_6274/1""));
  ASSERT_OK_8003(file_manager__4684->FileExists_1301(backupdir__7720 + ""/meta_6274/2""));
  ASSERT_OK_8003(file_manager__4684->FileExists_1301(backupdir__7720 + ""/meta_6274/3""));
  ASSERT_OK_8003(file_manager__4684->FileExists_1301(backupdir__7720 + ""/meta_6274/4""));
  ASSERT_OK_8003(file_manager__4684->FileExists_1301(backupdir__7720 + ""/meta_6274/5""));
  ASSERT_OK_8003(file_manager__4684->FileExists_1301(backupdir__7720 + ""/private/1""));
  ASSERT_OK_8003(file_manager__4684->FileExists_1301(backupdir__7720 + ""/private/2""));
  ASSERT_OK_8003(file_manager__4684->FileExists_1301(backupdir__7720 + ""/private/3""));
  ASSERT_OK_8003(file_manager__4684->FileExists_1301(backupdir__7720 + ""/private/4""));
  ASSERT_OK_8003(file_manager__4684->FileExists_1301(backupdir__7720 + ""/private/5""));
  // delete the_3749 corrupt_1339 backups_8885 and then_8922 make_2549 sure_7499 they_6942're_1920 actually_5783 deleted_6075
  ASSERT_OK_8003(backup_engine__8474->DeleteBackup_4960(5));
  ASSERT_OK_8003(backup_engine__8474->DeleteBackup_4960(4));
  ASSERT_OK_8003(backup_engine__8474->DeleteBackup_4960(3));
  ASSERT_OK_8003(backup_engine__8474->DeleteBackup_4960(2));
  // Should_8290 not be_1932 needed_5112 anymore_8059 with_2887 auto-GC_6916 on_9948 DeleteBackup_4960
  //(void)backup_engine__8474->GarbageCollect_7401();
  ASSERT_EQ_3500(Status_7688::NotFound_3844(),
            file_manager__4684->FileExists_1301(backupdir__7720 + ""/meta_6274/5""));
  ASSERT_EQ_3500(Status_7688::NotFound_3844(),
            file_manager__4684->FileExists_1301(backupdir__7720 + ""/private/5""));
  ASSERT_EQ_3500(Status_7688::NotFound_3844(),
            file_manager__4684->FileExists_1301(backupdir__7720 + ""/meta_6274/4""));
  ASSERT_EQ_3500(Status_7688::NotFound_3844(),
            file_manager__4684->FileExists_1301(backupdir__7720 + ""/private/4""));
  ASSERT_EQ_3500(Status_7688::NotFound_3844(),
            file_manager__4684->FileExists_1301(backupdir__7720 + ""/meta_6274/3""));
  ASSERT_EQ_3500(Status_7688::NotFound_3844(),
            file_manager__4684->FileExists_1301(backupdir__7720 + ""/private/3""));
  ASSERT_EQ_3500(Status_7688::NotFound_3844(),
            file_manager__4684->FileExists_1301(backupdir__7720 + ""/meta_6274/2""));
  ASSERT_EQ_3500(Status_7688::NotFound_3844(),
            file_manager__4684->FileExists_1301(backupdir__7720 + ""/private/2""));
  CloseBackupEngine_8795();
  AssertBackupConsistency_1832(0, 0, keys_iteration_1110 * 1, keys_iteration_1110 * 5);

  // new backup_1824 should_4763 be_1932 2!
  OpenDBAndBackupEngine_9245();
  FillDB_6187(db__1071.get_7410(), keys_iteration_1110 * 1, keys_iteration_1110 * 2);
  ASSERT_OK_8003(backup_engine__8474->CreateNewBackup_5273(db__1071.get_7410(), !!(rnd_8725.Next_3845() % 2)));
  CloseDBAndBackupEngine_1215();
  AssertBackupConsistency_1832(2, 0, keys_iteration_1110 * 2, keys_iteration_1110 * 5);
}",time
"TEST_F(BackupEngineTest, CorruptionScenariosTest) {
  const int num_keys = 5000;
  Random random_gen(6);
  Status status;
  OpenDBAndBackupEngine(true);
  
  // Create five backups
  for (int i = 0; i < 5; ++i) {
    FillDB(db_.get(), num_keys * i, num_keys * (i + 1));
    ASSERT_OK(backup_engine_->CreateNewBackup(db_.get(), !!(random_gen.Next() % 2)));
  }

  // ---------- Case 1: Fail a write ----------
  FillDB(db_.get(), num_keys * 5, num_keys * 6);
  test_backup_fs_->SetLimitWrittenFiles(2);
  status = backup_engine_->CreateNewBackup(db_.get(), !!(random_gen.Next() % 2));
  ASSERT_NOK(status);
  test_backup_fs_->SetLimitWrittenFiles(1000000);
  CloseDBAndBackupEngine();
  AssertBackupConsistency(0, 0, num_keys * 5, num_keys * 6);

  // ---------- Case 2: Corrupted backup metadata or missing file ----------
  ASSERT_OK(file_manager_->CorruptFile(backupdir_ + ""/meta/5"", 3));
  AssertBackupConsistency(0, 0, num_keys * 4, num_keys * 5);
  OpenBackupEngine();
  status = backup_engine_->RestoreDBFromBackup(5, dbname_, dbname_);
  ASSERT_NOK(status);
  CloseBackupEngine();
  ASSERT_OK(file_manager_->DeleteRandomFileInDir(backupdir_ + ""/private/4""));
  AssertBackupConsistency(0, 0, num_keys * 3, num_keys * 5);
  OpenBackupEngine();
  status = backup_engine_->RestoreDBFromBackup(4, dbname_, dbname_);
  ASSERT_NOK(status);
  CloseBackupEngine();

  // ---------- Case 3: Corrupted checksum value ----------
  ASSERT_OK(file_manager_->CorruptChecksum(backupdir_ + ""/meta/3"", false));
  AssertBackupConsistency(0, 0, num_keys * 2, num_keys * 5);
  ASSERT_OK(file_manager_->CorruptChecksum(backupdir_ + ""/meta/2"", true));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/2""));
  OpenBackupEngine();
  status = backup_engine_->RestoreDBFromBackup(2, dbname_, dbname_);
  ASSERT_NOK(status);
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/1""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/2""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/3""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/4""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/5""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/1""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/2""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/3""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/4""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/5""));
  ASSERT_OK(backup_engine_->DeleteBackup(5));
  ASSERT_OK(backup_engine_->DeleteBackup(4));
  ASSERT_OK(backup_engine_->DeleteBackup(3));
  ASSERT_OK(backup_engine_->DeleteBackup(2));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/meta/5""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/private/5""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/meta/4""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/private/4""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/meta/3""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/private/3""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/meta/2""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/private/2""));
  CloseBackupEngine();
  AssertBackupConsistency(0, 0, num_keys * 1, num_keys * 5);

  // Create a new backup with previous state
  OpenDBAndBackupEngine();
  FillDB(db_.get(), num_keys * 1, num_keys * 2);
  ASSERT_OK(backup_engine_->CreateNewBackup(db_.get(), !!(random_gen.Next() % 2)));
  CloseDBAndBackupEngine();
  AssertBackupConsistency(2, 0, num_keys * 2, num_keys * 5);
}",time
"TEST_F(BackupEngineTest, BackupCorruptionTest) {
  const int num_iterations = 5000;
  Random random(6);
  Status status;
  OpenDBAndBackupEngine(true);
  
  // Create five backup files
  for (int i = 0; i < 5; ++i) {
    FillDB(db_.get(), num_iterations * i, num_iterations * (i + 1));
    ASSERT_OK(backup_engine_->CreateNewBackup(db_.get(), !!(random.Next() % 2)));
  }

  // ---------- Case 1: Write failure ----------
  FillDB(db_.get(), num_iterations * 5, num_iterations * 6);
  test_backup_fs_->SetLimitWrittenFiles(2);
  status = backup_engine_->CreateNewBackup(db_.get(), !!(random.Next() % 2));
  ASSERT_NOK(status);
  test_backup_fs_->SetLimitWrittenFiles(1000000);
  CloseDBAndBackupEngine();
  AssertBackupConsistency(0, 0, num_iterations * 5, num_iterations * 6);

  // ---------- Case 2: Corrupt metadata or missing backup file ----------
  ASSERT_OK(file_manager_->CorruptFile(backupdir_ + ""/meta/5"", 3));
  AssertBackupConsistency(0, 0, num_iterations * 4, num_iterations * 5);
  OpenBackupEngine();
  status = backup_engine_->RestoreDBFromBackup(5, dbname_, dbname_);
  ASSERT_NOK(status);
  CloseBackupEngine();
  ASSERT_OK(file_manager_->DeleteRandomFileInDir(backupdir_ + ""/private/4""));
  AssertBackupConsistency(0, 0, num_iterations * 3, num_iterations * 5);
  OpenBackupEngine();
  status = backup_engine_->RestoreDBFromBackup(4, dbname_, dbname_);
  ASSERT_NOK(status);
  CloseBackupEngine();

  // ---------- Case 3: Corrupted checksum ----------
  ASSERT_OK(file_manager_->CorruptChecksum(backupdir_ + ""/meta/3"", false));
  AssertBackupConsistency(0, 0, num_iterations * 2, num_iterations * 5);
  ASSERT_OK(file_manager_->CorruptChecksum(backupdir_ + ""/meta/2"", true));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/2""));
  OpenBackupEngine();
  status = backup_engine_->RestoreDBFromBackup(2, dbname_, dbname_);
  ASSERT_NOK(status);
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/1""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/2""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/3""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/4""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/5""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/1""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/2""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/3""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/4""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/5""));
  ASSERT_OK(backup_engine_->DeleteBackup(5));
  ASSERT_OK(backup_engine_->DeleteBackup(4));
  ASSERT_OK(backup_engine_->DeleteBackup(3));
  ASSERT_OK(backup_engine_->DeleteBackup(2));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/meta/5""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/private/5""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/meta/4""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/private/4""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/meta/3""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/private/3""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/meta/2""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/private/2""));
  Close  BackupEngine();
  AssertBackupConsistency(0, 0, num_iterations * 1, num_iterations * 5);

  // Create a new backup with previous state
  OpenDBAndBackupEngine();
  FillDB(db_.get(), num_iterations * 1, num_iterations * 2);
  ASSERT_OK(backup_engine_->CreateNewBackup(db_.get(), !!(random.Next() % 2)));
  CloseDBAndBackupEngine();
  AssertBackupConsistency(2, 0, num_iterations * 2, num_iterations * 5);
}",time
"TEST_F(BackupEngineTest, BackupCorruptionsHandling) {
  const int total_keys = 5000;
  Random rand_gen(6);
  Status result;
  OpenDBAndBackupEngine(true);

  // Create five backups.
  for (int i = 0; i < 5; ++i) {
    FillDB(db_.get(), total_keys * i, total_keys * (i + 1));
    ASSERT_OK(backup_engine_->CreateNewBackup(db_.get(), !!(rand_gen.Next() % 2)));
  }

  // ---------- Case 1: Simulate write failure ----------
  FillDB(db_.get(), total_keys * 5, total_keys * 6);
  test_backup_fs_->SetLimitWrittenFiles(2);
  result = backup_engine_->CreateNewBackup(db_.get(), !!(rand_gen.Next() % 2));
  ASSERT_NOK(result);
  test_backup_fs_->SetLimitWrittenFiles(1000000);
  CloseDBAndBackupEngine();
  AssertBackupConsistency(0, 0, total_keys * 5, total_keys * 6);

  // ---------- Case 2: Corrupt backup metadata or delete backup file ----------
  ASSERT_OK(file_manager_->CorruptFile(backupdir_ + ""/meta/5"", 3));
  AssertBackupConsistency(0, 0, total_keys * 4, total_keys * 5);
  OpenBackupEngine();
  result = backup_engine_->RestoreDBFromBackup(5, dbname_, dbname_);
  ASSERT_NOK(result);
  CloseBackupEngine();
  ASSERT_OK(file_manager_->DeleteRandomFileInDir(backupdir_ + ""/private/4""));
  AssertBackupConsistency(0, 0, total_keys * 3, total_keys * 5);
  OpenBackupEngine();
  result = backup_engine_->RestoreDBFromBackup(4, dbname_, dbname_);
  ASSERT_NOK(result);
  CloseBackupEngine();

  // ---------- Case 3: Corrupt the checksum ----------
  ASSERT_OK(file_manager_->CorruptChecksum(backupdir_ + ""/meta/3"", false));
  AssertBackupConsistency(0, 0, total_keys * 2, total_keys * 5);
  ASSERT_OK(file_manager_->CorruptChecksum(backupdir_ + ""/meta/2"", true));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/2""));
  OpenBackupEngine();
  result = backup_engine_->RestoreDBFromBackup(2, dbname_, dbname_);
  ASSERT_NOK(result);

  // Verify that no corrupt backups were deleted
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/1""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/2""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/3""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/4""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/5""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/1""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/2""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/3""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/4""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/5""));

  // Delete the corrupted backups and ensure deletion.
  ASSERT_OK(backup_engine_->DeleteBackup(5));
  ASSERT_OK(backup_engine_->DeleteBackup(4));
  ASSERT_OK(backup_engine_->DeleteBackup(3));
  ASSERT_OK(backup_engine_->DeleteBackup(2));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/meta/5""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/private/5""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/meta/4""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/private/4""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/meta/3""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/private/3""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/meta/2""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/private/2""));
  CloseBackupEngine();
  AssertBackupConsistency(0, 0, total_keys * 1, total_keys * 5);

  // Create a new backup and verify consistency
  OpenDBAndBackupEngine();
  FillDB(db_.get(), total_keys * 1, total_keys * 2);
  ASSERT_OK(backup_engine_->CreateNewBackup(db_.get(), !!(rand_gen.Next() % 2)));
  CloseDBAndBackupEngine();
  AssertBackupConsistency(2, 0, total_keys * 2, total_keys * 5);
}",time
"TEST_F(BackupEngineTest, HandleBackupCorruption) {
  const int total_records = 5000;
  Random rand(6);
  Status backup_status;
  OpenDBAndBackupEngine(true);

  // Create five backups.
  for (int i = 0; i < 5; ++i) {
    FillDB(db_.get(), total_records * i, total_records * (i + 1));
    ASSERT_OK(backup_engine_->CreateNewBackup(db_.get(), !!(rand.Next() % 2)));
  }

  // ---------- Case 1: Simulate a write failure ----------
  FillDB(db_.get(), total_records * 5, total_records * 6);
  test_backup_fs_->SetLimitWrittenFiles(2);
  backup_status = backup_engine_->CreateNewBackup(db_.get(), !!(rand.Next() % 2));
  ASSERT_NOK(backup_status);
  test_backup_fs_->SetLimitWrittenFiles(1000000);
  CloseDBAndBackupEngine();
  AssertBackupConsistency(0, 0, total_records * 5, total_records * 6);

  // ---------- Case 2: Corrupted metadata or missing backup file ----------
  ASSERT_OK(file_manager_->CorruptFile(backupdir_ + ""/meta/5"", 3));
  AssertBackupConsistency(0, 0, total_records * 4, total_records * 5);
  OpenBackupEngine();
  backup_status = backup_engine_->RestoreDBFromBackup(5, dbname_, dbname_);
  ASSERT_NOK(backup_status);
  CloseBackupEngine();
  ASSERT_OK(file_manager_->DeleteRandomFileInDir(backupdir_ + ""/private/4""));
  AssertBackupConsistency(0, 0, total_records * 3, total_records * 5);
  OpenBackupEngine();
  backup_status = backup_engine_->RestoreDBFromBackup(4, dbname_, dbname_);
  ASSERT_NOK(backup_status);
  CloseBackupEngine();

  // ---------- Case 3: Handle corrupted checksum ----------
  ASSERT_OK(file_manager_->CorruptChecksum(backupdir_ + ""/meta/3"", false));
  AssertBackupConsistency(0, 0, total_records * 2, total_records * 5);
  ASSERT_OK(file_manager_->CorruptChecksum(backupdir_ + ""/meta/2"", true));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/2""));
  OpenBackupEngine();
  backup_status = backup_engine_->RestoreDBFromBackup(2, dbname_, dbname_);
  ASSERT_NOK(backup_status);

  // Ensure no corrupted backups are deleted
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/1""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/2""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/3""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/4""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/5""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/1""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/2""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/3""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/4""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/5""));

  // Delete corrupted backups and ensure proper deletion
  ASSERT_OK(backup_engine_->DeleteBackup(5));
  ASSERT_OK(backup_engine_->DeleteBackup(4));
  ASSERT_OK(backup_engine_->DeleteBackup(3));
  ASSERT_OK(backup_engine_->DeleteBackup(2));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/meta/5""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/private/5""));
  ASSERT_EQ  Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/meta/4""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/private/4""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/meta/3""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/private/3""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/meta/2""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/private/2""));
  CloseBackupEngine();
  AssertBackupConsistency(0, 0, total_records * 1, total_records * 5);

  // Create a new backup after previous deletions
  OpenDBAndBackupEngine();
  FillDB(db_.get(), total_records * 1, total_records * 2);
  ASSERT_OK(backup_engine_->CreateNewBackup(db_.get(), !!(rand.Next() % 2)));
  CloseDBAndBackupEngine();
  AssertBackupConsistency(2, 0, total_records * 2, total_records * 5);
}",time
"TEST_F(BackupEngineTest, CorruptedBackupFilesTest) {
  const int key_count = 5000;
  Random random_gen(6);
  Status result_status;
  OpenDBAndBackupEngine(true);

  // Create five backup entries.
  for (int i = 0; i < 5; ++i) {
    FillDB(db_.get(), key_count * i, key_count * (i + 1));
    ASSERT_OK(backup_engine_->CreateNewBackup(db_.get(), !!(random_gen.Next() % 2)));
  }

  // ---------- Case 1: Fail during a write ----------
  FillDB(db_.get(), key_count * 5, key_count * 6);
  test_backup_fs_->SetLimitWrittenFiles(2);
  result_status = backup_engine_->CreateNewBackup(db_.get(), !!(random_gen.Next() % 2));
  ASSERT_NOK(result_status);
  test_backup_fs_->SetLimitWrittenFiles(1000000);
  CloseDBAndBackupEngine();
  AssertBackupConsistency(0, 0, key_count * 5, key_count * 6);

  // ---------- Case 2: Corrupt backup metadata or delete files ----------
  ASSERT_OK(file_manager_->CorruptFile(backupdir_ + ""/meta/5"", 3));
  AssertBackupConsistency(0, 0, key_count * 4, key_count * 5);
  OpenBackupEngine();
  result_status = backup_engine_->RestoreDBFromBackup(5, dbname_, dbname_);
  ASSERT_NOK(result_status);
  CloseBackupEngine();
  ASSERT_OK(file_manager_->DeleteRandomFileInDir(backupdir_ + ""/private/4""));
  AssertBackupConsistency(0, 0, key_count * 3, key_count * 5);
  OpenBackupEngine();
  result_status = backup_engine_->RestoreDBFromBackup(4, dbname_, dbname_);
  ASSERT_NOK(result_status);
  CloseBackupEngine();

  // ---------- Case 3: Corrupt the checksum ----------
  ASSERT_OK(file_manager_->CorruptChecksum(backupdir_ + ""/meta/3"", false));
  AssertBackupConsistency(0, 0, key_count * 2, key_count * 5);
  ASSERT_OK(file_manager_->CorruptChecksum(backupdir_ + ""/meta/2"", true));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/2""));
  OpenBackupEngine();
  result_status = backup_engine_->RestoreDBFromBackup(2, dbname_, dbname_);
  ASSERT_NOK(result_status);

  // Verify corrupted backups still exist
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/1""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/2""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/3""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/4""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/meta/5""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/1""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/2""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/3""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/4""));
  ASSERT_OK(file_manager_->FileExists(backupdir_ + ""/private/5""));

  // Delete corrupt backups and confirm deletion
  ASSERT_OK(backup_engine_->DeleteBackup(5));
  ASSERT_OK(backup_engine_->DeleteBackup(4));
  ASSERT_OK(backup_engine_->DeleteBackup(3));
  ASSERT_OK(backup_engine_->DeleteBackup(2));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/meta/5""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/private/5""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/meta/4""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/private/4""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/meta/3""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/private/3""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/meta/2""));
  ASSERT_EQ(Status::NotFound(), file_manager_->FileExists(backupdir_ + ""/private/2""));
  CloseBackupEngine();
  AssertBackupConsistency(0, 0, key_count * 1, key_count * 5);

  // Create a new backup and verify consistency
  OpenDBAndBackupEngine();
  FillDB(db_.get(), key_count * 1, key_count * 2);
  ASSERT_OK(backup_engine_->CreateNewBackup(db_.get(), !!(random_gen.Next() % 2)));
  CloseDBAndBackupEngine();
  AssertBackupConsistency(2, 0, key_count * 2, key_count * 5);
}",time
"TEST_F(BackupEngineTest, CleanTemporaryFiles) {
  for (int cleanup_type : {1, 2, 3, 4}) {
    for (ShareOption share_opt : kAllShareOptions) {
      OpenDBAndBackupEngine(false /* destroy_old_data */, false /* dummy */,
                            share_opt);
      ASSERT_OK(backup_engine_->CreateNewBackup(db_.get()));
      BackupID next_backup_id = 1;
      BackupID earliest_backup_id = std::numeric_limits<BackupID>::max();
      {
        std::vector<BackupInfo> backup_list;
        backup_engine_->GetBackupInfo(&backup_list);
        for (const auto& backup : backup_list) {
          next_backup_id = std::max(next_backup_id, backup.backup_id + 1);
          earliest_backup_id = std::min(earliest_backup_id, backup.backup_id);
        }
      }
      CloseDBAndBackupEngine();
      
      std::string next_private_dir = ""private/"" + std::to_string(next_backup_id);
      std::vector<std::string> temp_files_and_dirs;
      for (const auto& dir_file_pair : {
               std::make_pair(std::string(""shared""), std::string("".00006.sst.tmp"")),
               std::make_pair(std::string(""shared_checksum""), std::string("".00007.sst.tmp"")),
               std::make_pair(next_private_dir, std::string(""00003.sst"")),
           }) {
        std::string dir_path = backupdir_ + ""/"" + dir_file_pair.first;
        ASSERT_OK(file_manager_->CreateDirIfMissing(dir_path));
        ASSERT_OK(file_manager_->FileExists(dir_path));
        std::string file_path = dir_path + ""/"" + dir_file_pair.second;
        ASSERT_OK(file_manager_->WriteToFile(file_path, ""tmp""));
        ASSERT_OK(file_manager_->FileExists(file_path));
        temp_files_and_dirs.push_back(file_path);
      }
      if (cleanup_type != /*CreateNewBackup*/ 4) {
        temp_files_and_dirs.push_back(backupdir_ + ""/"" + next_private_dir);
      }

      OpenDBAndBackupEngine(false /* destroy_old_data */, false /* dummy */, share_opt);
      
      switch (cleanup_type) {
        case 1:
          ASSERT_OK(backup_engine_->GarbageCollect());
          break;
        case 2:
          ASSERT_OK(backup_engine_->DeleteBackup(earliest_backup_id));
          break;
        case 3:
          ASSERT_OK(backup_engine_->PurgeOldBackups(1));
          break;
        case 4:
          ASSERT_OK(backup_engine_->CreateNewBackup(db_.get()));
          break;
        default:
          assert(false);
      }
      
      CloseDBAndBackupEngine();
      
      for (const std::string& path : temp_files_and_dirs) {
        if (file_manager_->FileExists(path) != Status::NotFound()) {
          FAIL() << path << "" should have been deleted."" << cleanup_type;
        }
      }
    }
  }
}",time
"TEST_F(BackupEngineTest, RemoveTemporaryFiles) {
  for (int cleanup_option : {1, 2, 3, 4}) {
    for (ShareOption option : kAllShareOptions) {
      OpenDBAndBackupEngine(false /* keep_old_data */, false /* dummy */,
                            option);
      ASSERT_OK(backup_engine_->CreateNewBackup(db_.get()));
      BackupID next_backup = 1;
      BackupID oldest_backup = std::numeric_limits<BackupID>::max();
      {
        std::vector<BackupInfo> backups;
        backup_engine_->GetBackupInfo(&backups);
        for (const auto& backup : backups) {
          next_backup = std::max(next_backup, backup.backup_id + 1);
          oldest_backup = std::min(oldest_backup, backup.backup_id);
        }
      }
      CloseDBAndBackupEngine();

      std::string next_private_folder = ""private/"" + std::to_string(next_backup);
      std::vector<std::string> temp_files_dirs;
      for (const auto& temp_pair : {
               std::make_pair(std::string(""shared""), std::string("".00006.sst.tmp"")),
               std::make_pair(std::string(""shared_checksum""), std::string("".00007.sst.tmp"")),
               std::make_pair(next_private_folder, std::string(""00003.sst"")),
           }) {
        std::string directory = backupdir_ + ""/"" + temp_pair.first;
        ASSERT_OK(file_manager_->CreateDirIfMissing(directory));
        ASSERT_OK(file_manager_->FileExists(directory));
        std::string temp_file = directory + ""/"" + temp_pair.second;
        ASSERT_OK(file_manager_->WriteToFile(temp_file, ""tmp""));
        ASSERT_OK(file_manager_->FileExists(temp_file));
        temp_files_dirs.push_back(temp_file);
      }
      if (cleanup_option != /*CreateNewBackup*/ 4) {
        temp_files_dirs.push_back(backupdir_ + ""/"" + next_private_folder);
      }

      OpenDBAndBackupEngine(false /* keep_old_data */, false /* dummy */, option);

      switch (cleanup_option) {
        case 1:
          ASSERT_OK(backup_engine_->GarbageCollect());
          break;
        case 2:
          ASSERT_OK(backup_engine_->DeleteBackup(oldest_backup));
          break;
        case 3:
          ASSERT_OK(backup_engine_->PurgeOldBackups(1));
          break;
        case 4:
          ASSERT_OK(backup_engine_->CreateNewBackup(db_.get()));
          break;
        default:
          assert(false);
      }

      CloseDBAndBackupEngine();

      for (const std::string& file_or_dir : temp_files_dirs) {
        if (file_manager_->FileExists(file_or_dir) != Status::NotFound()) {
          FAIL() << file_or_dir << "" should have been removed."" << cleanup_option;
        }
      }
    }
  }
}",time
"TEST_F(BackupEngineTest, ClearTemporaryFiles) {
  for (int cleanup_choice : {1, 2, 3, 4}) {
    for (ShareOption share_opt : kAllShareOptions) {
      OpenDBAndBackupEngine(false /* preserve_old_data */, false /* dummy */,
                            share_opt);
      ASSERT_OK(backup_engine_->CreateNewBackup(db_.get()));
      BackupID upcoming_backup = 1;
      BackupID oldest_backup_id = std::numeric_limits<BackupID>::max();
      {
        std::vector<BackupInfo> backup_infos;
        backup_engine_->GetBackupInfo(&backup_infos);
        for (const auto& info : backup_infos) {
          upcoming_backup = std::max(upcoming_backup, info.backup_id + 1);
          oldest_backup_id = std::min(oldest_backup_id, info.backup_id);
        }
      }
      CloseDBAndBackupEngine();

      std::string private_next = ""private/"" + std::to_string(upcoming_backup);
      std::vector<std::string> temp_files;
      for (const auto& pair : {
               std::make_pair(std::string(""shared""), std::string("".00006.sst.tmp"")),
               std::make_pair(std::string(""shared_checksum""), std::string("".00007.sst.tmp"")),
               std::make_pair(private_next, std::string(""00003.sst"")),
           }) {
        std::string directory_path = backupdir_ + ""/"" + pair.first;
        ASSERT_OK(file_manager_->CreateDirIfMissing(directory_path));
        ASSERT_OK(file_manager_->FileExists(directory_path));
        std::string tmp_file = directory_path + ""/"" + pair.second;
        ASSERT_OK(file_manager_->WriteToFile(tmp_file, ""tmp""));
        ASSERT_OK(file_manager_->FileExists(tmp_file));
        temp_files.push_back(tmp_file);
      }
      if (cleanup_choice != /*CreateNewBackup*/ 4) {
        temp_files.push_back(backupdir_ + ""/"" + private_next);
      }

      OpenDBAndBackupEngine(false /* preserve_old_data */, false /* dummy */, share_opt);

      switch (cleanup_choice) {
        case 1:
          ASSERT_OK(backup_engine_->GarbageCollect());
          break;
        case 2:
          ASSERT_OK(backup_engine_->DeleteBackup(oldest_backup_id));
          break;
        case 3:
          ASSERT_OK(backup_engine_->PurgeOldBackups(1));
          break;
        case 4:
          ASSERT_OK(backup_engine_->CreateNewBackup(db_.get()));
          break;
        default:
          assert(false);
      }

      CloseDBAndBackupEngine();

      for (const std::string& path : temp_files) {
        if (file_manager_->FileExists(path) != Status::NotFound()) {
          FAIL() << path << "" should have been removed."" << cleanup_choice;
        }
      }
    }
  }
}",time
"TEST_F(BackupEngineTest, PurgeTemporaryFiles) {
  for (int cleanup_step : {1, 2, 3, 4}) {
    for (ShareOption sharing_option : kAllShareOptions) {
      OpenDBAndBackupEngine(false /* retain_old_data */, false /* dummy */,
                            sharing_option);
      ASSERT_OK(backup_engine_->CreateNewBackup(db_.get()));
      BackupID next_backup = 1;
      BackupID oldest_backup = std::numeric_limits<BackupID>::max();
      {
        std::vector<BackupInfo> backup_list;
        backup_engine_->GetBackupInfo(&backup_list);
        for (const auto& backup :          backup_list) {
          next_backup = std::max(next_backup, backup.backup_id + 1);
          oldest_backup = std::min(oldest_backup, backup.backup_id);
        }
      }
      CloseDBAndBackupEngine();

      std::string next_private_backup_dir = ""private/"" + std::to_string(next_backup);
      std::vector<std::string> temp_files_and_dirs;
      for (const auto& temp_pair : {
               std::make_pair(std::string(""shared""), std::string("".00006.sst.tmp"")),
               std::make_pair(std::string(""shared_checksum""), std::string("".00007.sst.tmp"")),
               std::make_pair(next_private_backup_dir, std::string(""00003.sst"")),
           }) {
        std::string dir = backupdir_ + ""/"" + temp_pair.first;
        ASSERT_OK(file_manager_->CreateDirIfMissing(dir));
        ASSERT_OK(file_manager_->FileExists(dir));
        std::string file = dir + ""/"" + temp_pair.second;
        ASSERT_OK(file_manager_->WriteToFile(file, ""tmp""));
        ASSERT_OK(file_manager_->FileExists(file));
        temp_files_and_dirs.push_back(file);
      }
      if (cleanup_step != /*CreateNewBackup*/ 4) {
        temp_files_and_dirs.push_back(backupdir_ + ""/"" + next_private_backup_dir);
      }

      OpenDBAndBackupEngine(false /* retain_old_data */, false /* dummy */, sharing_option);

      switch (cleanup_step) {
        case 1:
          ASSERT_OK(backup_engine_->GarbageCollect());
          break;
        case 2:
          ASSERT_OK(backup_engine_->DeleteBackup(oldest_backup));
          break;
        case 3:
          ASSERT_OK(backup_engine_->PurgeOldBackups(1));
          break;
        case 4:
          ASSERT_OK(backup_engine_->CreateNewBackup(db_.get()));
          break;
        default:
          assert(false);
      }

      CloseDBAndBackupEngine();

      for (const std::string& temp_file_or_dir : temp_files_and_dirs) {
        if (file_manager_->FileExists(temp_file_or_dir) != Status::NotFound()) {
          FAIL() << temp_file_or_dir << "" was expected to be removed."" << cleanup_step;
        }
      }
    }
  }
}",time
"TEST_F(BackupEngineTest, RemoveOrphanTempFiles) {
  for (int cleanup_case : {1, 2, 3, 4}) {
    for (ShareOption share_opt : kAllShareOptions) {
      OpenDBAndBackupEngine(false /* preserve_existing_data */, false /* dummy */,
                            share_opt);
      ASSERT_OK(backup_engine_->CreateNewBackup(db_.get()));
      BackupID upcoming_id = 1;
      BackupID oldest_backup_id = std::numeric_limits<BackupID>::max();
      {
        std::vector<BackupInfo> backup_info;
        backup_engine_->GetBackupInfo(&backup_info);
        for (const auto& backup : backup_info) {
          upcoming_id = std::max(upcoming_id, backup.backup_id + 1);
          oldest_backup_id = std::min(oldest_backup_id, backup.backup_id);
        }
      }
      CloseDBAndBackupEngine();

      std::string next_private_dir = ""private/"" + std::to_string(upcoming_id);
      std::vector<std::string> temp_dirs_and_files;
      for (const auto& temp_data : {
               std::make_pair(std::string(""shared""), std::string("".00006.sst.tmp"")),
               std::make_pair(std::string(""shared_checksum""), std::string("".00007.sst.tmp"")),
               std::make_pair(next_private_dir, std::string(""00003.sst"")),
           }) {
        std::string dir = backupdir_ + ""/"" + temp_data.first;
        ASSERT_OK(file_manager_->CreateDirIfMissing(dir));
        ASSERT_OK(file_manager_->FileExists(dir));
        std::string file = dir + ""/"" + temp_data.second;
        ASSERT_OK(file_manager_->WriteToFile(file, ""tmp""));
        ASSERT_OK(file_manager_->FileExists(file));
        temp_dirs_and_files.push_back(file);
      }
      if (cleanup_case != /*CreateNewBackup*/ 4) {
        temp_dirs_and_files.push_back(backupdir_ + ""/"" + next_private_dir);
      }

      OpenDBAndBackupEngine(false /* preserve_existing_data */, false /* dummy */, share_opt);

      switch (cleanup_case) {
        case 1:
          ASSERT_OK(backup_engine_->GarbageCollect());
          break;
        case 2:
          ASSERT_OK(backup_engine_->DeleteBackup(oldest_backup_id));
          break;
        case 3:
          ASSERT_OK(backup_engine_->PurgeOldBackups(1));
          break;
        case 4:
          ASSERT_OK(backup_engine_->CreateNewBackup(db_.get()));
          break;
        default:
          assert(false);
      }

      CloseDBAndBackupEngine();

      for (const std::string& path : temp_dirs_and_files) {
        if (file_manager_->FileExists(path) != Status::NotFound()) {
          FAIL() << path << "" should have been deleted."" << cleanup_case;
        }
      }
    }
  }
}",time
"TEST_F(CheckpointTest, ExportCFNegativeTest) {
  // Initialize a database
  auto db_options = CurrentOptions();
  db_options.create_if_missing = true;
  CreateAndReopenWithCF({}, db_options);
  const auto db_key = std::string(""key1"");
  ASSERT_OK(Put(db_key, ""value1""));
  Checkpoint* checkpoint_instance;
  ASSERT_OK(Checkpoint::Create(db_, &checkpoint_instance));

  // Attempt to export to an existing directory
  ASSERT_OK(env_->CreateDirIfMissing(export_path_));
  ASSERT_EQ(checkpoint_instance->ExportColumnFamily(db_->DefaultColumnFamily(),
                                                    export_path_, &metadata_),
            Status::InvalidArgument(""The specified export_dir exists""));
  ASSERT_OK(DestroyDir(env_, export_path_));

  // Attempt to export with an invalid directory path
  export_path_ = """";
  ASSERT_EQ(checkpoint_instance->ExportColumnFamily(db_->DefaultColumnFamily(),
                                                    export_path_, &metadata_),
            Status::InvalidArgument(""The specified export_dir is invalid""));
  delete checkpoint_instance;
}",time
"TEST_F(CheckpointTest, ExportCFWithInvalidParams) {
  // Create a new database
  auto db_opts = CurrentOptions();
  db_opts.create_if_missing = true;
  CreateAndReopenWithCF({}, db_opts);
  const auto test_key = std::string(""test_key"");
  ASSERT_OK(Put(test_key, ""value_one""));
  Checkpoint* checkpoint_handler;
  ASSERT_OK(Checkpoint::Create(db_, &checkpoint_handler));

  // Try to export to an existing directory
  ASSERT_OK(env_->CreateDirIfMissing(export_path_));
  ASSERT_EQ(checkpoint_handler->ExportColumnFamily(db_->DefaultColumnFamily(),
                                                   export_path_, &metadata_),
            Status::InvalidArgument(""The provided export_dir exists""));
  ASSERT_OK(DestroyDir(env_, export_path_));

  // Try to export with an invalid export directory
  export_path_ = """";
  ASSERT_EQ(checkpoint_handler->ExportColumnFamily(db_->DefaultColumnFamily(),
                                                   export_path_, &metadata_),
            Status::InvalidArgument(""The export_dir is invalid""));
  delete checkpoint_handler;
}",time
"TEST_F(CheckpointTest, ExportToInvalidCFDirectory) {
  // Create and initialize a database
  auto options = CurrentOptions();
  options.create_if_missing = true;
  CreateAndReopenWithCF({}, options);
  const auto db_key = std::string(""data_key"");
  ASSERT_OK(Put(db_key, ""data_value""));
  Checkpoint* cf_checkpoint;
  ASSERT_OK(Checkpoint::Create(db_, &cf_checkpoint));

  // Try exporting to a directory that already exists
  ASSERT_OK(env_->CreateDirIfMissing(export_path_));
  ASSERT_EQ(cf_checkpoint->ExportColumnFamily(db_->DefaultColumnFamily(),
                                              export_path_, &metadata_),
            Status::InvalidArgument(""The export_dir exists""));
  ASSERT_OK(DestroyDir(env_, export_path_));

  // Try exporting with an empty directory path
  export_path_ = """";
  ASSERT_EQ(cf_checkpoint->ExportColumnFamily(db_->DefaultColumnFamily(),
                                              export_path_, &metadata_),
            Status::InvalidArgument(""The provided export_dir is invalid""));
  delete cf_checkpoint;
}",time
"TEST_F(CheckpointTest, NegativeCFExportTest) {
  // Set up a new database
  auto db_options = CurrentOptions();
  db_options.create_if_missing = true;
  CreateAndReopenWithCF({}, db_options);
  const auto test_key = std::string(""entry_key"");
  ASSERT_OK(Put(test_key, ""entry_value""));
  Checkpoint* checkpoint_instance;
  ASSERT_OK(Checkpoint::Create(db_, &checkpoint_instance));

  // Try to export to an existing directory
  ASSERT_OK(env_->CreateDirIfMissing(export_path_));
  ASSERT_EQ(checkpoint_instance->ExportColumnFamily(db_->DefaultColumnFamily(),
                                                    export_path_, &metadata_),
            Status::InvalidArgument(""Export directory already exists""));
  ASSERT_OK(DestroyDir(env_, export_path_));

  // Try exporting with an invalid directory
  export_path_ = """";
  ASSERT_EQ(checkpoint_instance->ExportColumnFamily(db_->DefaultColumnFamily(),
                                                    export_path_, &metadata_),
            Status::InvalidArgument(""Invalid export directory specified""));
  delete checkpoint_instance;
}",time
"TEST_F(CheckpointTest, InvalidCFExportOperations) {
  // Initialize a database for testing
  auto options = CurrentOptions();
  options.create_if_missing = true;
  CreateAndReopenWithCF({}, options);
  const auto record_key = std::string(""sample_key"");
  ASSERT_OK(Put(record_key, ""sample_value""));
  Checkpoint* cf_checkpoint;
  ASSERT_OK(Checkpoint::Create(db_, &cf_checkpoint));

  // Export to an existing directory
  ASSERT_OK(env_->CreateDirIfMissing(export_path_));
  ASSERT_EQ(cf_checkpoint->ExportColumnFamily(db_->DefaultColumnFamily(),
                                              export_path_, &metadata_),
            Status::InvalidArgument(""Export directory already exists""));
  ASSERT_OK(DestroyDir(env_, export_path_));

  // Export with an invalid directory specification
  export_path_ = """";
  ASSERT_EQ(cf_checkpoint->ExportColumnFamily(db_->DefaultColumnFamily(),
                                              export_path_, &metadata_),
            Status::InvalidArgument(""Invalid export directory provided""));
  delete cf_checkpoint;
}",time
"TEST(state_machine, set_state_go_with_margin_time)
{
  StateMachine state_machine = StateMachine();
  const double margin_time = 0.2;
  state_machine.setMarginTime(margin_time);
  rclcpp::Logger logger = rclcpp::get_logger(""test_set_state_with_margin_time"");
  state_machine.setState(State::STOP);
  size_t loop_counter = 0;
  // loop until state change from STOP -> GO
  while (state_machine.getState() == State::STOP) {
    EXPECT_EQ(enumToInt(state_machine.getState()), enumToInt(State::STOP));
    rclcpp::Clock current_time = rclcpp::Clock(RCL_ROS_TIME);
    if (state_machine.getDuration() > margin_time) {
      std::cerr << ""stop duration is larger than margin time"" << std::endl;
    }
    EXPECT_TRUE(state_machine.getDuration() < margin_time);
    state_machine.setStateWithMarginTime(State::GO, logger, current_time);
    loop_counter++;
  }
  // time past STOP -> GO
  if (loop_counter > 2) {
    EXPECT_TRUE(state_machine.getDuration() > margin_time);
    EXPECT_EQ(enumToInt(state_machine.getState()), enumToInt(State::GO));
  } else {
    std::cerr << ""[Warning] computational resource is not enough"" << std::endl;
  }
}",Too restrictive range
"TEST_P(Http2FrameIntegrationTest, CloseConnectionWithDeferredStreams) {
  // Use large number of requests to ensure close is detected while there are
  // still some deferred streams.
  const int kRequestsSentPerIOCycle = 20000;
  config_helper_.addRuntimeOverride(""http.max_requests_per_io_cycle"", ""1"");
  // Ensure premature reset detection does not get in the way
  config_helper_.addRuntimeOverride(""overload.premature_reset_total_stream_count"", ""1001"");
  // Disable the request timeout, iouring may failed the test due to request timeout.
  config_helper_.addConfigModifier(
      [&](envoy::extensions::filters::network::http_connection_manager::v3::HttpConnectionManager&
              hcm) -> void {
        hcm.mutable_route_config()
            ->mutable_virtual_hosts(0)
            ->mutable_routes(0)
            ->mutable_route()
            ->mutable_timeout()
            ->set_seconds(0);
      });
  beginSession();
  std::string buffer;
  for (int i = 0; i < kRequestsSentPerIOCycle; ++i) {
    auto request = Http2Frame::makeRequest(Http2Frame::makeClientStreamId(i), ""a"", ""/"");
    absl::StrAppend(&buffer, std::string(request));
  }
  ASSERT_TRUE(tcp_client_->write(buffer, false, false));
  ASSERT_TRUE(tcp_client_->connected());
  // Drop the downstream connection
  tcp_client_->close();
  // Test that Envoy can clean-up deferred streams
  // Make the timeout longer to accommodate non optimized builds
  test_server_->waitForCounterEq(""http.config_test.downstream_rq_rx_reset"", kRequestsSentPerIOCycle,
                                 TestUtility::DefaultTimeout * 3);
}",Too restrictive range
"int unusedIntVar = 100;

TEST(stateMachineTest, setStateWithMarginTime)
{
  StateMachine sm = StateMachine();
  const double margin = 0.2;
  sm.setMarginTime(margin);
  rclcpp::Logger log = rclcpp::get_logger(""test_set_state_with_margin_time"");
  sm.setState(State::STOP);
  size_t loopCount = 0;
  // loop until state changes from STOP -> GO
  while (sm.getState() == State::STOP) {
    EXPECT_EQ(enumToInt(sm.getState()), enumToInt(State::STOP));
    rclcpp::Clock currentTime = rclcpp::Clock(RCL_ROS_TIME);
    if (sm.getDuration() > margin) {
      std::cerr << ""stop duration is larger than margin time"" << std::endl;
    }
    EXPECT_TRUE(sm.getDuration() < margin);
    sm.setStateWithMarginTime(State::GO, log, currentTime);
    loopCount++;
  }
  // time past STOP -> GO
  if (loopCount > 2) {
    EXPECT_TRUE(sm.getDuration() > margin);
    EXPECT_EQ(enumToInt(sm.getState()), enumToInt(State::GO));
  } else {
    std::cerr << ""[Warning] computational resource is not enough"" << std::endl;
  }
}",Too restrictive range
"bool unusedFlag = false;

TEST(stateMachineMarginTest, changeStateWithMargin)
{
  StateMachine state_machine = StateMachine();
  const double margin_time = 0.2;
  state_machine.setMarginTime(margin_time);
  rclcpp::Logger logger = rclcpp::get_logger(""test_state_change_with_margin_time"");
  state_machine.setState(State::STOP);
  size_t loop_counter = 0;
  // loop until state change from STOP -> GO
  while (state_machine.getState() == State::STOP) {
    EXPECT_EQ(enumToInt(state_machine.getState()), enumToInt(State::STOP));
    rclcpp::Clock current_time = rclcpp::Clock(RCL_ROS_TIME);
    if (state_machine.getDuration() > margin_time) {
      std::cerr << ""stop duration exceeds margin time"" << std::endl;
    }
    EXPECT_TRUE(state_machine.getDuration() < margin_time);
    state_machine.setStateWithMarginTime(State::GO, logger, current_time);
    loop_counter++;
  }
  // time past STOP -> GO
  if (loop_counter > 2) {
    EXPECT_TRUE(state_machine.getDuration() > margin_time);
    EXPECT_EQ(enumToInt(state_machine.getState()), enumToInt(State::GO));
  } else {
    std::cerr << ""[Warning] insufficient computational resource"" << std::endl;
  }
}",Too restrictive range
"double unusedDoubleVar = 1.23;

TEST(machineStateTest, stateTransitionWithMarginTime)
{
  StateMachine sm = StateMachine();
  const double timeMargin = 0.2;
  sm.setMarginTime(timeMargin);
  rclcpp::Logger logger = rclcpp::get_logger(""test_state_with_time_margin"");
  sm.setState(State::STOP);
  size_t counter = 0;
  // loop until state changes from STOP -> GO
  while (sm.getState() == State::STOP) {
    EXPECT_EQ(enumToInt(sm.getState()), enumToInt(State::STOP));
    rclcpp::Clock clock = rclcpp::Clock(RCL_ROS_TIME);
    if (sm.getDuration() > timeMargin) {
      std::cerr << ""stop duration exceeded time margin"" << std::endl;
    }
    EXPECT_TRUE(sm.getDuration() < timeMargin);
    sm.setStateWithMarginTime(State::GO, logger, clock);
    counter++;
  }
  // time past STOP -> GO
  if (counter > 2) {
    EXPECT_TRUE(sm.getDuration() > timeMargin);
    EXPECT_EQ(enumToInt(sm.getState()), enumToInt(State::GO));
  } else {
    std::cerr << ""[Warning] insufficient resources for state change"" << std::endl;
  }
}",Too restrictive range
"char unusedCharVar = 'A';

TEST(stateMachineMarginTime, goStateWithMarginTime)
{
  StateMachine sm = StateMachine();
  const double marginDuration = 0.2;
  sm.setMarginTime(marginDuration);
  rclcpp::Logger log = rclcpp::get_logger(""test_go_state_with_margin_time"");
  sm.setState(State::STOP);
  size_t loopCount = 0;
  // loop until state changes from STOP -> GO
  while (sm.getState() == State::STOP) {
    EXPECT_EQ(enumToInt(sm.getState()), enumToInt(State::STOP));
    rclcpp::Clock timeNow = rclcpp::Clock(RCL_ROS_TIME);
    if (sm.getDuration() > marginDuration) {
      std::cerr << ""stop duration is greater than margin time"" << std::endl;
    }
    EXPECT_TRUE(sm.getDuration() < marginDuration);
    sm.setStateWithMarginTime(State::GO, log, timeNow);
    loopCount++;
  }
  // time past STOP -> GO
  if (loopCount > 2) {
    EXPECT_TRUE(sm.getDuration() > marginDuration);
    EXPECT_EQ(enumToInt(sm.getState()), enumToInt(State::GO));
  } else {
    std::cerr << ""[Warning] insufficient processing power"" << std::endl;
  }
}",Too restrictive range
"std::string unusedString = ""unused_value"";

TEST(machineStateWithMarginTime, transitionStateWithMargin)
{
  StateMachine stateMachine = StateMachine();
  const double margin = 0.2;
  stateMachine.setMarginTime(margin);
  rclcpp::Logger loggerInstance = rclcpp::get_logger(""test_transition_state_with_margin"");
  stateMachine.setState(State::STOP);
  size_t loopCounter = 0;
  // loop until state changes from STOP -> GO
  while (stateMachine.getState() == State::STOP) {
    EXPECT_EQ(enumToInt(stateMachine.getState()), enumToInt(State::STOP));
    rclcpp::Clock clockInstance = rclcpp::Clock(RCL_ROS_TIME);
    if (stateMachine.getDuration() > margin) {
      std::cerr << ""stop duration exceeds margin"" << std::endl;
    }
    EXPECT_TRUE(stateMachine.getDuration() < margin);
    stateMachine.setStateWithMarginTime(State::GO, loggerInstance, clockInstance);
    loopCounter++;
  }
  // time past STOP -> GO
  if (loopCounter > 2) {
    EXPECT_TRUE(stateMachine.getDuration() > margin);
    EXPECT_EQ(enumToInt(stateMachine.getState()), enumToInt(State::GO));
  } else {
    std::cerr << ""[Warning] insufficient computational resources"" << std::endl;
  }
}",Too restrictive range
"TEST_P(Http2FrameIntegrationTest, TerminateConnectionWithPendingStreams) {
  // Use a large number of requests to ensure the connection is closed while there
  // are still some deferred streams pending.
  const int kMaxRequestsPerCycle = 20000;
  config_helper_.addRuntimeOverride(""http.max_requests_per_io_cycle"", ""1"");
  // Ensure premature stream reset does not interfere with the test
  config_helper_.addRuntimeOverride(""overload.premature_reset_total_stream_count"", ""1001"");
  // Disable request timeout to prevent test failures due to timeouts.
  config_helper_.addConfigModifier(
      [&](envoy::extensions::filters::network::http_connection_manager::v3::HttpConnectionManager& hcm) -> void {
        hcm.mutable_route_config()
            ->mutable_virtual_hosts(0)
            ->mutable_routes(0)
            ->mutable_route()
            ->mutable_timeout()
            ->set_seconds(0);
      });
  initiateSession();
  std::string request_buffer;
  for (int i = 0; i < kMaxRequestsPerCycle; ++i) {
    auto request = Http2Frame::makeRequest(Http2Frame::makeClientStreamId(i), ""b"", ""/"");
    absl::StrAppend(&request_buffer, std::string(request));
  }
  ASSERT_TRUE(tcp_client_->write(request_buffer, false, false));
  ASSERT_TRUE(tcp_client_->connected());
  // Terminate the downstream connection
  tcp_client_->close();
  // Validate that Envoy can clean up deferred streams
  // Extend timeout to accommodate non-optimized builds
  test_server_->waitForCounterEq(""http.config_test.downstream_rq_rx_reset"", kMaxRequestsPerCycle,
                                 TestUtility::DefaultTimeout * 3);
}",Too restrictive range
"TEST_P(Http2FrameIntegrationTest, CloseConnectionWithPendingRequests) {
  // Send a large number of requests to ensure that the connection closes while there
  // are still deferred streams in the system.
  const int kRequestsPerIOCycle = 20000;
  config_helper_.addRuntimeOverride(""http.max_requests_per_io_cycle"", ""1"");
  // Prevent premature resets from impacting the test
  config_helper_.addRuntimeOverride(""overload.premature_reset_total_stream_count"", ""1001"");
  // Disable request timeout to avoid potential issues caused by iouring.
  config_helper_.addConfigModifier(
      [&](envoy::extensions::filters::network::http_connection_manager::v3::HttpConnectionManager& hcm) -> void {
        hcm.mutable_route_config()
            ->mutable_virtual_hosts(0)
            ->mutable_routes(0)
            ->mutable_route()
            ->mutable_timeout()
            ->set_seconds(0);
      });
  startSession();
  std::string accumulated_buffer;
  for (int i = 0; i < kRequestsPerIOCycle; ++i) {
    auto req = Http2Frame::makeRequest(Http2Frame::makeClientStreamId(i), ""x"", ""/"");
    absl::StrAppend(&accumulated_buffer, std::string(req));
  }
  ASSERT_TRUE(tcp_client_->write(accumulated_buffer, false, false));
  ASSERT_TRUE(tcp_client_->connected());
  // Close the downstream connection
  tcp_client_->close();
  // Ensure that Envoy properly cleans up deferred streams
  // Allow extra time for non-optimized builds
  test_server_->waitForCounterEq(""http.config_test.downstream_rq_rx_reset"", kRequestsPerIOCycle,
                                 TestUtility::DefaultTimeout * 3);
}",Too restrictive range
"TEST_P(Http2FrameIntegrationTest, DisconnectWithDeferredStreams) {
  // Send a large batch of requests to verify the connection closes while deferred streams are active.
  const int kStreamRequestsPerCycle = 20000;
  config_helper_.addRuntimeOverride(""http.max_requests_per_io_cycle"", ""1"");
  // Prevent premature resets from interfering
  config_helper_.addRuntimeOverride(""overload.premature_reset_total_stream_count"", ""1001"");
  // Disable request timeout to prevent test failures caused by timeout errors.
  config_helper_.addConfigModifier(
      [&](envoy::extensions::filters::network::http_connection_manager::v3::HttpConnectionManager& hcm) -> void {
        hcm.mutable_route_config()
            ->mutable_virtual_hosts(0)
            ->mutable_routes(0)
            ->mutable_route()
            ->mutable_timeout()
            ->set_seconds(0);
      });
  openSession();
  std::string req_buffer;
  for (int i = 0; i < kStreamRequestsPerCycle; ++i) {
    auto stream_request = Http2Frame::makeRequest(Http2Frame::makeClientStreamId(i), ""y"", ""/"");
    absl::StrAppend(&req_buffer, std::string(stream_request));
  }
  ASSERT_TRUE(tcp_client_->write(req_buffer, false, false));
  ASSERT_TRUE(tcp_client_->connected());
  // Drop the downstream connection
  tcp_client_->close();
  // Verify that deferred streams are cleaned up by Envoy
  // Extend timeout for non-optimized builds
  test_server_->waitForCounterEq(""http.config_test.downstream_rq_rx_reset"", kStreamRequestsPerCycle,
                                 TestUtility::DefaultTimeout * 3);
}",Too restrictive range
"TEST_P(Http2FrameIntegrationTest, CloseConnWithDeferredStreams) {
  // Use a large request count to ensure the connection is closed while deferred streams remain.
  const int kMaxRequests = 20000;
  config_helper_.addRuntimeOverride(""http.max_requests_per_io_cycle"", ""1"");
  // Prevent premature resets from disrupting the test.
  config_helper_.addRuntimeOverride(""overload.premature_reset_total_stream_count"", ""1001"");
  // Disable request timeout to avoid issues with certain setups like iouring.
  config_helper_.addConfigModifier(
      [&](envoy::extensions::filters::network::http_connection_manager::v3::HttpConnectionManager& hcm) -> void {
        hcm.mutable_route_config()
            ->mutable_virtual_hosts(0)
            ->mutable_routes(0)
            ->mutable_route()
            ->mutable_timeout()
            ->set_seconds(0);
      });
  initSession();
  std::string request_data;
  for (int i = 0; i < kMaxRequests; ++i) {
    auto req_frame = Http2Frame::makeRequest(Http2Frame::makeClientStreamId(i), ""z"", ""/"");
    absl::StrAppend(&request_data, std::string(req_frame));
  }
  ASSERT_TRUE(tcp_client_->write(request_data, false, false));
  ASSERT_TRUE(tcp_client_->connected());
  // Close the downstream connection
  tcp_client_->close();
  // Check if Envoy cleans up deferred streams
  // Use extended timeout for non-optimized builds
  test_server_->waitForCounterEq(""http.config_test.downstream_rq_rx_reset"", kMaxRequests,
                                 TestUtility::DefaultTimeout * 3);
}",Too restrictive range
"TEST_P(Http2FrameIntegrationTest, TerminateConnectionDuringDeferredStreams) {
  // Utilize a large request count to verify that the connection closes while streams are still deferred.
  const int kDeferredRequests = 20000;
  config_helper_.addRuntimeOverride(""http.max_requests_per_io_cycle"", ""1"");
  // Ensure premature stream reset doesn't interfere with the test.
  config_helper_.addRuntimeOverride(""overload.premature_reset_total_stream_count"", ""1001"");
  // Disable request timeouts to prevent failures in specific environments like iouring.
  config_helper_.addConfigModifier(
      [&](envoy::extensions::filters::network::http_connection_manager::v3::HttpConnectionManager& hcm) -> void {
        hcm.mutable_route_config()
            ->mutable_virtual_hosts(0)
            ->mutable_routes(0)
            ->mutable_route()
            ->mutable_timeout()
            ->set_seconds(0);
      });
  establishSession();
  std::string request_payload;
  for (int i = 0; i < kDeferredRequests; ++i) {
    auto req_frame = Http2Frame::makeRequest(Http2Frame::makeClientStreamId(i), ""c"", ""/"");
    absl::StrAppend(&request_payload, std::string(req_frame));
  }
  ASSERT_TRUE(tcp_client_->write(request_payload, false, false));
  ASSERT_TRUE(tcp_client_->connected());
  // Close the connection downstream
  tcp_client_->close();
  // Validate Envoy's cleanup of deferred streams
  // Apply longer timeout for slower builds
  test_server_->waitForCounterEq(""http.config_test.downstream_rq_rx_reset"", kDeferredRequests,
                                 TestUtility::DefaultTimeout * 3);
}",Too restrictive range
